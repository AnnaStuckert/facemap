{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: Error(s) in loading state_dict for VisionTransformer:\n",
      "\tMissing key(s) in state_dict: \"cls_token\", \"pos_embed\", \"patch_embed.proj.weight\", \"patch_embed.proj.bias\", \"blocks.0.norm1.weight\", \"blocks.0.norm1.bias\", \"blocks.0.attn.qkv.weight\", \"blocks.0.attn.qkv.bias\", \"blocks.0.attn.proj.weight\", \"blocks.0.attn.proj.bias\", \"blocks.0.norm2.weight\", \"blocks.0.norm2.bias\", \"blocks.0.mlp.fc1.weight\", \"blocks.0.mlp.fc1.bias\", \"blocks.0.mlp.fc2.weight\", \"blocks.0.mlp.fc2.bias\", \"blocks.1.norm1.weight\", \"blocks.1.norm1.bias\", \"blocks.1.attn.qkv.weight\", \"blocks.1.attn.qkv.bias\", \"blocks.1.attn.proj.weight\", \"blocks.1.attn.proj.bias\", \"blocks.1.norm2.weight\", \"blocks.1.norm2.bias\", \"blocks.1.mlp.fc1.weight\", \"blocks.1.mlp.fc1.bias\", \"blocks.1.mlp.fc2.weight\", \"blocks.1.mlp.fc2.bias\", \"blocks.2.norm1.weight\", \"blocks.2.norm1.bias\", \"blocks.2.attn.qkv.weight\", \"blocks.2.attn.qkv.bias\", \"blocks.2.attn.proj.weight\", \"blocks.2.attn.proj.bias\", \"blocks.2.norm2.weight\", \"blocks.2.norm2.bias\", \"blocks.2.mlp.fc1.weight\", \"blocks.2.mlp.fc1.bias\", \"blocks.2.mlp.fc2.weight\", \"blocks.2.mlp.fc2.bias\", \"blocks.3.norm1.weight\", \"blocks.3.norm1.bias\", \"blocks.3.attn.qkv.weight\", \"blocks.3.attn.qkv.bias\", \"blocks.3.attn.proj.weight\", \"blocks.3.attn.proj.bias\", \"blocks.3.norm2.weight\", \"blocks.3.norm2.bias\", \"blocks.3.mlp.fc1.weight\", \"blocks.3.mlp.fc1.bias\", \"blocks.3.mlp.fc2.weight\", \"blocks.3.mlp.fc2.bias\", \"blocks.4.norm1.weight\", \"blocks.4.norm1.bias\", \"blocks.4.attn.qkv.weight\", \"blocks.4.attn.qkv.bias\", \"blocks.4.attn.proj.weight\", \"blocks.4.attn.proj.bias\", \"blocks.4.norm2.weight\", \"blocks.4.norm2.bias\", \"blocks.4.mlp.fc1.weight\", \"blocks.4.mlp.fc1.bias\", \"blocks.4.mlp.fc2.weight\", \"blocks.4.mlp.fc2.bias\", \"blocks.5.norm1.weight\", \"blocks.5.norm1.bias\", \"blocks.5.attn.qkv.weight\", \"blocks.5.attn.qkv.bias\", \"blocks.5.attn.proj.weight\", \"blocks.5.attn.proj.bias\", \"blocks.5.norm2.weight\", \"blocks.5.norm2.bias\", \"blocks.5.mlp.fc1.weight\", \"blocks.5.mlp.fc1.bias\", \"blocks.5.mlp.fc2.weight\", \"blocks.5.mlp.fc2.bias\", \"blocks.6.norm1.weight\", \"blocks.6.norm1.bias\", \"blocks.6.attn.qkv.weight\", \"blocks.6.attn.qkv.bias\", \"blocks.6.attn.proj.weight\", \"blocks.6.attn.proj.bias\", \"blocks.6.norm2.weight\", \"blocks.6.norm2.bias\", \"blocks.6.mlp.fc1.weight\", \"blocks.6.mlp.fc1.bias\", \"blocks.6.mlp.fc2.weight\", \"blocks.6.mlp.fc2.bias\", \"blocks.7.norm1.weight\", \"blocks.7.norm1.bias\", \"blocks.7.attn.qkv.weight\", \"blocks.7.attn.qkv.bias\", \"blocks.7.attn.proj.weight\", \"blocks.7.attn.proj.bias\", \"blocks.7.norm2.weight\", \"blocks.7.norm2.bias\", \"blocks.7.mlp.fc1.weight\", \"blocks.7.mlp.fc1.bias\", \"blocks.7.mlp.fc2.weight\", \"blocks.7.mlp.fc2.bias\", \"blocks.8.norm1.weight\", \"blocks.8.norm1.bias\", \"blocks.8.attn.qkv.weight\", \"blocks.8.attn.qkv.bias\", \"blocks.8.attn.proj.weight\", \"blocks.8.attn.proj.bias\", \"blocks.8.norm2.weight\", \"blocks.8.norm2.bias\", \"blocks.8.mlp.fc1.weight\", \"blocks.8.mlp.fc1.bias\", \"blocks.8.mlp.fc2.weight\", \"blocks.8.mlp.fc2.bias\", \"blocks.9.norm1.weight\", \"blocks.9.norm1.bias\", \"blocks.9.attn.qkv.weight\", \"blocks.9.attn.qkv.bias\", \"blocks.9.attn.proj.weight\", \"blocks.9.attn.proj.bias\", \"blocks.9.norm2.weight\", \"blocks.9.norm2.bias\", \"blocks.9.mlp.fc1.weight\", \"blocks.9.mlp.fc1.bias\", \"blocks.9.mlp.fc2.weight\", \"blocks.9.mlp.fc2.bias\", \"blocks.10.norm1.weight\", \"blocks.10.norm1.bias\", \"blocks.10.attn.qkv.weight\", \"blocks.10.attn.qkv.bias\", \"blocks.10.attn.proj.weight\", \"blocks.10.attn.proj.bias\", \"blocks.10.norm2.weight\", \"blocks.10.norm2.bias\", \"blocks.10.mlp.fc1.weight\", \"blocks.10.mlp.fc1.bias\", \"blocks.10.mlp.fc2.weight\", \"blocks.10.mlp.fc2.bias\", \"blocks.11.norm1.weight\", \"blocks.11.norm1.bias\", \"blocks.11.attn.qkv.weight\", \"blocks.11.attn.qkv.bias\", \"blocks.11.attn.proj.weight\", \"blocks.11.attn.proj.bias\", \"blocks.11.norm2.weight\", \"blocks.11.norm2.bias\", \"blocks.11.mlp.fc1.weight\", \"blocks.11.mlp.fc1.bias\", \"blocks.11.mlp.fc2.weight\", \"blocks.11.mlp.fc2.bias\", \"norm.weight\", \"norm.bias\", \"head.weight\", \"head.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"uEnc11.weight\", \"uEnc11.bias\", \"uEnc12.weight\", \"uEnc12.bias\", \"uEnc2.enc1.weight\", \"uEnc2.enc1.bias\", \"uEnc2.enc2.weight\", \"uEnc2.enc2.bias\", \"uEnc2.bn.weight\", \"uEnc2.bn.bias\", \"uEnc2.bn.running_mean\", \"uEnc2.bn.running_var\", \"uEnc2.bn.num_batches_tracked\", \"uEnc3.enc1.weight\", \"uEnc3.enc1.bias\", \"uEnc3.enc2.weight\", \"uEnc3.enc2.bias\", \"uEnc3.bn.weight\", \"uEnc3.bn.bias\", \"uEnc3.bn.running_mean\", \"uEnc3.bn.running_var\", \"uEnc3.bn.num_batches_tracked\", \"dec3.enc1.weight\", \"dec3.enc1.bias\", \"dec3.enc2.weight\", \"dec3.enc2.bias\", \"dec3.bn.weight\", \"dec3.bn.bias\", \"dec3.bn.running_mean\", \"dec3.bn.running_var\", \"dec3.bn.num_batches_tracked\", \"dec2.enc1.weight\", \"dec2.enc1.bias\", \"dec2.enc2.weight\", \"dec2.enc2.bias\", \"dec2.bn.weight\", \"dec2.bn.bias\", \"dec2.bn.running_mean\", \"dec2.bn.running_var\", \"dec2.bn.num_batches_tracked\", \"dec11.weight\", \"dec11.bias\", \"dec12.weight\", \"dec12.bias\". \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from timm import create_model\n",
    "import torch.nn.functional as F\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from models import Unet\n",
    "\n",
    "# Paths\n",
    "video_path = r\"C:\\Users\\avs20\\Documents\\GitHub\\facemap\\cam1_G7c1_1_10seconds.avi\"\n",
    "output_video_path = \"output_video_with_keypoints_224x224.mp4\"\n",
    "model_path = r\"C:\\Users\\avs20\\Documents\\GitHub\\facemap\\models\\best_model.pt\"\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model loading function\n",
    "def load_model(model_path, num_output_classes=24):\n",
    "    model = create_model(\"vit_base_patch16_224\", pretrained=False, in_chans=1, num_classes=num_output_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = load_model(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames saved to 'frames' folder with 224x224 resolution.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    " # Define video path and output folder\n",
    "video_path = r'C:\\Users\\avs20\\Documents\\GitHub\\facemap_avs\\cam1_G7c1_1_10seconds.avi'  # Update with the path to your video\n",
    "output_folder = 'frames'  # Folder to save frames\n",
    "\n",
    " # Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    " # Set up video capture\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_count = 0\n",
    "\n",
    " # Process each frame in the video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "     # Resize the frame to 224x224\n",
    "     #frame = cv2.resize(frame, (224, 224))\n",
    "\n",
    "     # Save each frame as an image in the output folder\n",
    "    frame_filename = os.path.join(output_folder, f\"frame_{frame_count:04d}.jpg\")\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    " # Release resources\n",
    "cap.release()\n",
    "print(f\"Frames saved to '{output_folder}' folder with 224x224 resolution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists!\n",
      "Found 300 images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define image location\n",
    "IMG_LOC = r'C:\\Users\\avs20\\Documents\\GitHub\\facemap_avs\\frames'\n",
    "\n",
    "# Make folder called 'low_res' if it doesn't exist\n",
    "low_res_folder = os.path.join(IMG_LOC, 'low_res')\n",
    "if os.path.isdir(low_res_folder):\n",
    "    print('Folder exists!')\n",
    "else:\n",
    "    os.makedirs(low_res_folder)\n",
    "    print('Created low_res folder.')\n",
    "\n",
    "# Find all jpg images in IMG_LOC\n",
    "img_files = sorted(glob.glob(os.path.join(IMG_LOC, '*.jpg')))\n",
    "\n",
    "# Check if any images were found\n",
    "if not img_files:\n",
    "    print(\"No .jpg images found in the specified directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(img_files)} images.\")\n",
    "\n",
    "# specify height and width\n",
    "h = w = 224\n",
    "\n",
    "# read one image\n",
    "img = plt.imread(img_files[0])\n",
    "\n",
    "# find aspects of original image\n",
    "h_org = img.shape[0]\n",
    "w_org = img.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avs20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\timm\\models\\vision_transformer.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  x = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to 'output_video.avi'.\n",
      "Processed images saved to 'C:\\Users\\avs20\\Documents\\GitHub\\facemap_avs\\frames\\low_res'.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from skimage import io, transform, color\n",
    "\n",
    "# Define the low-res folder and video output path\n",
    "low_res_folder = r'C:\\Users\\avs20\\Documents\\GitHub\\facemap_avs\\frames\\low_res'\n",
    "os.makedirs(low_res_folder, exist_ok=True)\n",
    "video_output_path = 'output_video.avi'  # Adjust the output path\n",
    "\n",
    "# Initialize transformation for the video frames\n",
    "vit_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_height = target_width = 224\n",
    "\n",
    "# Get list of image files\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model should be defined and loaded here\n",
    "# model = YourModelClass().to(device)\n",
    "# model.load_state_dict(torch.load('your_model.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# List to hold frames for video\n",
    "frames = []\n",
    "\n",
    "# Process each image file\n",
    "for img_file in img_files:  # Limit to first 10 images or adjust as necessary\n",
    "    # Read the image\n",
    "    img = io.imread(img_file)\n",
    "\n",
    "    # Get original dimensions\n",
    "    h_org, w_org = img.shape[:2]\n",
    "    \n",
    "    # Determine the center crop region\n",
    "    start_x = (w_org - h_org) // 2  # Horizontal start point\n",
    "    start_y = 0                     # Vertical start point (full height)\n",
    "    \n",
    "    # Crop the center square\n",
    "    cropped_img = img[start_y:start_y + h_org, start_x:start_x + h_org]\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray_img = color.rgb2gray(cropped_img)\n",
    "\n",
    "    # Resize the image to 224x224 pixels\n",
    "    resized_img = transform.resize(gray_img, (target_height, target_width), anti_aliasing=True)\n",
    "\n",
    "    # Convert grayscale to RGB for drawing (3 channels)\n",
    "    color_img = np.stack([resized_img] * 3, axis=-1)  # Create an RGB version of the grayscale image\n",
    "\n",
    "    # Add key point inference\n",
    "    input_frame = vit_transform(resized_img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Model inference for keypoints\n",
    "    with torch.no_grad():\n",
    "        scores = F.softplus(model(input_frame))\n",
    "        keypoints = scores.squeeze().cpu().numpy()\n",
    "\n",
    "    # Overlay keypoints on the color image\n",
    "    for i in range(0, len(keypoints), 2):\n",
    "        x, y = int(keypoints[i]), int(keypoints[i + 1])\n",
    "        # Draw a red circle on the RGB image\n",
    "        cv2.circle(color_img, (x, y), radius=5, color=(1, 0, 0), thickness=-1)\n",
    "\n",
    "    # Save the RGB image with keypoints to the low_res folder\n",
    "    base_filename = os.path.basename(img_file)\n",
    "    save_path = os.path.join(low_res_folder, base_filename)\n",
    "    io.imsave(save_path, (color_img * 255).astype(np.uint8))  # Convert back to uint8 for saving\n",
    "\n",
    "    # Add processed frame to the frames list for video\n",
    "    frames.append((color_img * 255).astype(np.uint8))  # Convert to uint8 for video\n",
    "\n",
    "# Create a video from the frames\n",
    "if frames:\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # You can change the codec if necessary\n",
    "    video_writer = cv2.VideoWriter(video_output_path, fourcc, 30, (width, height))  # 30 fps\n",
    "\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()  # Finalize the video file\n",
    "    print(f\"Video saved to '{video_output_path}'.\")\n",
    "\n",
    "print(f\"Processed images saved to '{low_res_folder}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
