{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 224 size of images\n",
      "Num. train = 165, Num. val = 55, Num. test = 55\n",
      "Number of parameters:85 M\n",
      "Epoch [1/300], Step [1/42], Loss: 2366.9194\n",
      "Epoch [1/300], Step [2/42], Loss: 2034.3840\n",
      "Epoch [1/300], Step [3/42], Loss: 1308.1278\n",
      "Epoch [1/300], Step [4/42], Loss: 1051.6830\n",
      "Epoch [1/300], Step [5/42], Loss: 759.5570\n",
      "Epoch [1/300], Step [6/42], Loss: 717.6893\n",
      "Epoch [1/300], Step [7/42], Loss: 553.8998\n",
      "Epoch [1/300], Step [8/42], Loss: 561.1817\n",
      "Epoch [1/300], Step [9/42], Loss: 480.5417\n",
      "Epoch [1/300], Step [10/42], Loss: 466.3611\n",
      "Epoch [1/300], Step [11/42], Loss: 421.8515\n",
      "Epoch [1/300], Step [12/42], Loss: 460.6295\n",
      "Epoch [1/300], Step [13/42], Loss: 349.2466\n",
      "Epoch [1/300], Step [14/42], Loss: 419.2116\n",
      "Epoch [1/300], Step [15/42], Loss: 295.9254\n",
      "Epoch [1/300], Step [16/42], Loss: 340.0916\n",
      "Epoch [1/300], Step [17/42], Loss: 374.8528\n",
      "Epoch [1/300], Step [18/42], Loss: 316.1220\n",
      "Epoch [1/300], Step [19/42], Loss: 340.6238\n",
      "Epoch [1/300], Step [20/42], Loss: 346.9474\n",
      "Epoch [1/300], Step [21/42], Loss: 295.5872\n",
      "Epoch [1/300], Step [22/42], Loss: 320.8541\n",
      "Epoch [1/300], Step [23/42], Loss: 349.7769\n",
      "Epoch [1/300], Step [24/42], Loss: 273.7444\n",
      "Epoch [1/300], Step [25/42], Loss: 252.8738\n",
      "Epoch [1/300], Step [26/42], Loss: 268.5242\n",
      "Epoch [1/300], Step [27/42], Loss: 275.5640\n",
      "Epoch [1/300], Step [28/42], Loss: 207.4816\n",
      "Epoch [1/300], Step [29/42], Loss: 258.5573\n",
      "Epoch [1/300], Step [30/42], Loss: 220.6906\n",
      "Epoch [1/300], Step [31/42], Loss: 327.4095\n",
      "Epoch [1/300], Step [32/42], Loss: 213.2668\n",
      "Epoch [1/300], Step [33/42], Loss: 208.3708\n",
      "Epoch [1/300], Step [34/42], Loss: 201.6837\n",
      "Epoch [1/300], Step [35/42], Loss: 238.0214\n",
      "Epoch [1/300], Step [36/42], Loss: 177.6403\n",
      "Epoch [1/300], Step [37/42], Loss: 232.9785\n",
      "Epoch [1/300], Step [38/42], Loss: 210.0340\n",
      "Epoch [1/300], Step [39/42], Loss: 173.2607\n",
      "Epoch [1/300], Step [40/42], Loss: 196.6799\n",
      "Epoch [1/300], Step [41/42], Loss: 127.5375\n",
      "Epoch [1/300], Step [42/42], Loss: 32.8402\n",
      "Val. loss :195.4337\n",
      "Epoch [2/300], Step [1/42], Loss: 181.7989\n",
      "Epoch [2/300], Step [2/42], Loss: 138.9955\n",
      "Epoch [2/300], Step [3/42], Loss: 119.0183\n",
      "Epoch [2/300], Step [4/42], Loss: 179.1685\n",
      "Epoch [2/300], Step [5/42], Loss: 164.8337\n",
      "Epoch [2/300], Step [6/42], Loss: 172.1048\n",
      "Epoch [2/300], Step [7/42], Loss: 180.3384\n",
      "Epoch [2/300], Step [8/42], Loss: 170.2432\n",
      "Epoch [2/300], Step [9/42], Loss: 169.3040\n",
      "Epoch [2/300], Step [10/42], Loss: 196.3989\n",
      "Epoch [2/300], Step [11/42], Loss: 211.8497\n",
      "Epoch [2/300], Step [12/42], Loss: 155.3885\n",
      "Epoch [2/300], Step [13/42], Loss: 152.9489\n",
      "Epoch [2/300], Step [14/42], Loss: 154.0888\n",
      "Epoch [2/300], Step [15/42], Loss: 174.0343\n",
      "Epoch [2/300], Step [16/42], Loss: 160.2123\n",
      "Epoch [2/300], Step [17/42], Loss: 144.7750\n",
      "Epoch [2/300], Step [18/42], Loss: 150.0187\n",
      "Epoch [2/300], Step [19/42], Loss: 151.8516\n",
      "Epoch [2/300], Step [20/42], Loss: 156.2267\n",
      "Epoch [2/300], Step [21/42], Loss: 202.9619\n",
      "Epoch [2/300], Step [22/42], Loss: 125.2342\n",
      "Epoch [2/300], Step [23/42], Loss: 169.1593\n",
      "Epoch [2/300], Step [24/42], Loss: 142.9890\n",
      "Epoch [2/300], Step [25/42], Loss: 147.0583\n",
      "Epoch [2/300], Step [26/42], Loss: 135.4991\n",
      "Epoch [2/300], Step [27/42], Loss: 155.1789\n",
      "Epoch [2/300], Step [28/42], Loss: 139.3907\n",
      "Epoch [2/300], Step [29/42], Loss: 107.2950\n",
      "Epoch [2/300], Step [30/42], Loss: 169.3699\n",
      "Epoch [2/300], Step [31/42], Loss: 121.7969\n",
      "Epoch [2/300], Step [32/42], Loss: 109.0488\n",
      "Epoch [2/300], Step [33/42], Loss: 137.1167\n",
      "Epoch [2/300], Step [34/42], Loss: 140.1357\n",
      "Epoch [2/300], Step [35/42], Loss: 153.3545\n",
      "Epoch [2/300], Step [36/42], Loss: 148.2128\n",
      "Epoch [2/300], Step [37/42], Loss: 136.1004\n",
      "Epoch [2/300], Step [38/42], Loss: 141.7921\n",
      "Epoch [2/300], Step [39/42], Loss: 181.7627\n",
      "Epoch [2/300], Step [40/42], Loss: 103.6765\n",
      "Epoch [2/300], Step [41/42], Loss: 132.1361\n",
      "Epoch [2/300], Step [42/42], Loss: 23.3913\n",
      "Val. loss :123.8448\n",
      "Epoch [3/300], Step [1/42], Loss: 135.9749\n",
      "Epoch [3/300], Step [2/42], Loss: 105.3713\n",
      "Epoch [3/300], Step [3/42], Loss: 136.3516\n",
      "Epoch [3/300], Step [4/42], Loss: 149.3006\n",
      "Epoch [3/300], Step [5/42], Loss: 115.3834\n",
      "Epoch [3/300], Step [6/42], Loss: 137.6914\n",
      "Epoch [3/300], Step [7/42], Loss: 130.1634\n",
      "Epoch [3/300], Step [8/42], Loss: 126.4849\n",
      "Epoch [3/300], Step [9/42], Loss: 141.7601\n",
      "Epoch [3/300], Step [10/42], Loss: 111.1394\n",
      "Epoch [3/300], Step [11/42], Loss: 104.8672\n",
      "Epoch [3/300], Step [12/42], Loss: 112.8885\n",
      "Epoch [3/300], Step [13/42], Loss: 97.1734\n",
      "Epoch [3/300], Step [14/42], Loss: 126.6865\n",
      "Epoch [3/300], Step [15/42], Loss: 93.9798\n",
      "Epoch [3/300], Step [16/42], Loss: 92.3337\n",
      "Epoch [3/300], Step [17/42], Loss: 103.1829\n",
      "Epoch [3/300], Step [18/42], Loss: 99.9190\n",
      "Epoch [3/300], Step [19/42], Loss: 120.8658\n",
      "Epoch [3/300], Step [20/42], Loss: 99.1646\n",
      "Epoch [3/300], Step [21/42], Loss: 82.2120\n",
      "Epoch [3/300], Step [22/42], Loss: 119.9683\n",
      "Epoch [3/300], Step [23/42], Loss: 96.2182\n",
      "Epoch [3/300], Step [24/42], Loss: 104.7932\n",
      "Epoch [3/300], Step [25/42], Loss: 67.3104\n",
      "Epoch [3/300], Step [26/42], Loss: 71.1802\n",
      "Epoch [3/300], Step [27/42], Loss: 119.0608\n",
      "Epoch [3/300], Step [28/42], Loss: 85.9389\n",
      "Epoch [3/300], Step [29/42], Loss: 79.7156\n",
      "Epoch [3/300], Step [30/42], Loss: 102.0429\n",
      "Epoch [3/300], Step [31/42], Loss: 86.8645\n",
      "Epoch [3/300], Step [32/42], Loss: 94.0564\n",
      "Epoch [3/300], Step [33/42], Loss: 104.7915\n",
      "Epoch [3/300], Step [34/42], Loss: 110.3630\n",
      "Epoch [3/300], Step [35/42], Loss: 112.4381\n",
      "Epoch [3/300], Step [36/42], Loss: 99.2396\n",
      "Epoch [3/300], Step [37/42], Loss: 89.3764\n",
      "Epoch [3/300], Step [38/42], Loss: 71.7748\n",
      "Epoch [3/300], Step [39/42], Loss: 83.7446\n",
      "Epoch [3/300], Step [40/42], Loss: 92.2284\n",
      "Epoch [3/300], Step [41/42], Loss: 90.2003\n",
      "Epoch [3/300], Step [42/42], Loss: 27.3631\n",
      "Val. loss :93.4078\n",
      "Epoch [4/300], Step [1/42], Loss: 96.5241\n",
      "Epoch [4/300], Step [2/42], Loss: 127.8404\n",
      "Epoch [4/300], Step [3/42], Loss: 91.7352\n",
      "Epoch [4/300], Step [4/42], Loss: 76.9770\n",
      "Epoch [4/300], Step [5/42], Loss: 100.0498\n",
      "Epoch [4/300], Step [6/42], Loss: 92.3761\n",
      "Epoch [4/300], Step [7/42], Loss: 77.0006\n",
      "Epoch [4/300], Step [8/42], Loss: 76.9938\n",
      "Epoch [4/300], Step [9/42], Loss: 88.5546\n",
      "Epoch [4/300], Step [10/42], Loss: 106.6855\n",
      "Epoch [4/300], Step [11/42], Loss: 86.9709\n",
      "Epoch [4/300], Step [12/42], Loss: 93.5853\n",
      "Epoch [4/300], Step [13/42], Loss: 76.4792\n",
      "Epoch [4/300], Step [14/42], Loss: 62.5854\n",
      "Epoch [4/300], Step [15/42], Loss: 85.0234\n",
      "Epoch [4/300], Step [16/42], Loss: 65.9291\n",
      "Epoch [4/300], Step [17/42], Loss: 101.7041\n",
      "Epoch [4/300], Step [18/42], Loss: 66.0497\n",
      "Epoch [4/300], Step [19/42], Loss: 87.0139\n",
      "Epoch [4/300], Step [20/42], Loss: 71.7048\n",
      "Epoch [4/300], Step [21/42], Loss: 82.5848\n",
      "Epoch [4/300], Step [22/42], Loss: 70.5567\n",
      "Epoch [4/300], Step [23/42], Loss: 91.6141\n",
      "Epoch [4/300], Step [24/42], Loss: 86.0517\n",
      "Epoch [4/300], Step [25/42], Loss: 75.3322\n",
      "Epoch [4/300], Step [26/42], Loss: 77.2331\n",
      "Epoch [4/300], Step [27/42], Loss: 70.6968\n",
      "Epoch [4/300], Step [28/42], Loss: 67.2959\n",
      "Epoch [4/300], Step [29/42], Loss: 73.8165\n",
      "Epoch [4/300], Step [30/42], Loss: 91.5240\n",
      "Epoch [4/300], Step [31/42], Loss: 76.2906\n",
      "Epoch [4/300], Step [32/42], Loss: 71.2086\n",
      "Epoch [4/300], Step [33/42], Loss: 49.3364\n",
      "Epoch [4/300], Step [34/42], Loss: 76.6408\n",
      "Epoch [4/300], Step [35/42], Loss: 75.3253\n",
      "Epoch [4/300], Step [36/42], Loss: 75.8978\n",
      "Epoch [4/300], Step [37/42], Loss: 61.7324\n",
      "Epoch [4/300], Step [38/42], Loss: 76.4130\n",
      "Epoch [4/300], Step [39/42], Loss: 78.7347\n",
      "Epoch [4/300], Step [40/42], Loss: 74.0171\n",
      "Epoch [4/300], Step [41/42], Loss: 75.8561\n",
      "Epoch [4/300], Step [42/42], Loss: 15.7239\n",
      "Val. loss :73.1328\n",
      "Epoch [5/300], Step [1/42], Loss: 78.2681\n",
      "Epoch [5/300], Step [2/42], Loss: 85.2590\n",
      "Epoch [5/300], Step [3/42], Loss: 71.1421\n",
      "Epoch [5/300], Step [4/42], Loss: 62.2668\n",
      "Epoch [5/300], Step [5/42], Loss: 83.4534\n",
      "Epoch [5/300], Step [6/42], Loss: 58.5011\n",
      "Epoch [5/300], Step [7/42], Loss: 73.3174\n",
      "Epoch [5/300], Step [8/42], Loss: 78.1734\n",
      "Epoch [5/300], Step [9/42], Loss: 65.2068\n",
      "Epoch [5/300], Step [10/42], Loss: 68.7307\n",
      "Epoch [5/300], Step [11/42], Loss: 67.8676\n",
      "Epoch [5/300], Step [12/42], Loss: 73.0489\n",
      "Epoch [5/300], Step [13/42], Loss: 67.9671\n",
      "Epoch [5/300], Step [14/42], Loss: 81.8889\n",
      "Epoch [5/300], Step [15/42], Loss: 60.6087\n",
      "Epoch [5/300], Step [16/42], Loss: 84.4241\n",
      "Epoch [5/300], Step [17/42], Loss: 72.7454\n",
      "Epoch [5/300], Step [18/42], Loss: 71.4706\n",
      "Epoch [5/300], Step [19/42], Loss: 64.9421\n",
      "Epoch [5/300], Step [20/42], Loss: 65.6146\n",
      "Epoch [5/300], Step [21/42], Loss: 55.6583\n",
      "Epoch [5/300], Step [22/42], Loss: 60.6701\n",
      "Epoch [5/300], Step [23/42], Loss: 71.6747\n",
      "Epoch [5/300], Step [24/42], Loss: 68.3576\n",
      "Epoch [5/300], Step [25/42], Loss: 64.5209\n",
      "Epoch [5/300], Step [26/42], Loss: 59.8985\n",
      "Epoch [5/300], Step [27/42], Loss: 59.2435\n",
      "Epoch [5/300], Step [28/42], Loss: 59.9152\n",
      "Epoch [5/300], Step [29/42], Loss: 56.1267\n",
      "Epoch [5/300], Step [30/42], Loss: 69.2948\n",
      "Epoch [5/300], Step [31/42], Loss: 61.2177\n",
      "Epoch [5/300], Step [32/42], Loss: 67.1326\n",
      "Epoch [5/300], Step [33/42], Loss: 62.2464\n",
      "Epoch [5/300], Step [34/42], Loss: 60.5517\n",
      "Epoch [5/300], Step [35/42], Loss: 49.5499\n",
      "Epoch [5/300], Step [36/42], Loss: 62.0392\n",
      "Epoch [5/300], Step [37/42], Loss: 64.6889\n",
      "Epoch [5/300], Step [38/42], Loss: 64.7048\n",
      "Epoch [5/300], Step [39/42], Loss: 59.3686\n",
      "Epoch [5/300], Step [40/42], Loss: 55.1926\n",
      "Epoch [5/300], Step [41/42], Loss: 66.3649\n",
      "Epoch [5/300], Step [42/42], Loss: 9.1992\n",
      "Val. loss :63.0725\n",
      "Epoch [6/300], Step [1/42], Loss: 68.0791\n",
      "Epoch [6/300], Step [2/42], Loss: 57.4729\n",
      "Epoch [6/300], Step [3/42], Loss: 61.1148\n",
      "Epoch [6/300], Step [4/42], Loss: 58.1201\n",
      "Epoch [6/300], Step [5/42], Loss: 48.5404\n",
      "Epoch [6/300], Step [6/42], Loss: 72.5980\n",
      "Epoch [6/300], Step [7/42], Loss: 48.5105\n",
      "Epoch [6/300], Step [8/42], Loss: 58.5352\n",
      "Epoch [6/300], Step [9/42], Loss: 62.2969\n",
      "Epoch [6/300], Step [10/42], Loss: 59.7419\n",
      "Epoch [6/300], Step [11/42], Loss: 57.7426\n",
      "Epoch [6/300], Step [12/42], Loss: 59.9033\n",
      "Epoch [6/300], Step [13/42], Loss: 62.8847\n",
      "Epoch [6/300], Step [14/42], Loss: 60.8367\n",
      "Epoch [6/300], Step [15/42], Loss: 72.8167\n",
      "Epoch [6/300], Step [16/42], Loss: 62.4536\n",
      "Epoch [6/300], Step [17/42], Loss: 54.7230\n",
      "Epoch [6/300], Step [18/42], Loss: 51.0783\n",
      "Epoch [6/300], Step [19/42], Loss: 62.4102\n",
      "Epoch [6/300], Step [20/42], Loss: 56.6539\n",
      "Epoch [6/300], Step [21/42], Loss: 54.6743\n",
      "Epoch [6/300], Step [22/42], Loss: 60.4498\n",
      "Epoch [6/300], Step [23/42], Loss: 44.7961\n",
      "Epoch [6/300], Step [24/42], Loss: 51.7997\n",
      "Epoch [6/300], Step [25/42], Loss: 66.4632\n",
      "Epoch [6/300], Step [26/42], Loss: 65.3335\n",
      "Epoch [6/300], Step [27/42], Loss: 48.6630\n",
      "Epoch [6/300], Step [28/42], Loss: 61.2135\n",
      "Epoch [6/300], Step [29/42], Loss: 51.5842\n",
      "Epoch [6/300], Step [30/42], Loss: 54.3092\n",
      "Epoch [6/300], Step [31/42], Loss: 52.3417\n",
      "Epoch [6/300], Step [32/42], Loss: 53.8995\n",
      "Epoch [6/300], Step [33/42], Loss: 52.1908\n",
      "Epoch [6/300], Step [34/42], Loss: 66.4983\n",
      "Epoch [6/300], Step [35/42], Loss: 58.8476\n",
      "Epoch [6/300], Step [36/42], Loss: 48.1995\n",
      "Epoch [6/300], Step [37/42], Loss: 55.7976\n",
      "Epoch [6/300], Step [38/42], Loss: 55.7959\n",
      "Epoch [6/300], Step [39/42], Loss: 52.3890\n",
      "Epoch [6/300], Step [40/42], Loss: 56.4176\n",
      "Epoch [6/300], Step [41/42], Loss: 46.4459\n",
      "Epoch [6/300], Step [42/42], Loss: 11.9737\n",
      "Val. loss :55.5489\n",
      "Epoch [7/300], Step [1/42], Loss: 50.2440\n",
      "Epoch [7/300], Step [2/42], Loss: 57.2405\n",
      "Epoch [7/300], Step [3/42], Loss: 49.2935\n",
      "Epoch [7/300], Step [4/42], Loss: 44.3665\n",
      "Epoch [7/300], Step [5/42], Loss: 53.6306\n",
      "Epoch [7/300], Step [6/42], Loss: 42.6607\n",
      "Epoch [7/300], Step [7/42], Loss: 57.9563\n",
      "Epoch [7/300], Step [8/42], Loss: 54.6521\n",
      "Epoch [7/300], Step [9/42], Loss: 57.4989\n",
      "Epoch [7/300], Step [10/42], Loss: 53.1859\n",
      "Epoch [7/300], Step [11/42], Loss: 56.1494\n",
      "Epoch [7/300], Step [12/42], Loss: 47.4177\n",
      "Epoch [7/300], Step [13/42], Loss: 51.7833\n",
      "Epoch [7/300], Step [14/42], Loss: 55.5240\n",
      "Epoch [7/300], Step [15/42], Loss: 58.8814\n",
      "Epoch [7/300], Step [16/42], Loss: 55.6623\n",
      "Epoch [7/300], Step [17/42], Loss: 48.7488\n",
      "Epoch [7/300], Step [18/42], Loss: 55.7859\n",
      "Epoch [7/300], Step [19/42], Loss: 47.1015\n",
      "Epoch [7/300], Step [20/42], Loss: 56.3605\n",
      "Epoch [7/300], Step [21/42], Loss: 41.2614\n",
      "Epoch [7/300], Step [22/42], Loss: 43.4780\n",
      "Epoch [7/300], Step [23/42], Loss: 50.5868\n",
      "Epoch [7/300], Step [24/42], Loss: 48.4000\n",
      "Epoch [7/300], Step [25/42], Loss: 63.6406\n",
      "Epoch [7/300], Step [26/42], Loss: 55.3036\n",
      "Epoch [7/300], Step [27/42], Loss: 67.3190\n",
      "Epoch [7/300], Step [28/42], Loss: 43.5293\n",
      "Epoch [7/300], Step [29/42], Loss: 53.7515\n",
      "Epoch [7/300], Step [30/42], Loss: 48.4616\n",
      "Epoch [7/300], Step [31/42], Loss: 45.7471\n",
      "Epoch [7/300], Step [32/42], Loss: 45.3194\n",
      "Epoch [7/300], Step [33/42], Loss: 52.4141\n",
      "Epoch [7/300], Step [34/42], Loss: 49.4677\n",
      "Epoch [7/300], Step [35/42], Loss: 63.6211\n",
      "Epoch [7/300], Step [36/42], Loss: 49.5519\n",
      "Epoch [7/300], Step [37/42], Loss: 42.4424\n",
      "Epoch [7/300], Step [38/42], Loss: 49.1320\n",
      "Epoch [7/300], Step [39/42], Loss: 46.5206\n",
      "Epoch [7/300], Step [40/42], Loss: 46.9802\n",
      "Epoch [7/300], Step [41/42], Loss: 45.8139\n",
      "Epoch [7/300], Step [42/42], Loss: 11.9377\n",
      "Val. loss :50.1472\n",
      "Epoch [8/300], Step [1/42], Loss: 49.3078\n",
      "Epoch [8/300], Step [2/42], Loss: 56.5275\n",
      "Epoch [8/300], Step [3/42], Loss: 54.1861\n",
      "Epoch [8/300], Step [4/42], Loss: 58.6284\n",
      "Epoch [8/300], Step [5/42], Loss: 47.1412\n",
      "Epoch [8/300], Step [6/42], Loss: 50.4980\n",
      "Epoch [8/300], Step [7/42], Loss: 38.9497\n",
      "Epoch [8/300], Step [8/42], Loss: 54.3348\n",
      "Epoch [8/300], Step [9/42], Loss: 43.0642\n",
      "Epoch [8/300], Step [10/42], Loss: 50.0446\n",
      "Epoch [8/300], Step [11/42], Loss: 49.3118\n",
      "Epoch [8/300], Step [12/42], Loss: 55.6911\n",
      "Epoch [8/300], Step [13/42], Loss: 52.6206\n",
      "Epoch [8/300], Step [14/42], Loss: 46.8609\n",
      "Epoch [8/300], Step [15/42], Loss: 48.9634\n",
      "Epoch [8/300], Step [16/42], Loss: 47.7342\n",
      "Epoch [8/300], Step [17/42], Loss: 46.0153\n",
      "Epoch [8/300], Step [18/42], Loss: 42.1796\n",
      "Epoch [8/300], Step [19/42], Loss: 45.0091\n",
      "Epoch [8/300], Step [20/42], Loss: 40.6127\n",
      "Epoch [8/300], Step [21/42], Loss: 39.9543\n",
      "Epoch [8/300], Step [22/42], Loss: 38.2333\n",
      "Epoch [8/300], Step [23/42], Loss: 42.9950\n",
      "Epoch [8/300], Step [24/42], Loss: 50.0220\n",
      "Epoch [8/300], Step [25/42], Loss: 46.4779\n",
      "Epoch [8/300], Step [26/42], Loss: 44.7535\n",
      "Epoch [8/300], Step [27/42], Loss: 47.1941\n",
      "Epoch [8/300], Step [28/42], Loss: 43.3492\n",
      "Epoch [8/300], Step [29/42], Loss: 58.0286\n",
      "Epoch [8/300], Step [30/42], Loss: 43.0585\n",
      "Epoch [8/300], Step [31/42], Loss: 44.2474\n",
      "Epoch [8/300], Step [32/42], Loss: 43.5354\n",
      "Epoch [8/300], Step [33/42], Loss: 50.8289\n",
      "Epoch [8/300], Step [34/42], Loss: 41.5221\n",
      "Epoch [8/300], Step [35/42], Loss: 46.9068\n",
      "Epoch [8/300], Step [36/42], Loss: 41.4329\n",
      "Epoch [8/300], Step [37/42], Loss: 40.2234\n",
      "Epoch [8/300], Step [38/42], Loss: 54.4854\n",
      "Epoch [8/300], Step [39/42], Loss: 33.4389\n",
      "Epoch [8/300], Step [40/42], Loss: 57.6734\n",
      "Epoch [8/300], Step [41/42], Loss: 42.0877\n",
      "Epoch [8/300], Step [42/42], Loss: 10.1880\n",
      "Val. loss :45.7235\n",
      "Epoch [9/300], Step [1/42], Loss: 38.1163\n",
      "Epoch [9/300], Step [2/42], Loss: 44.5460\n",
      "Epoch [9/300], Step [3/42], Loss: 44.7426\n",
      "Epoch [9/300], Step [4/42], Loss: 43.0236\n",
      "Epoch [9/300], Step [5/42], Loss: 52.4583\n",
      "Epoch [9/300], Step [6/42], Loss: 47.3464\n",
      "Epoch [9/300], Step [7/42], Loss: 38.9879\n",
      "Epoch [9/300], Step [8/42], Loss: 38.9625\n",
      "Epoch [9/300], Step [9/42], Loss: 45.6144\n",
      "Epoch [9/300], Step [10/42], Loss: 41.2361\n",
      "Epoch [9/300], Step [11/42], Loss: 35.1355\n",
      "Epoch [9/300], Step [12/42], Loss: 40.2477\n",
      "Epoch [9/300], Step [13/42], Loss: 34.7166\n",
      "Epoch [9/300], Step [14/42], Loss: 39.2291\n",
      "Epoch [9/300], Step [15/42], Loss: 39.9158\n",
      "Epoch [9/300], Step [16/42], Loss: 72.8652\n",
      "Epoch [9/300], Step [17/42], Loss: 49.0813\n",
      "Epoch [9/300], Step [18/42], Loss: 50.8744\n",
      "Epoch [9/300], Step [19/42], Loss: 43.6795\n",
      "Epoch [9/300], Step [20/42], Loss: 45.5145\n",
      "Epoch [9/300], Step [21/42], Loss: 38.0628\n",
      "Epoch [9/300], Step [22/42], Loss: 41.1276\n",
      "Epoch [9/300], Step [23/42], Loss: 41.9181\n",
      "Epoch [9/300], Step [24/42], Loss: 47.0107\n",
      "Epoch [9/300], Step [25/42], Loss: 55.2727\n",
      "Epoch [9/300], Step [26/42], Loss: 44.0447\n",
      "Epoch [9/300], Step [27/42], Loss: 44.1624\n",
      "Epoch [9/300], Step [28/42], Loss: 55.2701\n",
      "Epoch [9/300], Step [29/42], Loss: 45.3361\n",
      "Epoch [9/300], Step [30/42], Loss: 39.3505\n",
      "Epoch [9/300], Step [31/42], Loss: 42.8841\n",
      "Epoch [9/300], Step [32/42], Loss: 41.0083\n",
      "Epoch [9/300], Step [33/42], Loss: 48.0905\n",
      "Epoch [9/300], Step [34/42], Loss: 46.1484\n",
      "Epoch [9/300], Step [35/42], Loss: 52.9709\n",
      "Epoch [9/300], Step [36/42], Loss: 42.5185\n",
      "Epoch [9/300], Step [37/42], Loss: 38.7313\n",
      "Epoch [9/300], Step [38/42], Loss: 41.1031\n",
      "Epoch [9/300], Step [39/42], Loss: 33.6710\n",
      "Epoch [9/300], Step [40/42], Loss: 43.8757\n",
      "Epoch [9/300], Step [41/42], Loss: 42.1678\n",
      "Epoch [9/300], Step [42/42], Loss: 12.1750\n",
      "Val. loss :44.1157\n",
      "Epoch [10/300], Step [1/42], Loss: 45.4833\n",
      "Epoch [10/300], Step [2/42], Loss: 37.6480\n",
      "Epoch [10/300], Step [3/42], Loss: 46.6672\n",
      "Epoch [10/300], Step [4/42], Loss: 49.6450\n",
      "Epoch [10/300], Step [5/42], Loss: 43.5180\n",
      "Epoch [10/300], Step [6/42], Loss: 52.0051\n",
      "Epoch [10/300], Step [7/42], Loss: 43.4945\n",
      "Epoch [10/300], Step [8/42], Loss: 37.9757\n",
      "Epoch [10/300], Step [9/42], Loss: 43.3931\n",
      "Epoch [10/300], Step [10/42], Loss: 40.7511\n",
      "Epoch [10/300], Step [11/42], Loss: 53.8232\n",
      "Epoch [10/300], Step [12/42], Loss: 34.8274\n",
      "Epoch [10/300], Step [13/42], Loss: 36.3487\n",
      "Epoch [10/300], Step [14/42], Loss: 44.3189\n",
      "Epoch [10/300], Step [15/42], Loss: 35.4434\n",
      "Epoch [10/300], Step [16/42], Loss: 42.6827\n",
      "Epoch [10/300], Step [17/42], Loss: 45.7953\n",
      "Epoch [10/300], Step [18/42], Loss: 30.6205\n",
      "Epoch [10/300], Step [19/42], Loss: 54.3782\n",
      "Epoch [10/300], Step [20/42], Loss: 44.4337\n",
      "Epoch [10/300], Step [21/42], Loss: 34.0106\n",
      "Epoch [10/300], Step [22/42], Loss: 39.7460\n",
      "Epoch [10/300], Step [23/42], Loss: 39.3206\n",
      "Epoch [10/300], Step [24/42], Loss: 39.9327\n",
      "Epoch [10/300], Step [25/42], Loss: 44.4614\n",
      "Epoch [10/300], Step [26/42], Loss: 43.3360\n",
      "Epoch [10/300], Step [27/42], Loss: 49.8350\n",
      "Epoch [10/300], Step [28/42], Loss: 43.9992\n",
      "Epoch [10/300], Step [29/42], Loss: 38.2638\n",
      "Epoch [10/300], Step [30/42], Loss: 47.6415\n",
      "Epoch [10/300], Step [31/42], Loss: 44.4862\n",
      "Epoch [10/300], Step [32/42], Loss: 43.5714\n",
      "Epoch [10/300], Step [33/42], Loss: 35.6575\n",
      "Epoch [10/300], Step [34/42], Loss: 47.2724\n",
      "Epoch [10/300], Step [35/42], Loss: 43.2971\n",
      "Epoch [10/300], Step [36/42], Loss: 35.9968\n",
      "Epoch [10/300], Step [37/42], Loss: 43.7523\n",
      "Epoch [10/300], Step [38/42], Loss: 38.2164\n",
      "Epoch [10/300], Step [39/42], Loss: 35.7441\n",
      "Epoch [10/300], Step [40/42], Loss: 38.5197\n",
      "Epoch [10/300], Step [41/42], Loss: 34.4595\n",
      "Epoch [10/300], Step [42/42], Loss: 15.5004\n",
      "Val. loss :42.9249\n",
      "Epoch [11/300], Step [1/42], Loss: 34.2353\n",
      "Epoch [11/300], Step [2/42], Loss: 33.5938\n",
      "Epoch [11/300], Step [3/42], Loss: 42.0889\n",
      "Epoch [11/300], Step [4/42], Loss: 37.1374\n",
      "Epoch [11/300], Step [5/42], Loss: 37.3111\n",
      "Epoch [11/300], Step [6/42], Loss: 37.1043\n",
      "Epoch [11/300], Step [7/42], Loss: 39.7391\n",
      "Epoch [11/300], Step [8/42], Loss: 44.5564\n",
      "Epoch [11/300], Step [9/42], Loss: 42.4842\n",
      "Epoch [11/300], Step [10/42], Loss: 45.9261\n",
      "Epoch [11/300], Step [11/42], Loss: 42.8048\n",
      "Epoch [11/300], Step [12/42], Loss: 46.0361\n",
      "Epoch [11/300], Step [13/42], Loss: 36.8339\n",
      "Epoch [11/300], Step [14/42], Loss: 51.9242\n",
      "Epoch [11/300], Step [15/42], Loss: 35.4819\n",
      "Epoch [11/300], Step [16/42], Loss: 34.8825\n",
      "Epoch [11/300], Step [17/42], Loss: 38.4952\n",
      "Epoch [11/300], Step [18/42], Loss: 39.2079\n",
      "Epoch [11/300], Step [19/42], Loss: 35.7643\n",
      "Epoch [11/300], Step [20/42], Loss: 35.8108\n",
      "Epoch [11/300], Step [21/42], Loss: 42.0540\n",
      "Epoch [11/300], Step [22/42], Loss: 48.2859\n",
      "Epoch [11/300], Step [23/42], Loss: 42.5282\n",
      "Epoch [11/300], Step [24/42], Loss: 37.4475\n",
      "Epoch [11/300], Step [25/42], Loss: 36.8433\n",
      "Epoch [11/300], Step [26/42], Loss: 35.2931\n",
      "Epoch [11/300], Step [27/42], Loss: 29.7809\n",
      "Epoch [11/300], Step [28/42], Loss: 33.6010\n",
      "Epoch [11/300], Step [29/42], Loss: 34.2541\n",
      "Epoch [11/300], Step [30/42], Loss: 36.8881\n",
      "Epoch [11/300], Step [31/42], Loss: 42.9721\n",
      "Epoch [11/300], Step [32/42], Loss: 47.1080\n",
      "Epoch [11/300], Step [33/42], Loss: 32.6033\n",
      "Epoch [11/300], Step [34/42], Loss: 46.7822\n",
      "Epoch [11/300], Step [35/42], Loss: 42.3632\n",
      "Epoch [11/300], Step [36/42], Loss: 40.8519\n",
      "Epoch [11/300], Step [37/42], Loss: 35.7394\n",
      "Epoch [11/300], Step [38/42], Loss: 67.2540\n",
      "Epoch [11/300], Step [39/42], Loss: 40.3541\n",
      "Epoch [11/300], Step [40/42], Loss: 32.1382\n",
      "Epoch [11/300], Step [41/42], Loss: 44.4611\n",
      "Epoch [11/300], Step [42/42], Loss: 8.3235\n",
      "Val. loss :43.0620\n",
      "Epoch [12/300], Step [1/42], Loss: 51.0106\n",
      "Epoch [12/300], Step [2/42], Loss: 39.7751\n",
      "Epoch [12/300], Step [3/42], Loss: 62.0260\n",
      "Epoch [12/300], Step [4/42], Loss: 50.4188\n",
      "Epoch [12/300], Step [5/42], Loss: 39.4127\n",
      "Epoch [12/300], Step [6/42], Loss: 36.2683\n",
      "Epoch [12/300], Step [7/42], Loss: 40.1350\n",
      "Epoch [12/300], Step [8/42], Loss: 37.8544\n",
      "Epoch [12/300], Step [9/42], Loss: 39.2179\n",
      "Epoch [12/300], Step [10/42], Loss: 43.9182\n",
      "Epoch [12/300], Step [11/42], Loss: 42.3320\n",
      "Epoch [12/300], Step [12/42], Loss: 47.0401\n",
      "Epoch [12/300], Step [13/42], Loss: 38.8052\n",
      "Epoch [12/300], Step [14/42], Loss: 44.8006\n",
      "Epoch [12/300], Step [15/42], Loss: 36.0934\n",
      "Epoch [12/300], Step [16/42], Loss: 36.7369\n",
      "Epoch [12/300], Step [17/42], Loss: 49.1187\n",
      "Epoch [12/300], Step [18/42], Loss: 38.6390\n",
      "Epoch [12/300], Step [19/42], Loss: 30.4104\n",
      "Epoch [12/300], Step [20/42], Loss: 34.3644\n",
      "Epoch [12/300], Step [21/42], Loss: 37.3689\n",
      "Epoch [12/300], Step [22/42], Loss: 35.1749\n",
      "Epoch [12/300], Step [23/42], Loss: 41.4264\n",
      "Epoch [12/300], Step [24/42], Loss: 37.0339\n",
      "Epoch [12/300], Step [25/42], Loss: 40.8323\n",
      "Epoch [12/300], Step [26/42], Loss: 34.7890\n",
      "Epoch [12/300], Step [27/42], Loss: 48.6236\n",
      "Epoch [12/300], Step [28/42], Loss: 40.4196\n",
      "Epoch [12/300], Step [29/42], Loss: 32.1792\n",
      "Epoch [12/300], Step [30/42], Loss: 41.4808\n",
      "Epoch [12/300], Step [31/42], Loss: 44.6999\n",
      "Epoch [12/300], Step [32/42], Loss: 30.9549\n",
      "Epoch [12/300], Step [33/42], Loss: 37.7567\n",
      "Epoch [12/300], Step [34/42], Loss: 47.2705\n",
      "Epoch [12/300], Step [35/42], Loss: 47.0782\n",
      "Epoch [12/300], Step [36/42], Loss: 41.2778\n",
      "Epoch [12/300], Step [37/42], Loss: 39.0098\n",
      "Epoch [12/300], Step [38/42], Loss: 31.6561\n",
      "Epoch [12/300], Step [39/42], Loss: 35.7011\n",
      "Epoch [12/300], Step [40/42], Loss: 28.9639\n",
      "Epoch [12/300], Step [41/42], Loss: 30.4898\n",
      "Epoch [12/300], Step [42/42], Loss: 11.0407\n",
      "Val. loss :40.2098\n",
      "Epoch [13/300], Step [1/42], Loss: 49.4378\n",
      "Epoch [13/300], Step [2/42], Loss: 30.3852\n",
      "Epoch [13/300], Step [3/42], Loss: 29.1489\n",
      "Epoch [13/300], Step [4/42], Loss: 39.6441\n",
      "Epoch [13/300], Step [5/42], Loss: 43.1920\n",
      "Epoch [13/300], Step [6/42], Loss: 42.2058\n",
      "Epoch [13/300], Step [7/42], Loss: 41.8789\n",
      "Epoch [13/300], Step [8/42], Loss: 50.2362\n",
      "Epoch [13/300], Step [9/42], Loss: 51.0161\n",
      "Epoch [13/300], Step [10/42], Loss: 31.5979\n",
      "Epoch [13/300], Step [11/42], Loss: 40.4423\n",
      "Epoch [13/300], Step [12/42], Loss: 41.3767\n",
      "Epoch [13/300], Step [13/42], Loss: 38.6408\n",
      "Epoch [13/300], Step [14/42], Loss: 36.0523\n",
      "Epoch [13/300], Step [15/42], Loss: 37.1603\n",
      "Epoch [13/300], Step [16/42], Loss: 31.6214\n",
      "Epoch [13/300], Step [17/42], Loss: 43.5427\n",
      "Epoch [13/300], Step [18/42], Loss: 30.4514\n",
      "Epoch [13/300], Step [19/42], Loss: 42.1420\n",
      "Epoch [13/300], Step [20/42], Loss: 40.2546\n",
      "Epoch [13/300], Step [21/42], Loss: 40.3967\n",
      "Epoch [13/300], Step [22/42], Loss: 29.8144\n",
      "Epoch [13/300], Step [23/42], Loss: 36.1972\n",
      "Epoch [13/300], Step [24/42], Loss: 27.4597\n",
      "Epoch [13/300], Step [25/42], Loss: 33.8248\n",
      "Epoch [13/300], Step [26/42], Loss: 28.9962\n",
      "Epoch [13/300], Step [27/42], Loss: 37.8731\n",
      "Epoch [13/300], Step [28/42], Loss: 35.7762\n",
      "Epoch [13/300], Step [29/42], Loss: 38.3992\n",
      "Epoch [13/300], Step [30/42], Loss: 44.4809\n",
      "Epoch [13/300], Step [31/42], Loss: 54.7503\n",
      "Epoch [13/300], Step [32/42], Loss: 34.9161\n",
      "Epoch [13/300], Step [33/42], Loss: 28.2809\n",
      "Epoch [13/300], Step [34/42], Loss: 44.4378\n",
      "Epoch [13/300], Step [35/42], Loss: 35.3670\n",
      "Epoch [13/300], Step [36/42], Loss: 33.4526\n",
      "Epoch [13/300], Step [37/42], Loss: 37.6818\n",
      "Epoch [13/300], Step [38/42], Loss: 39.4833\n",
      "Epoch [13/300], Step [39/42], Loss: 39.9769\n",
      "Epoch [13/300], Step [40/42], Loss: 30.1402\n",
      "Epoch [13/300], Step [41/42], Loss: 40.1319\n",
      "Epoch [13/300], Step [42/42], Loss: 9.2513\n",
      "Val. loss :34.0821\n",
      "Epoch [14/300], Step [1/42], Loss: 41.4049\n",
      "Epoch [14/300], Step [2/42], Loss: 80717.7344\n",
      "Epoch [14/300], Step [3/42], Loss: 34.1775\n",
      "Epoch [14/300], Step [4/42], Loss: 38.5592\n",
      "Epoch [14/300], Step [5/42], Loss: 51.1612\n",
      "Epoch [14/300], Step [6/42], Loss: 39.1180\n",
      "Epoch [14/300], Step [7/42], Loss: 38.5401\n",
      "Epoch [14/300], Step [8/42], Loss: 39.2775\n",
      "Epoch [14/300], Step [9/42], Loss: 36.0779\n",
      "Epoch [14/300], Step [10/42], Loss: 36.8762\n",
      "Epoch [14/300], Step [11/42], Loss: 35.1762\n",
      "Epoch [14/300], Step [12/42], Loss: 34.9179\n",
      "Epoch [14/300], Step [13/42], Loss: 37.1274\n",
      "Epoch [14/300], Step [14/42], Loss: 30.4957\n",
      "Epoch [14/300], Step [15/42], Loss: 30.8735\n",
      "Epoch [14/300], Step [16/42], Loss: 29.4553\n",
      "Epoch [14/300], Step [17/42], Loss: 42.5463\n",
      "Epoch [14/300], Step [18/42], Loss: 59.5695\n",
      "Epoch [14/300], Step [19/42], Loss: 37.0827\n",
      "Epoch [14/300], Step [20/42], Loss: 46.5548\n",
      "Epoch [14/300], Step [21/42], Loss: 41.5733\n",
      "Epoch [14/300], Step [22/42], Loss: 46.4468\n",
      "Epoch [14/300], Step [23/42], Loss: 36.1880\n",
      "Epoch [14/300], Step [24/42], Loss: 28.7144\n",
      "Epoch [14/300], Step [25/42], Loss: 37.0822\n",
      "Epoch [14/300], Step [26/42], Loss: 32.9829\n",
      "Epoch [14/300], Step [27/42], Loss: 37.7060\n",
      "Epoch [14/300], Step [28/42], Loss: 33.0749\n",
      "Epoch [14/300], Step [29/42], Loss: 51.6429\n",
      "Epoch [14/300], Step [30/42], Loss: 40.9134\n",
      "Epoch [14/300], Step [31/42], Loss: 40.5153\n",
      "Epoch [14/300], Step [32/42], Loss: 36.2527\n",
      "Epoch [14/300], Step [33/42], Loss: 46.2732\n",
      "Epoch [14/300], Step [34/42], Loss: 27.7612\n",
      "Epoch [14/300], Step [35/42], Loss: 53.1940\n",
      "Epoch [14/300], Step [36/42], Loss: 43.1414\n",
      "Epoch [14/300], Step [37/42], Loss: 39.7079\n",
      "Epoch [14/300], Step [38/42], Loss: 41.9091\n",
      "Epoch [14/300], Step [39/42], Loss: 39.2361\n",
      "Epoch [14/300], Step [40/42], Loss: 45.1223\n",
      "Epoch [14/300], Step [41/42], Loss: 35.2517\n",
      "Epoch [14/300], Step [42/42], Loss: 14.6138\n",
      "Val. loss :57.9771\n",
      "Epoch [15/300], Step [1/42], Loss: 65.8761\n",
      "Epoch [15/300], Step [2/42], Loss: 59.7604\n",
      "Epoch [15/300], Step [3/42], Loss: 51.0256\n",
      "Epoch [15/300], Step [4/42], Loss: 54.8524\n",
      "Epoch [15/300], Step [5/42], Loss: 40.8294\n",
      "Epoch [15/300], Step [6/42], Loss: 34.8071\n",
      "Epoch [15/300], Step [7/42], Loss: 40.0422\n",
      "Epoch [15/300], Step [8/42], Loss: 39.9668\n",
      "Epoch [15/300], Step [9/42], Loss: 39.9185\n",
      "Epoch [15/300], Step [10/42], Loss: 45.1155\n",
      "Epoch [15/300], Step [11/42], Loss: 41.4664\n",
      "Epoch [15/300], Step [12/42], Loss: 42.7872\n",
      "Epoch [15/300], Step [13/42], Loss: 39.5496\n",
      "Epoch [15/300], Step [14/42], Loss: 39.6342\n",
      "Epoch [15/300], Step [15/42], Loss: 37.7223\n",
      "Epoch [15/300], Step [16/42], Loss: 39.3489\n",
      "Epoch [15/300], Step [17/42], Loss: 46.6086\n",
      "Epoch [15/300], Step [18/42], Loss: 33.7633\n",
      "Epoch [15/300], Step [19/42], Loss: 44.1397\n",
      "Epoch [15/300], Step [20/42], Loss: 37.4068\n",
      "Epoch [15/300], Step [21/42], Loss: 34.2481\n",
      "Epoch [15/300], Step [22/42], Loss: 33.9950\n",
      "Epoch [15/300], Step [23/42], Loss: 37.6388\n",
      "Epoch [15/300], Step [24/42], Loss: 36.1927\n",
      "Epoch [15/300], Step [25/42], Loss: 39.1219\n",
      "Epoch [15/300], Step [26/42], Loss: 30.5149\n",
      "Epoch [15/300], Step [27/42], Loss: 42.1133\n",
      "Epoch [15/300], Step [28/42], Loss: 50.1724\n",
      "Epoch [15/300], Step [29/42], Loss: 31.2554\n",
      "Epoch [15/300], Step [30/42], Loss: 34.9032\n",
      "Epoch [15/300], Step [31/42], Loss: 26.9266\n",
      "Epoch [15/300], Step [32/42], Loss: 28.6540\n",
      "Epoch [15/300], Step [33/42], Loss: 39.6357\n",
      "Epoch [15/300], Step [34/42], Loss: 34.5290\n",
      "Epoch [15/300], Step [35/42], Loss: 35.9241\n",
      "Epoch [15/300], Step [36/42], Loss: 30.9675\n",
      "Epoch [15/300], Step [37/42], Loss: 31.6155\n",
      "Epoch [15/300], Step [38/42], Loss: 38.5991\n",
      "Epoch [15/300], Step [39/42], Loss: 31.5811\n",
      "Epoch [15/300], Step [40/42], Loss: 36.5280\n",
      "Epoch [15/300], Step [41/42], Loss: 36.8791\n",
      "Epoch [15/300], Step [42/42], Loss: 6.7048\n",
      "Val. loss :35.8640\n",
      "Epoch [16/300], Step [1/42], Loss: 34.6406\n",
      "Epoch [16/300], Step [2/42], Loss: 37.4849\n",
      "Epoch [16/300], Step [3/42], Loss: 30.4617\n",
      "Epoch [16/300], Step [4/42], Loss: 32.5887\n",
      "Epoch [16/300], Step [5/42], Loss: 32.0305\n",
      "Epoch [16/300], Step [6/42], Loss: 30.0046\n",
      "Epoch [16/300], Step [7/42], Loss: 38.1596\n",
      "Epoch [16/300], Step [8/42], Loss: 29.7356\n",
      "Epoch [16/300], Step [9/42], Loss: 26.9917\n",
      "Epoch [16/300], Step [10/42], Loss: 29.5441\n",
      "Epoch [16/300], Step [11/42], Loss: 34.4644\n",
      "Epoch [16/300], Step [12/42], Loss: 23.4687\n",
      "Epoch [16/300], Step [13/42], Loss: 23.0642\n",
      "Epoch [16/300], Step [14/42], Loss: 33.8175\n",
      "Epoch [16/300], Step [15/42], Loss: 28.3142\n",
      "Epoch [16/300], Step [16/42], Loss: 24.9694\n",
      "Epoch [16/300], Step [17/42], Loss: 28.1014\n",
      "Epoch [16/300], Step [18/42], Loss: 28.2586\n",
      "Epoch [16/300], Step [19/42], Loss: 32.0535\n",
      "Epoch [16/300], Step [20/42], Loss: 23.7676\n",
      "Epoch [16/300], Step [21/42], Loss: 26.6259\n",
      "Epoch [16/300], Step [22/42], Loss: 27.3005\n",
      "Epoch [16/300], Step [23/42], Loss: 19.8187\n",
      "Epoch [16/300], Step [24/42], Loss: 23.9658\n",
      "Epoch [16/300], Step [25/42], Loss: 30.6813\n",
      "Epoch [16/300], Step [26/42], Loss: 24.8221\n",
      "Epoch [16/300], Step [27/42], Loss: 22.9269\n",
      "Epoch [16/300], Step [28/42], Loss: 28.6621\n",
      "Epoch [16/300], Step [29/42], Loss: 28.7996\n",
      "Epoch [16/300], Step [30/42], Loss: 23.5372\n",
      "Epoch [16/300], Step [31/42], Loss: 21.5294\n",
      "Epoch [16/300], Step [32/42], Loss: 22.6408\n",
      "Epoch [16/300], Step [33/42], Loss: 25.8848\n",
      "Epoch [16/300], Step [34/42], Loss: 25.3753\n",
      "Epoch [16/300], Step [35/42], Loss: 25.4764\n",
      "Epoch [16/300], Step [36/42], Loss: 26.1549\n",
      "Epoch [16/300], Step [37/42], Loss: 23.6301\n",
      "Epoch [16/300], Step [38/42], Loss: 31.1898\n",
      "Epoch [16/300], Step [39/42], Loss: 33.8098\n",
      "Epoch [16/300], Step [40/42], Loss: 31.3634\n",
      "Epoch [16/300], Step [41/42], Loss: 45.2337\n",
      "Epoch [16/300], Step [42/42], Loss: 6.6205\n",
      "Val. loss :40.2976\n",
      "Epoch [17/300], Step [1/42], Loss: 48.6743\n",
      "Epoch [17/300], Step [2/42], Loss: 35.1713\n",
      "Epoch [17/300], Step [3/42], Loss: 44.6788\n",
      "Epoch [17/300], Step [4/42], Loss: 26.6788\n",
      "Epoch [17/300], Step [5/42], Loss: 29.9617\n",
      "Epoch [17/300], Step [6/42], Loss: 48.3959\n",
      "Epoch [17/300], Step [7/42], Loss: 37.2241\n",
      "Epoch [17/300], Step [8/42], Loss: 44.2943\n",
      "Epoch [17/300], Step [9/42], Loss: 38.1461\n",
      "Epoch [17/300], Step [10/42], Loss: 26.7302\n",
      "Epoch [17/300], Step [11/42], Loss: 30.4592\n",
      "Epoch [17/300], Step [12/42], Loss: 36.3948\n",
      "Epoch [17/300], Step [13/42], Loss: 35.2345\n",
      "Epoch [17/300], Step [14/42], Loss: 34.6663\n",
      "Epoch [17/300], Step [15/42], Loss: 31.0998\n",
      "Epoch [17/300], Step [16/42], Loss: 50.3273\n",
      "Epoch [17/300], Step [17/42], Loss: 25.1161\n",
      "Epoch [17/300], Step [18/42], Loss: 35.0666\n",
      "Epoch [17/300], Step [19/42], Loss: 39.2603\n",
      "Epoch [17/300], Step [20/42], Loss: 39.5615\n",
      "Epoch [17/300], Step [21/42], Loss: 24.1506\n",
      "Epoch [17/300], Step [22/42], Loss: 30.7110\n",
      "Epoch [17/300], Step [23/42], Loss: 34.5333\n",
      "Epoch [17/300], Step [24/42], Loss: 37.8753\n",
      "Epoch [17/300], Step [25/42], Loss: 29.8043\n",
      "Epoch [17/300], Step [26/42], Loss: 29.0287\n",
      "Epoch [17/300], Step [27/42], Loss: 28.5502\n",
      "Epoch [17/300], Step [28/42], Loss: 34.5832\n",
      "Epoch [17/300], Step [29/42], Loss: 39.9717\n",
      "Epoch [17/300], Step [30/42], Loss: 33.7309\n",
      "Epoch [17/300], Step [31/42], Loss: 27.0514\n",
      "Epoch [17/300], Step [32/42], Loss: 28.0245\n",
      "Epoch [17/300], Step [33/42], Loss: 34.9149\n",
      "Epoch [17/300], Step [34/42], Loss: 27.1491\n",
      "Epoch [17/300], Step [35/42], Loss: 23.6335\n",
      "Epoch [17/300], Step [36/42], Loss: 28.8122\n",
      "Epoch [17/300], Step [37/42], Loss: 22.5880\n",
      "Epoch [17/300], Step [38/42], Loss: 22.9909\n",
      "Epoch [17/300], Step [39/42], Loss: 22.0524\n",
      "Epoch [17/300], Step [40/42], Loss: 54.7572\n",
      "Epoch [17/300], Step [41/42], Loss: 40.6007\n",
      "Epoch [17/300], Step [42/42], Loss: 5.1768\n",
      "Val. loss :49.7398\n",
      "Epoch [18/300], Step [1/42], Loss: 55.9498\n",
      "Epoch [18/300], Step [2/42], Loss: 28.8974\n",
      "Epoch [18/300], Step [3/42], Loss: 37.5220\n",
      "Epoch [18/300], Step [4/42], Loss: 30.8986\n",
      "Epoch [18/300], Step [5/42], Loss: 38.2635\n",
      "Epoch [18/300], Step [6/42], Loss: 46.4270\n",
      "Epoch [18/300], Step [7/42], Loss: 40.2811\n",
      "Epoch [18/300], Step [8/42], Loss: 36.2636\n",
      "Epoch [18/300], Step [9/42], Loss: 37.2051\n",
      "Epoch [18/300], Step [10/42], Loss: 44.4127\n",
      "Epoch [18/300], Step [11/42], Loss: 29.8696\n",
      "Epoch [18/300], Step [12/42], Loss: 39.2396\n",
      "Epoch [18/300], Step [13/42], Loss: 38.8844\n",
      "Epoch [18/300], Step [14/42], Loss: 35.3594\n",
      "Epoch [18/300], Step [15/42], Loss: 23.5177\n",
      "Epoch [18/300], Step [16/42], Loss: 38.3759\n",
      "Epoch [18/300], Step [17/42], Loss: 32.5921\n",
      "Epoch [18/300], Step [18/42], Loss: 28.4344\n",
      "Epoch [18/300], Step [19/42], Loss: 34.7446\n",
      "Epoch [18/300], Step [20/42], Loss: 26.6958\n",
      "Epoch [18/300], Step [21/42], Loss: 49.7639\n",
      "Epoch [18/300], Step [22/42], Loss: 56.6527\n",
      "Epoch [18/300], Step [23/42], Loss: 44.2961\n",
      "Epoch [18/300], Step [24/42], Loss: 30.6641\n",
      "Epoch [18/300], Step [25/42], Loss: 31.1704\n",
      "Epoch [18/300], Step [26/42], Loss: 26.8612\n",
      "Epoch [18/300], Step [27/42], Loss: 27.5012\n",
      "Epoch [18/300], Step [28/42], Loss: 27.8967\n",
      "Epoch [18/300], Step [29/42], Loss: 31.8874\n",
      "Epoch [18/300], Step [30/42], Loss: 27.3254\n",
      "Epoch [18/300], Step [31/42], Loss: 26.0221\n",
      "Epoch [18/300], Step [32/42], Loss: 27.1812\n",
      "Epoch [18/300], Step [33/42], Loss: 23.6484\n",
      "Epoch [18/300], Step [34/42], Loss: 36.1168\n",
      "Epoch [18/300], Step [35/42], Loss: 33.5681\n",
      "Epoch [18/300], Step [36/42], Loss: 38.0897\n",
      "Epoch [18/300], Step [37/42], Loss: 31.0076\n",
      "Epoch [18/300], Step [38/42], Loss: 39.3772\n",
      "Epoch [18/300], Step [39/42], Loss: 27.0894\n",
      "Epoch [18/300], Step [40/42], Loss: 30.6258\n",
      "Epoch [18/300], Step [41/42], Loss: 31.7915\n",
      "Epoch [18/300], Step [42/42], Loss: 5.9802\n",
      "Val. loss :30.3298\n",
      "Epoch [19/300], Step [1/42], Loss: 28.9351\n",
      "Epoch [19/300], Step [2/42], Loss: 33.7965\n",
      "Epoch [19/300], Step [3/42], Loss: 28.8296\n",
      "Epoch [19/300], Step [4/42], Loss: 30.2090\n",
      "Epoch [19/300], Step [5/42], Loss: 24.9354\n",
      "Epoch [19/300], Step [6/42], Loss: 53.2426\n",
      "Epoch [19/300], Step [7/42], Loss: 43.8087\n",
      "Epoch [19/300], Step [8/42], Loss: 24.0312\n",
      "Epoch [19/300], Step [9/42], Loss: 25.4180\n",
      "Epoch [19/300], Step [10/42], Loss: 41.0210\n",
      "Epoch [19/300], Step [11/42], Loss: 46.0689\n",
      "Epoch [19/300], Step [12/42], Loss: 38.8124\n",
      "Epoch [19/300], Step [13/42], Loss: 51.3739\n",
      "Epoch [19/300], Step [14/42], Loss: 48.3634\n",
      "Epoch [19/300], Step [15/42], Loss: 44.4069\n",
      "Epoch [19/300], Step [16/42], Loss: 37.1671\n",
      "Epoch [19/300], Step [17/42], Loss: 29.1302\n",
      "Epoch [19/300], Step [18/42], Loss: 43.8017\n",
      "Epoch [19/300], Step [19/42], Loss: 39.6576\n",
      "Epoch [19/300], Step [20/42], Loss: 47.4018\n",
      "Epoch [19/300], Step [21/42], Loss: 31.6505\n",
      "Epoch [19/300], Step [22/42], Loss: 37.0269\n",
      "Epoch [19/300], Step [23/42], Loss: 33.9014\n",
      "Epoch [19/300], Step [24/42], Loss: 49.2836\n",
      "Epoch [19/300], Step [25/42], Loss: 33.9438\n",
      "Epoch [19/300], Step [26/42], Loss: 40.1985\n",
      "Epoch [19/300], Step [27/42], Loss: 32.9704\n",
      "Epoch [19/300], Step [28/42], Loss: 37.0597\n",
      "Epoch [19/300], Step [29/42], Loss: 34.3269\n",
      "Epoch [19/300], Step [30/42], Loss: 36.3409\n",
      "Epoch [19/300], Step [31/42], Loss: 39.7456\n",
      "Epoch [19/300], Step [32/42], Loss: 34.3634\n",
      "Epoch [19/300], Step [33/42], Loss: 37.4810\n",
      "Epoch [19/300], Step [34/42], Loss: 36.2102\n",
      "Epoch [19/300], Step [35/42], Loss: 31.2637\n",
      "Epoch [19/300], Step [36/42], Loss: 30.3548\n",
      "Epoch [19/300], Step [37/42], Loss: 27.8038\n",
      "Epoch [19/300], Step [38/42], Loss: 34.6024\n",
      "Epoch [19/300], Step [39/42], Loss: 31.7835\n",
      "Epoch [19/300], Step [40/42], Loss: 25.5510\n",
      "Epoch [19/300], Step [41/42], Loss: 27.0313\n",
      "Epoch [19/300], Step [42/42], Loss: 5.9420\n",
      "Val. loss :30.5664\n",
      "Epoch [20/300], Step [1/42], Loss: 48.6827\n",
      "Epoch [20/300], Step [2/42], Loss: 27.2828\n",
      "Epoch [20/300], Step [3/42], Loss: 31.6953\n",
      "Epoch [20/300], Step [4/42], Loss: 33.6725\n",
      "Epoch [20/300], Step [5/42], Loss: 22.3857\n",
      "Epoch [20/300], Step [6/42], Loss: 23.5024\n",
      "Epoch [20/300], Step [7/42], Loss: 27.7684\n",
      "Epoch [20/300], Step [8/42], Loss: 27.7115\n",
      "Epoch [20/300], Step [9/42], Loss: 26.5907\n",
      "Epoch [20/300], Step [10/42], Loss: 24.3685\n",
      "Epoch [20/300], Step [11/42], Loss: 27.6119\n",
      "Epoch [20/300], Step [12/42], Loss: 24.4042\n",
      "Epoch [20/300], Step [13/42], Loss: 23.5755\n",
      "Epoch [20/300], Step [14/42], Loss: 26.1614\n",
      "Epoch [20/300], Step [15/42], Loss: 25.7638\n",
      "Epoch [20/300], Step [16/42], Loss: 24.0845\n",
      "Epoch [20/300], Step [17/42], Loss: 26.7030\n",
      "Epoch [20/300], Step [18/42], Loss: 23.6540\n",
      "Epoch [20/300], Step [19/42], Loss: 20.3102\n",
      "Epoch [20/300], Step [20/42], Loss: 28.1010\n",
      "Epoch [20/300], Step [21/42], Loss: 26.1421\n",
      "Epoch [20/300], Step [22/42], Loss: 22.8430\n",
      "Epoch [20/300], Step [23/42], Loss: 21.2341\n",
      "Epoch [20/300], Step [24/42], Loss: 26.4698\n",
      "Epoch [20/300], Step [25/42], Loss: 22.8767\n",
      "Epoch [20/300], Step [26/42], Loss: 24.1138\n",
      "Epoch [20/300], Step [27/42], Loss: 21.7295\n",
      "Epoch [20/300], Step [28/42], Loss: 23.5756\n",
      "Epoch [20/300], Step [29/42], Loss: 22.5456\n",
      "Epoch [20/300], Step [30/42], Loss: 25.6652\n",
      "Epoch [20/300], Step [31/42], Loss: 24.1706\n",
      "Epoch [20/300], Step [32/42], Loss: 26.1545\n",
      "Epoch [20/300], Step [33/42], Loss: 25.2006\n",
      "Epoch [20/300], Step [34/42], Loss: 20.3012\n",
      "Epoch [20/300], Step [35/42], Loss: 28.3618\n",
      "Epoch [20/300], Step [36/42], Loss: 27.3917\n",
      "Epoch [20/300], Step [37/42], Loss: 26.9789\n",
      "Epoch [20/300], Step [38/42], Loss: 20.4650\n",
      "Epoch [20/300], Step [39/42], Loss: 20.9506\n",
      "Epoch [20/300], Step [40/42], Loss: 25.0434\n",
      "Epoch [20/300], Step [41/42], Loss: 22.5661\n",
      "Epoch [20/300], Step [42/42], Loss: 3.8501\n",
      "Val. loss :41.0289\n",
      "Epoch [21/300], Step [1/42], Loss: 40.9465\n",
      "Epoch [21/300], Step [2/42], Loss: 20.0757\n",
      "Epoch [21/300], Step [3/42], Loss: 33.8742\n",
      "Epoch [21/300], Step [4/42], Loss: 25.8795\n",
      "Epoch [21/300], Step [5/42], Loss: 32.3006\n",
      "Epoch [21/300], Step [6/42], Loss: 29.8829\n",
      "Epoch [21/300], Step [7/42], Loss: 25.6275\n",
      "Epoch [21/300], Step [8/42], Loss: 27.8173\n",
      "Epoch [21/300], Step [9/42], Loss: 28.2684\n",
      "Epoch [21/300], Step [10/42], Loss: 33.8056\n",
      "Epoch [21/300], Step [11/42], Loss: 32.5667\n",
      "Epoch [21/300], Step [12/42], Loss: 34.3633\n",
      "Epoch [21/300], Step [13/42], Loss: 36.2894\n",
      "Epoch [21/300], Step [14/42], Loss: 26.8451\n",
      "Epoch [21/300], Step [15/42], Loss: 27.4971\n",
      "Epoch [21/300], Step [16/42], Loss: 24.8862\n",
      "Epoch [21/300], Step [17/42], Loss: 24.3230\n",
      "Epoch [21/300], Step [18/42], Loss: 47.3088\n",
      "Epoch [21/300], Step [19/42], Loss: 20.9220\n",
      "Epoch [21/300], Step [20/42], Loss: 23.1577\n",
      "Epoch [21/300], Step [21/42], Loss: 40.0823\n",
      "Epoch [21/300], Step [22/42], Loss: 29.0935\n",
      "Epoch [21/300], Step [23/42], Loss: 42.1632\n",
      "Epoch [21/300], Step [24/42], Loss: 24.2833\n",
      "Epoch [21/300], Step [25/42], Loss: 30.8739\n",
      "Epoch [21/300], Step [26/42], Loss: 25.0078\n",
      "Epoch [21/300], Step [27/42], Loss: 24.6620\n",
      "Epoch [21/300], Step [28/42], Loss: 24.8142\n",
      "Epoch [21/300], Step [29/42], Loss: 54.6919\n",
      "Epoch [21/300], Step [30/42], Loss: 18.7342\n",
      "Epoch [21/300], Step [31/42], Loss: 23.6183\n",
      "Epoch [21/300], Step [32/42], Loss: 22.6239\n",
      "Epoch [21/300], Step [33/42], Loss: 27.2486\n",
      "Epoch [21/300], Step [34/42], Loss: 31.0904\n",
      "Epoch [21/300], Step [35/42], Loss: 20.5072\n",
      "Epoch [21/300], Step [36/42], Loss: 31.7674\n",
      "Epoch [21/300], Step [37/42], Loss: 24.5194\n",
      "Epoch [21/300], Step [38/42], Loss: 25.4778\n",
      "Epoch [21/300], Step [39/42], Loss: 20.6544\n",
      "Epoch [21/300], Step [40/42], Loss: 24.7307\n",
      "Epoch [21/300], Step [41/42], Loss: 25.0299\n",
      "Epoch [21/300], Step [42/42], Loss: 7.6079\n",
      "Val. loss :27.4199\n",
      "Epoch [22/300], Step [1/42], Loss: 25.4939\n",
      "Epoch [22/300], Step [2/42], Loss: 22.4824\n",
      "Epoch [22/300], Step [3/42], Loss: 29.1834\n",
      "Epoch [22/300], Step [4/42], Loss: 24.8081\n",
      "Epoch [22/300], Step [5/42], Loss: 21.3766\n",
      "Epoch [22/300], Step [6/42], Loss: 40.5210\n",
      "Epoch [22/300], Step [7/42], Loss: 46.1385\n",
      "Epoch [22/300], Step [8/42], Loss: 19.6243\n",
      "Epoch [22/300], Step [9/42], Loss: 25.6216\n",
      "Epoch [22/300], Step [10/42], Loss: 21.8938\n",
      "Epoch [22/300], Step [11/42], Loss: 24.4243\n",
      "Epoch [22/300], Step [12/42], Loss: 24.3769\n",
      "Epoch [22/300], Step [13/42], Loss: 21.3909\n",
      "Epoch [22/300], Step [14/42], Loss: 29.1872\n",
      "Epoch [22/300], Step [15/42], Loss: 25.9177\n",
      "Epoch [22/300], Step [16/42], Loss: 22.6594\n",
      "Epoch [22/300], Step [17/42], Loss: 29.0838\n",
      "Epoch [22/300], Step [18/42], Loss: 17.4691\n",
      "Epoch [22/300], Step [19/42], Loss: 20.3528\n",
      "Epoch [22/300], Step [20/42], Loss: 15.6050\n",
      "Epoch [22/300], Step [21/42], Loss: 18.8950\n",
      "Epoch [22/300], Step [22/42], Loss: 22.4587\n",
      "Epoch [22/300], Step [23/42], Loss: 19.4181\n",
      "Epoch [22/300], Step [24/42], Loss: 19.4282\n",
      "Epoch [22/300], Step [25/42], Loss: 25.9019\n",
      "Epoch [22/300], Step [26/42], Loss: 22.5059\n",
      "Epoch [22/300], Step [27/42], Loss: 20.3421\n",
      "Epoch [22/300], Step [28/42], Loss: 21.3086\n",
      "Epoch [22/300], Step [29/42], Loss: 17.9837\n",
      "Epoch [22/300], Step [30/42], Loss: 23.0529\n",
      "Epoch [22/300], Step [31/42], Loss: 20.6129\n",
      "Epoch [22/300], Step [32/42], Loss: 22.1559\n",
      "Epoch [22/300], Step [33/42], Loss: 23.4362\n",
      "Epoch [22/300], Step [34/42], Loss: 20.0011\n",
      "Epoch [22/300], Step [35/42], Loss: 25.2697\n",
      "Epoch [22/300], Step [36/42], Loss: 25.8196\n",
      "Epoch [22/300], Step [37/42], Loss: 21.2645\n",
      "Epoch [22/300], Step [38/42], Loss: 19.0439\n",
      "Epoch [22/300], Step [39/42], Loss: 19.0443\n",
      "Epoch [22/300], Step [40/42], Loss: 18.6465\n",
      "Epoch [22/300], Step [41/42], Loss: 21.8691\n",
      "Epoch [22/300], Step [42/42], Loss: 5.3346\n",
      "Val. loss :22.8257\n",
      "Epoch [23/300], Step [1/42], Loss: 19.2508\n",
      "Epoch [23/300], Step [2/42], Loss: 20.8989\n",
      "Epoch [23/300], Step [3/42], Loss: 28.9594\n",
      "Epoch [23/300], Step [4/42], Loss: 18.3277\n",
      "Epoch [23/300], Step [5/42], Loss: 20.4460\n",
      "Epoch [23/300], Step [6/42], Loss: 16.6489\n",
      "Epoch [23/300], Step [7/42], Loss: 27.5012\n",
      "Epoch [23/300], Step [8/42], Loss: 20.3130\n",
      "Epoch [23/300], Step [9/42], Loss: 23.4052\n",
      "Epoch [23/300], Step [10/42], Loss: 21.8392\n",
      "Epoch [23/300], Step [11/42], Loss: 24.3034\n",
      "Epoch [23/300], Step [12/42], Loss: 22.4664\n",
      "Epoch [23/300], Step [13/42], Loss: 21.5142\n",
      "Epoch [23/300], Step [14/42], Loss: 20.3609\n",
      "Epoch [23/300], Step [15/42], Loss: 23.2972\n",
      "Epoch [23/300], Step [16/42], Loss: 18.5410\n",
      "Epoch [23/300], Step [17/42], Loss: 21.4302\n",
      "Epoch [23/300], Step [18/42], Loss: 22.3147\n",
      "Epoch [23/300], Step [19/42], Loss: 17.8868\n",
      "Epoch [23/300], Step [20/42], Loss: 20.7509\n",
      "Epoch [23/300], Step [21/42], Loss: 22.9664\n",
      "Epoch [23/300], Step [22/42], Loss: 17.6202\n",
      "Epoch [23/300], Step [23/42], Loss: 19.3115\n",
      "Epoch [23/300], Step [24/42], Loss: 16.4783\n",
      "Epoch [23/300], Step [25/42], Loss: 19.1783\n",
      "Epoch [23/300], Step [26/42], Loss: 17.2648\n",
      "Epoch [23/300], Step [27/42], Loss: 20.8333\n",
      "Epoch [23/300], Step [28/42], Loss: 19.5045\n",
      "Epoch [23/300], Step [29/42], Loss: 16.2285\n",
      "Epoch [23/300], Step [30/42], Loss: 17.4040\n",
      "Epoch [23/300], Step [31/42], Loss: 23.1229\n",
      "Epoch [23/300], Step [32/42], Loss: 21.5207\n",
      "Epoch [23/300], Step [33/42], Loss: 19.2351\n",
      "Epoch [23/300], Step [34/42], Loss: 17.3979\n",
      "Epoch [23/300], Step [35/42], Loss: 26.0122\n",
      "Epoch [23/300], Step [36/42], Loss: 20.8678\n",
      "Epoch [23/300], Step [37/42], Loss: 24.0143\n",
      "Epoch [23/300], Step [38/42], Loss: 21.1450\n",
      "Epoch [23/300], Step [39/42], Loss: 24.8351\n",
      "Epoch [23/300], Step [40/42], Loss: 15.9039\n",
      "Epoch [23/300], Step [41/42], Loss: 16.5222\n",
      "Epoch [23/300], Step [42/42], Loss: 5.3519\n",
      "Val. loss :21.1288\n",
      "Epoch [24/300], Step [1/42], Loss: 19.9594\n",
      "Epoch [24/300], Step [2/42], Loss: 18.7282\n",
      "Epoch [24/300], Step [3/42], Loss: 21.6537\n",
      "Epoch [24/300], Step [4/42], Loss: 18.8434\n",
      "Epoch [24/300], Step [5/42], Loss: 19.7771\n",
      "Epoch [24/300], Step [6/42], Loss: 17.1417\n",
      "Epoch [24/300], Step [7/42], Loss: 20.5946\n",
      "Epoch [24/300], Step [8/42], Loss: 24.1317\n",
      "Epoch [24/300], Step [9/42], Loss: 23.4475\n",
      "Epoch [24/300], Step [10/42], Loss: 22.7533\n",
      "Epoch [24/300], Step [11/42], Loss: 14.2377\n",
      "Epoch [24/300], Step [12/42], Loss: 20.1693\n",
      "Epoch [24/300], Step [13/42], Loss: 19.2563\n",
      "Epoch [24/300], Step [14/42], Loss: 25.6041\n",
      "Epoch [24/300], Step [15/42], Loss: 19.9178\n",
      "Epoch [24/300], Step [16/42], Loss: 20.3853\n",
      "Epoch [24/300], Step [17/42], Loss: 18.7601\n",
      "Epoch [24/300], Step [18/42], Loss: 16.7020\n",
      "Epoch [24/300], Step [19/42], Loss: 19.6400\n",
      "Epoch [24/300], Step [20/42], Loss: 19.0448\n",
      "Epoch [24/300], Step [21/42], Loss: 15.9913\n",
      "Epoch [24/300], Step [22/42], Loss: 17.9168\n",
      "Epoch [24/300], Step [23/42], Loss: 26.3719\n",
      "Epoch [24/300], Step [24/42], Loss: 17.9422\n",
      "Epoch [24/300], Step [25/42], Loss: 14.2085\n",
      "Epoch [24/300], Step [26/42], Loss: 19.1421\n",
      "Epoch [24/300], Step [27/42], Loss: 22.2667\n",
      "Epoch [24/300], Step [28/42], Loss: 16.4387\n",
      "Epoch [24/300], Step [29/42], Loss: 14.1130\n",
      "Epoch [24/300], Step [30/42], Loss: 26.9927\n",
      "Epoch [24/300], Step [31/42], Loss: 15.9835\n",
      "Epoch [24/300], Step [32/42], Loss: 23.2338\n",
      "Epoch [24/300], Step [33/42], Loss: 14.9850\n",
      "Epoch [24/300], Step [34/42], Loss: 24.6287\n",
      "Epoch [24/300], Step [35/42], Loss: 16.9372\n",
      "Epoch [24/300], Step [36/42], Loss: 19.3691\n",
      "Epoch [24/300], Step [37/42], Loss: 22.9757\n",
      "Epoch [24/300], Step [38/42], Loss: 17.7175\n",
      "Epoch [24/300], Step [39/42], Loss: 18.5965\n",
      "Epoch [24/300], Step [40/42], Loss: 25.6604\n",
      "Epoch [24/300], Step [41/42], Loss: 18.4090\n",
      "Epoch [24/300], Step [42/42], Loss: 4.9408\n",
      "Val. loss :20.0471\n",
      "Epoch [25/300], Step [1/42], Loss: 18.9807\n",
      "Epoch [25/300], Step [2/42], Loss: 18.2225\n",
      "Epoch [25/300], Step [3/42], Loss: 17.3742\n",
      "Epoch [25/300], Step [4/42], Loss: 18.4955\n",
      "Epoch [25/300], Step [5/42], Loss: 16.4453\n",
      "Epoch [25/300], Step [6/42], Loss: 19.1189\n",
      "Epoch [25/300], Step [7/42], Loss: 23.8667\n",
      "Epoch [25/300], Step [8/42], Loss: 23.1022\n",
      "Epoch [25/300], Step [9/42], Loss: 18.0864\n",
      "Epoch [25/300], Step [10/42], Loss: 16.1418\n",
      "Epoch [25/300], Step [11/42], Loss: 16.9198\n",
      "Epoch [25/300], Step [12/42], Loss: 22.7662\n",
      "Epoch [25/300], Step [13/42], Loss: 15.8238\n",
      "Epoch [25/300], Step [14/42], Loss: 18.4916\n",
      "Epoch [25/300], Step [15/42], Loss: 20.5379\n",
      "Epoch [25/300], Step [16/42], Loss: 22.0009\n",
      "Epoch [25/300], Step [17/42], Loss: 20.6961\n",
      "Epoch [25/300], Step [18/42], Loss: 19.8976\n",
      "Epoch [25/300], Step [19/42], Loss: 14.7068\n",
      "Epoch [25/300], Step [20/42], Loss: 21.9140\n",
      "Epoch [25/300], Step [21/42], Loss: 21.4881\n",
      "Epoch [25/300], Step [22/42], Loss: 17.3687\n",
      "Epoch [25/300], Step [23/42], Loss: 17.4362\n",
      "Epoch [25/300], Step [24/42], Loss: 18.9864\n",
      "Epoch [25/300], Step [25/42], Loss: 16.1586\n",
      "Epoch [25/300], Step [26/42], Loss: 20.0183\n",
      "Epoch [25/300], Step [27/42], Loss: 15.8813\n",
      "Epoch [25/300], Step [28/42], Loss: 17.3395\n",
      "Epoch [25/300], Step [29/42], Loss: 15.7417\n",
      "Epoch [25/300], Step [30/42], Loss: 20.5159\n",
      "Epoch [25/300], Step [31/42], Loss: 17.5743\n",
      "Epoch [25/300], Step [32/42], Loss: 17.4825\n",
      "Epoch [25/300], Step [33/42], Loss: 16.6823\n",
      "Epoch [25/300], Step [34/42], Loss: 18.9204\n",
      "Epoch [25/300], Step [35/42], Loss: 13.3475\n",
      "Epoch [25/300], Step [36/42], Loss: 22.8853\n",
      "Epoch [25/300], Step [37/42], Loss: 18.5260\n",
      "Epoch [25/300], Step [38/42], Loss: 20.6533\n",
      "Epoch [25/300], Step [39/42], Loss: 14.6596\n",
      "Epoch [25/300], Step [40/42], Loss: 18.2915\n",
      "Epoch [25/300], Step [41/42], Loss: 22.1794\n",
      "Epoch [25/300], Step [42/42], Loss: 4.5143\n",
      "Val. loss :19.6977\n",
      "Epoch [26/300], Step [1/42], Loss: 21.1515\n",
      "Epoch [26/300], Step [2/42], Loss: 20.1377\n",
      "Epoch [26/300], Step [3/42], Loss: 15.9556\n",
      "Epoch [26/300], Step [4/42], Loss: 18.4041\n",
      "Epoch [26/300], Step [5/42], Loss: 18.9591\n",
      "Epoch [26/300], Step [6/42], Loss: 18.7511\n",
      "Epoch [26/300], Step [7/42], Loss: 16.8832\n",
      "Epoch [26/300], Step [8/42], Loss: 16.6540\n",
      "Epoch [26/300], Step [9/42], Loss: 16.6674\n",
      "Epoch [26/300], Step [10/42], Loss: 18.1725\n",
      "Epoch [26/300], Step [11/42], Loss: 18.8467\n",
      "Epoch [26/300], Step [12/42], Loss: 17.7239\n",
      "Epoch [26/300], Step [13/42], Loss: 15.5552\n",
      "Epoch [26/300], Step [14/42], Loss: 23.9767\n",
      "Epoch [26/300], Step [15/42], Loss: 17.8431\n",
      "Epoch [26/300], Step [16/42], Loss: 15.9639\n",
      "Epoch [26/300], Step [17/42], Loss: 13.4376\n",
      "Epoch [26/300], Step [18/42], Loss: 18.5985\n",
      "Epoch [26/300], Step [19/42], Loss: 16.8011\n",
      "Epoch [26/300], Step [20/42], Loss: 16.4108\n",
      "Epoch [26/300], Step [21/42], Loss: 21.1057\n",
      "Epoch [26/300], Step [22/42], Loss: 24.7089\n",
      "Epoch [26/300], Step [23/42], Loss: 19.5421\n",
      "Epoch [26/300], Step [24/42], Loss: 15.1532\n",
      "Epoch [26/300], Step [25/42], Loss: 22.2769\n",
      "Epoch [26/300], Step [26/42], Loss: 17.1207\n",
      "Epoch [26/300], Step [27/42], Loss: 16.9948\n",
      "Epoch [26/300], Step [28/42], Loss: 14.6463\n",
      "Epoch [26/300], Step [29/42], Loss: 19.5926\n",
      "Epoch [26/300], Step [30/42], Loss: 17.9134\n",
      "Epoch [26/300], Step [31/42], Loss: 19.7383\n",
      "Epoch [26/300], Step [32/42], Loss: 16.4250\n",
      "Epoch [26/300], Step [33/42], Loss: 17.6355\n",
      "Epoch [26/300], Step [34/42], Loss: 17.5852\n",
      "Epoch [26/300], Step [35/42], Loss: 15.2171\n",
      "Epoch [26/300], Step [36/42], Loss: 20.4103\n",
      "Epoch [26/300], Step [37/42], Loss: 20.3640\n",
      "Epoch [26/300], Step [38/42], Loss: 17.2558\n",
      "Epoch [26/300], Step [39/42], Loss: 12.8118\n",
      "Epoch [26/300], Step [40/42], Loss: 15.6792\n",
      "Epoch [26/300], Step [41/42], Loss: 15.1452\n",
      "Epoch [26/300], Step [42/42], Loss: 4.4685\n",
      "Val. loss :18.5395\n",
      "Epoch [27/300], Step [1/42], Loss: 14.0866\n",
      "Epoch [27/300], Step [2/42], Loss: 15.0254\n",
      "Epoch [27/300], Step [3/42], Loss: 17.1195\n",
      "Epoch [27/300], Step [4/42], Loss: 15.4502\n",
      "Epoch [27/300], Step [5/42], Loss: 16.6342\n",
      "Epoch [27/300], Step [6/42], Loss: 15.2182\n",
      "Epoch [27/300], Step [7/42], Loss: 19.5504\n",
      "Epoch [27/300], Step [8/42], Loss: 17.4311\n",
      "Epoch [27/300], Step [9/42], Loss: 21.1567\n",
      "Epoch [27/300], Step [10/42], Loss: 17.2972\n",
      "Epoch [27/300], Step [11/42], Loss: 15.1147\n",
      "Epoch [27/300], Step [12/42], Loss: 16.2777\n",
      "Epoch [27/300], Step [13/42], Loss: 14.6290\n",
      "Epoch [27/300], Step [14/42], Loss: 16.4422\n",
      "Epoch [27/300], Step [15/42], Loss: 17.6496\n",
      "Epoch [27/300], Step [16/42], Loss: 20.0702\n",
      "Epoch [27/300], Step [17/42], Loss: 20.6022\n",
      "Epoch [27/300], Step [18/42], Loss: 15.7608\n",
      "Epoch [27/300], Step [19/42], Loss: 15.8552\n",
      "Epoch [27/300], Step [20/42], Loss: 22.3178\n",
      "Epoch [27/300], Step [21/42], Loss: 12.6773\n",
      "Epoch [27/300], Step [22/42], Loss: 17.6595\n",
      "Epoch [27/300], Step [23/42], Loss: 17.0344\n",
      "Epoch [27/300], Step [24/42], Loss: 15.5398\n",
      "Epoch [27/300], Step [25/42], Loss: 19.3886\n",
      "Epoch [27/300], Step [26/42], Loss: 18.9140\n",
      "Epoch [27/300], Step [27/42], Loss: 16.1574\n",
      "Epoch [27/300], Step [28/42], Loss: 17.0796\n",
      "Epoch [27/300], Step [29/42], Loss: 16.6260\n",
      "Epoch [27/300], Step [30/42], Loss: 13.2767\n",
      "Epoch [27/300], Step [31/42], Loss: 17.7954\n",
      "Epoch [27/300], Step [32/42], Loss: 13.0309\n",
      "Epoch [27/300], Step [33/42], Loss: 18.3273\n",
      "Epoch [27/300], Step [34/42], Loss: 18.4908\n",
      "Epoch [27/300], Step [35/42], Loss: 15.6028\n",
      "Epoch [27/300], Step [36/42], Loss: 18.3480\n",
      "Epoch [27/300], Step [37/42], Loss: 16.0142\n",
      "Epoch [27/300], Step [38/42], Loss: 13.3471\n",
      "Epoch [27/300], Step [39/42], Loss: 15.6092\n",
      "Epoch [27/300], Step [40/42], Loss: 15.8593\n",
      "Epoch [27/300], Step [41/42], Loss: 15.3188\n",
      "Epoch [27/300], Step [42/42], Loss: 3.9416\n",
      "Val. loss :16.9924\n",
      "Epoch [28/300], Step [1/42], Loss: 13.2902\n",
      "Epoch [28/300], Step [2/42], Loss: 17.6054\n",
      "Epoch [28/300], Step [3/42], Loss: 19.9140\n",
      "Epoch [28/300], Step [4/42], Loss: 14.1645\n",
      "Epoch [28/300], Step [5/42], Loss: 16.1848\n",
      "Epoch [28/300], Step [6/42], Loss: 12.7131\n",
      "Epoch [28/300], Step [7/42], Loss: 19.1096\n",
      "Epoch [28/300], Step [8/42], Loss: 14.1281\n",
      "Epoch [28/300], Step [9/42], Loss: 16.2802\n",
      "Epoch [28/300], Step [10/42], Loss: 15.3828\n",
      "Epoch [28/300], Step [11/42], Loss: 9.5936\n",
      "Epoch [28/300], Step [12/42], Loss: 12.6988\n",
      "Epoch [28/300], Step [13/42], Loss: 15.7174\n",
      "Epoch [28/300], Step [14/42], Loss: 18.1441\n",
      "Epoch [28/300], Step [15/42], Loss: 14.1331\n",
      "Epoch [28/300], Step [16/42], Loss: 12.0630\n",
      "Epoch [28/300], Step [17/42], Loss: 15.8388\n",
      "Epoch [28/300], Step [18/42], Loss: 19.1102\n",
      "Epoch [28/300], Step [19/42], Loss: 23.0112\n",
      "Epoch [28/300], Step [20/42], Loss: 14.5879\n",
      "Epoch [28/300], Step [21/42], Loss: 12.1252\n",
      "Epoch [28/300], Step [22/42], Loss: 11.9934\n",
      "Epoch [28/300], Step [23/42], Loss: 18.5433\n",
      "Epoch [28/300], Step [24/42], Loss: 15.8493\n",
      "Epoch [28/300], Step [25/42], Loss: 11.3165\n",
      "Epoch [28/300], Step [26/42], Loss: 20.3057\n",
      "Epoch [28/300], Step [27/42], Loss: 14.6740\n",
      "Epoch [28/300], Step [28/42], Loss: 18.4915\n",
      "Epoch [28/300], Step [29/42], Loss: 15.7977\n",
      "Epoch [28/300], Step [30/42], Loss: 15.8136\n",
      "Epoch [28/300], Step [31/42], Loss: 16.3408\n",
      "Epoch [28/300], Step [32/42], Loss: 16.7793\n",
      "Epoch [28/300], Step [33/42], Loss: 16.8967\n",
      "Epoch [28/300], Step [34/42], Loss: 17.9586\n",
      "Epoch [28/300], Step [35/42], Loss: 17.4471\n",
      "Epoch [28/300], Step [36/42], Loss: 19.9052\n",
      "Epoch [28/300], Step [37/42], Loss: 20.4910\n",
      "Epoch [28/300], Step [38/42], Loss: 12.6416\n",
      "Epoch [28/300], Step [39/42], Loss: 16.2885\n",
      "Epoch [28/300], Step [40/42], Loss: 15.9052\n",
      "Epoch [28/300], Step [41/42], Loss: 14.2563\n",
      "Epoch [28/300], Step [42/42], Loss: 3.6479\n",
      "Val. loss :16.2119\n",
      "Epoch [29/300], Step [1/42], Loss: 15.9118\n",
      "Epoch [29/300], Step [2/42], Loss: 10.7063\n",
      "Epoch [29/300], Step [3/42], Loss: 12.1841\n",
      "Epoch [29/300], Step [4/42], Loss: 10.9586\n",
      "Epoch [29/300], Step [5/42], Loss: 18.3191\n",
      "Epoch [29/300], Step [6/42], Loss: 9.3036\n",
      "Epoch [29/300], Step [7/42], Loss: 13.6951\n",
      "Epoch [29/300], Step [8/42], Loss: 16.9369\n",
      "Epoch [29/300], Step [9/42], Loss: 16.5088\n",
      "Epoch [29/300], Step [10/42], Loss: 12.4054\n",
      "Epoch [29/300], Step [11/42], Loss: 17.0525\n",
      "Epoch [29/300], Step [12/42], Loss: 16.1342\n",
      "Epoch [29/300], Step [13/42], Loss: 14.6498\n",
      "Epoch [29/300], Step [14/42], Loss: 14.8166\n",
      "Epoch [29/300], Step [15/42], Loss: 16.3094\n",
      "Epoch [29/300], Step [16/42], Loss: 14.4076\n",
      "Epoch [29/300], Step [17/42], Loss: 12.1902\n",
      "Epoch [29/300], Step [18/42], Loss: 14.7930\n",
      "Epoch [29/300], Step [19/42], Loss: 13.6093\n",
      "Epoch [29/300], Step [20/42], Loss: 13.2578\n",
      "Epoch [29/300], Step [21/42], Loss: 18.0679\n",
      "Epoch [29/300], Step [22/42], Loss: 15.4748\n",
      "Epoch [29/300], Step [23/42], Loss: 16.6338\n",
      "Epoch [29/300], Step [24/42], Loss: 11.7279\n",
      "Epoch [29/300], Step [25/42], Loss: 16.5531\n",
      "Epoch [29/300], Step [26/42], Loss: 15.1460\n",
      "Epoch [29/300], Step [27/42], Loss: 19.1369\n",
      "Epoch [29/300], Step [28/42], Loss: 16.2820\n",
      "Epoch [29/300], Step [29/42], Loss: 14.9772\n",
      "Epoch [29/300], Step [30/42], Loss: 19.8936\n",
      "Epoch [29/300], Step [31/42], Loss: 13.2298\n",
      "Epoch [29/300], Step [32/42], Loss: 10.0861\n",
      "Epoch [29/300], Step [33/42], Loss: 14.6585\n",
      "Epoch [29/300], Step [34/42], Loss: 13.7612\n",
      "Epoch [29/300], Step [35/42], Loss: 13.6235\n",
      "Epoch [29/300], Step [36/42], Loss: 14.3222\n",
      "Epoch [29/300], Step [37/42], Loss: 11.1134\n",
      "Epoch [29/300], Step [38/42], Loss: 16.2062\n",
      "Epoch [29/300], Step [39/42], Loss: 19.4966\n",
      "Epoch [29/300], Step [40/42], Loss: 12.4111\n",
      "Epoch [29/300], Step [41/42], Loss: 13.4601\n",
      "Epoch [29/300], Step [42/42], Loss: 3.6023\n",
      "Val. loss :14.9727\n",
      "Epoch [30/300], Step [1/42], Loss: 12.1494\n",
      "Epoch [30/300], Step [2/42], Loss: 17.1580\n",
      "Epoch [30/300], Step [3/42], Loss: 10.2914\n",
      "Epoch [30/300], Step [4/42], Loss: 12.9358\n",
      "Epoch [30/300], Step [5/42], Loss: 10.1017\n",
      "Epoch [30/300], Step [6/42], Loss: 11.8755\n",
      "Epoch [30/300], Step [7/42], Loss: 15.2772\n",
      "Epoch [30/300], Step [8/42], Loss: 13.7959\n",
      "Epoch [30/300], Step [9/42], Loss: 12.7820\n",
      "Epoch [30/300], Step [10/42], Loss: 16.8377\n",
      "Epoch [30/300], Step [11/42], Loss: 18.9470\n",
      "Epoch [30/300], Step [12/42], Loss: 11.9408\n",
      "Epoch [30/300], Step [13/42], Loss: 14.5551\n",
      "Epoch [30/300], Step [14/42], Loss: 13.7237\n",
      "Epoch [30/300], Step [15/42], Loss: 15.7636\n",
      "Epoch [30/300], Step [16/42], Loss: 11.1372\n",
      "Epoch [30/300], Step [17/42], Loss: 11.8988\n",
      "Epoch [30/300], Step [18/42], Loss: 13.8583\n",
      "Epoch [30/300], Step [19/42], Loss: 14.2975\n",
      "Epoch [30/300], Step [20/42], Loss: 16.1009\n",
      "Epoch [30/300], Step [21/42], Loss: 14.5209\n",
      "Epoch [30/300], Step [22/42], Loss: 14.2885\n",
      "Epoch [30/300], Step [23/42], Loss: 11.6910\n",
      "Epoch [30/300], Step [24/42], Loss: 10.5929\n",
      "Epoch [30/300], Step [25/42], Loss: 11.6929\n",
      "Epoch [30/300], Step [26/42], Loss: 14.2320\n",
      "Epoch [30/300], Step [27/42], Loss: 16.9875\n",
      "Epoch [30/300], Step [28/42], Loss: 14.6049\n",
      "Epoch [30/300], Step [29/42], Loss: 12.9479\n",
      "Epoch [30/300], Step [30/42], Loss: 14.3932\n",
      "Epoch [30/300], Step [31/42], Loss: 18.2320\n",
      "Epoch [30/300], Step [32/42], Loss: 9.7857\n",
      "Epoch [30/300], Step [33/42], Loss: 15.7688\n",
      "Epoch [30/300], Step [34/42], Loss: 12.4770\n",
      "Epoch [30/300], Step [35/42], Loss: 15.1561\n",
      "Epoch [30/300], Step [36/42], Loss: 14.6475\n",
      "Epoch [30/300], Step [37/42], Loss: 12.3967\n",
      "Epoch [30/300], Step [38/42], Loss: 14.4203\n",
      "Epoch [30/300], Step [39/42], Loss: 17.7849\n",
      "Epoch [30/300], Step [40/42], Loss: 11.7407\n",
      "Epoch [30/300], Step [41/42], Loss: 10.7160\n",
      "Epoch [30/300], Step [42/42], Loss: 1.8120\n",
      "Val. loss :14.2683\n",
      "Epoch [31/300], Step [1/42], Loss: 14.8136\n",
      "Epoch [31/300], Step [2/42], Loss: 15.4587\n",
      "Epoch [31/300], Step [3/42], Loss: 12.3453\n",
      "Epoch [31/300], Step [4/42], Loss: 10.5397\n",
      "Epoch [31/300], Step [5/42], Loss: 15.4339\n",
      "Epoch [31/300], Step [6/42], Loss: 13.2881\n",
      "Epoch [31/300], Step [7/42], Loss: 14.2120\n",
      "Epoch [31/300], Step [8/42], Loss: 12.5812\n",
      "Epoch [31/300], Step [9/42], Loss: 13.8207\n",
      "Epoch [31/300], Step [10/42], Loss: 10.1404\n",
      "Epoch [31/300], Step [11/42], Loss: 9.8018\n",
      "Epoch [31/300], Step [12/42], Loss: 12.8099\n",
      "Epoch [31/300], Step [13/42], Loss: 15.1141\n",
      "Epoch [31/300], Step [14/42], Loss: 18.7425\n",
      "Epoch [31/300], Step [15/42], Loss: 14.4509\n",
      "Epoch [31/300], Step [16/42], Loss: 10.0527\n",
      "Epoch [31/300], Step [17/42], Loss: 7.9868\n",
      "Epoch [31/300], Step [18/42], Loss: 11.7782\n",
      "Epoch [31/300], Step [19/42], Loss: 12.5165\n",
      "Epoch [31/300], Step [20/42], Loss: 18.5483\n",
      "Epoch [31/300], Step [21/42], Loss: 14.1317\n",
      "Epoch [31/300], Step [22/42], Loss: 8.7091\n",
      "Epoch [31/300], Step [23/42], Loss: 12.0142\n",
      "Epoch [31/300], Step [24/42], Loss: 12.1906\n",
      "Epoch [31/300], Step [25/42], Loss: 11.6223\n",
      "Epoch [31/300], Step [26/42], Loss: 11.5070\n",
      "Epoch [31/300], Step [27/42], Loss: 16.0731\n",
      "Epoch [31/300], Step [28/42], Loss: 11.7844\n",
      "Epoch [31/300], Step [29/42], Loss: 8.3580\n",
      "Epoch [31/300], Step [30/42], Loss: 10.2228\n",
      "Epoch [31/300], Step [31/42], Loss: 14.7098\n",
      "Epoch [31/300], Step [32/42], Loss: 13.0316\n",
      "Epoch [31/300], Step [33/42], Loss: 14.1188\n",
      "Epoch [31/300], Step [34/42], Loss: 13.0667\n",
      "Epoch [31/300], Step [35/42], Loss: 11.8014\n",
      "Epoch [31/300], Step [36/42], Loss: 13.9073\n",
      "Epoch [31/300], Step [37/42], Loss: 16.3987\n",
      "Epoch [31/300], Step [38/42], Loss: 8.6903\n",
      "Epoch [31/300], Step [39/42], Loss: 16.0287\n",
      "Epoch [31/300], Step [40/42], Loss: 16.3648\n",
      "Epoch [31/300], Step [41/42], Loss: 14.1602\n",
      "Epoch [31/300], Step [42/42], Loss: 3.4648\n",
      "Val. loss :14.2957\n",
      "Epoch [32/300], Step [1/42], Loss: 13.1985\n",
      "Epoch [32/300], Step [2/42], Loss: 17.4472\n",
      "Epoch [32/300], Step [3/42], Loss: 14.3677\n",
      "Epoch [32/300], Step [4/42], Loss: 11.1307\n",
      "Epoch [32/300], Step [5/42], Loss: 12.4241\n",
      "Epoch [32/300], Step [6/42], Loss: 11.5506\n",
      "Epoch [32/300], Step [7/42], Loss: 14.2708\n",
      "Epoch [32/300], Step [8/42], Loss: 13.4805\n",
      "Epoch [32/300], Step [9/42], Loss: 8.9468\n",
      "Epoch [32/300], Step [10/42], Loss: 14.2910\n",
      "Epoch [32/300], Step [11/42], Loss: 15.8365\n",
      "Epoch [32/300], Step [12/42], Loss: 14.2956\n",
      "Epoch [32/300], Step [13/42], Loss: 12.3219\n",
      "Epoch [32/300], Step [14/42], Loss: 13.3553\n",
      "Epoch [32/300], Step [15/42], Loss: 12.9107\n",
      "Epoch [32/300], Step [16/42], Loss: 16.0050\n",
      "Epoch [32/300], Step [17/42], Loss: 9.9593\n",
      "Epoch [32/300], Step [18/42], Loss: 16.5278\n",
      "Epoch [32/300], Step [19/42], Loss: 13.2497\n",
      "Epoch [32/300], Step [20/42], Loss: 11.6157\n",
      "Epoch [32/300], Step [21/42], Loss: 9.7692\n",
      "Epoch [32/300], Step [22/42], Loss: 9.9677\n",
      "Epoch [32/300], Step [23/42], Loss: 17.9156\n",
      "Epoch [32/300], Step [24/42], Loss: 12.6026\n",
      "Epoch [32/300], Step [25/42], Loss: 11.5901\n",
      "Epoch [32/300], Step [26/42], Loss: 10.2383\n",
      "Epoch [32/300], Step [27/42], Loss: 11.2861\n",
      "Epoch [32/300], Step [28/42], Loss: 12.9228\n",
      "Epoch [32/300], Step [29/42], Loss: 9.7716\n",
      "Epoch [32/300], Step [30/42], Loss: 13.3289\n",
      "Epoch [32/300], Step [31/42], Loss: 9.0691\n",
      "Epoch [32/300], Step [32/42], Loss: 13.7491\n",
      "Epoch [32/300], Step [33/42], Loss: 11.1949\n",
      "Epoch [32/300], Step [34/42], Loss: 13.2807\n",
      "Epoch [32/300], Step [35/42], Loss: 8.1062\n",
      "Epoch [32/300], Step [36/42], Loss: 10.7850\n",
      "Epoch [32/300], Step [37/42], Loss: 8.5471\n",
      "Epoch [32/300], Step [38/42], Loss: 10.1260\n",
      "Epoch [32/300], Step [39/42], Loss: 13.2057\n",
      "Epoch [32/300], Step [40/42], Loss: 11.2782\n",
      "Epoch [32/300], Step [41/42], Loss: 14.2246\n",
      "Epoch [32/300], Step [42/42], Loss: 2.8723\n",
      "Val. loss :13.3562\n",
      "Epoch [33/300], Step [1/42], Loss: 9.3656\n",
      "Epoch [33/300], Step [2/42], Loss: 10.5220\n",
      "Epoch [33/300], Step [3/42], Loss: 11.4112\n",
      "Epoch [33/300], Step [4/42], Loss: 11.9554\n",
      "Epoch [33/300], Step [5/42], Loss: 11.2825\n",
      "Epoch [33/300], Step [6/42], Loss: 9.9329\n",
      "Epoch [33/300], Step [7/42], Loss: 11.9046\n",
      "Epoch [33/300], Step [8/42], Loss: 9.9036\n",
      "Epoch [33/300], Step [9/42], Loss: 10.0911\n",
      "Epoch [33/300], Step [10/42], Loss: 11.8635\n",
      "Epoch [33/300], Step [11/42], Loss: 10.9078\n",
      "Epoch [33/300], Step [12/42], Loss: 10.5940\n",
      "Epoch [33/300], Step [13/42], Loss: 9.7571\n",
      "Epoch [33/300], Step [14/42], Loss: 14.9433\n",
      "Epoch [33/300], Step [15/42], Loss: 11.9594\n",
      "Epoch [33/300], Step [16/42], Loss: 12.1882\n",
      "Epoch [33/300], Step [17/42], Loss: 12.8812\n",
      "Epoch [33/300], Step [18/42], Loss: 12.2958\n",
      "Epoch [33/300], Step [19/42], Loss: 13.1794\n",
      "Epoch [33/300], Step [20/42], Loss: 8.5369\n",
      "Epoch [33/300], Step [21/42], Loss: 12.3433\n",
      "Epoch [33/300], Step [22/42], Loss: 10.1868\n",
      "Epoch [33/300], Step [23/42], Loss: 11.7415\n",
      "Epoch [33/300], Step [24/42], Loss: 12.5398\n",
      "Epoch [33/300], Step [25/42], Loss: 9.3634\n",
      "Epoch [33/300], Step [26/42], Loss: 11.8391\n",
      "Epoch [33/300], Step [27/42], Loss: 11.7896\n",
      "Epoch [33/300], Step [28/42], Loss: 16.9350\n",
      "Epoch [33/300], Step [29/42], Loss: 11.2136\n",
      "Epoch [33/300], Step [30/42], Loss: 8.3087\n",
      "Epoch [33/300], Step [31/42], Loss: 13.0922\n",
      "Epoch [33/300], Step [32/42], Loss: 12.7219\n",
      "Epoch [33/300], Step [33/42], Loss: 15.1854\n",
      "Epoch [33/300], Step [34/42], Loss: 10.6314\n",
      "Epoch [33/300], Step [35/42], Loss: 12.3345\n",
      "Epoch [33/300], Step [36/42], Loss: 8.7808\n",
      "Epoch [33/300], Step [37/42], Loss: 13.4151\n",
      "Epoch [33/300], Step [38/42], Loss: 11.4976\n",
      "Epoch [33/300], Step [39/42], Loss: 10.6521\n",
      "Epoch [33/300], Step [40/42], Loss: 9.6074\n",
      "Epoch [33/300], Step [41/42], Loss: 12.2504\n",
      "Epoch [33/300], Step [42/42], Loss: 2.5125\n",
      "Val. loss :11.6193\n",
      "Epoch [34/300], Step [1/42], Loss: 11.2646\n",
      "Epoch [34/300], Step [2/42], Loss: 13.6246\n",
      "Epoch [34/300], Step [3/42], Loss: 13.6510\n",
      "Epoch [34/300], Step [4/42], Loss: 8.4585\n",
      "Epoch [34/300], Step [5/42], Loss: 11.2420\n",
      "Epoch [34/300], Step [6/42], Loss: 10.1489\n",
      "Epoch [34/300], Step [7/42], Loss: 13.3202\n",
      "Epoch [34/300], Step [8/42], Loss: 10.3414\n",
      "Epoch [34/300], Step [9/42], Loss: 11.8671\n",
      "Epoch [34/300], Step [10/42], Loss: 11.1930\n",
      "Epoch [34/300], Step [11/42], Loss: 7.0069\n",
      "Epoch [34/300], Step [12/42], Loss: 12.2969\n",
      "Epoch [34/300], Step [13/42], Loss: 10.0024\n",
      "Epoch [34/300], Step [14/42], Loss: 8.6099\n",
      "Epoch [34/300], Step [15/42], Loss: 7.9429\n",
      "Epoch [34/300], Step [16/42], Loss: 13.6574\n",
      "Epoch [34/300], Step [17/42], Loss: 13.3431\n",
      "Epoch [34/300], Step [18/42], Loss: 9.7470\n",
      "Epoch [34/300], Step [19/42], Loss: 12.0574\n",
      "Epoch [34/300], Step [20/42], Loss: 9.9773\n",
      "Epoch [34/300], Step [21/42], Loss: 6.7869\n",
      "Epoch [34/300], Step [22/42], Loss: 9.9295\n",
      "Epoch [34/300], Step [23/42], Loss: 10.6991\n",
      "Epoch [34/300], Step [24/42], Loss: 11.4630\n",
      "Epoch [34/300], Step [25/42], Loss: 9.2887\n",
      "Epoch [34/300], Step [26/42], Loss: 9.4154\n",
      "Epoch [34/300], Step [27/42], Loss: 6.0175\n",
      "Epoch [34/300], Step [28/42], Loss: 12.7634\n",
      "Epoch [34/300], Step [29/42], Loss: 13.7017\n",
      "Epoch [34/300], Step [30/42], Loss: 9.9847\n",
      "Epoch [34/300], Step [31/42], Loss: 11.1966\n",
      "Epoch [34/300], Step [32/42], Loss: 8.6262\n",
      "Epoch [34/300], Step [33/42], Loss: 8.7213\n",
      "Epoch [34/300], Step [34/42], Loss: 9.9139\n",
      "Epoch [34/300], Step [35/42], Loss: 11.2826\n",
      "Epoch [34/300], Step [36/42], Loss: 16.4872\n",
      "Epoch [34/300], Step [37/42], Loss: 11.8905\n",
      "Epoch [34/300], Step [38/42], Loss: 12.1843\n",
      "Epoch [34/300], Step [39/42], Loss: 8.0135\n",
      "Epoch [34/300], Step [40/42], Loss: 10.8978\n",
      "Epoch [34/300], Step [41/42], Loss: 8.6710\n",
      "Epoch [34/300], Step [42/42], Loss: 1.3999\n",
      "Val. loss :11.1795\n",
      "Epoch [35/300], Step [1/42], Loss: 12.4563\n",
      "Epoch [35/300], Step [2/42], Loss: 8.7375\n",
      "Epoch [35/300], Step [3/42], Loss: 11.5325\n",
      "Epoch [35/300], Step [4/42], Loss: 8.0629\n",
      "Epoch [35/300], Step [5/42], Loss: 7.9065\n",
      "Epoch [35/300], Step [6/42], Loss: 12.0339\n",
      "Epoch [35/300], Step [7/42], Loss: 8.3942\n",
      "Epoch [35/300], Step [8/42], Loss: 12.6608\n",
      "Epoch [35/300], Step [9/42], Loss: 9.6302\n",
      "Epoch [35/300], Step [10/42], Loss: 10.7603\n",
      "Epoch [35/300], Step [11/42], Loss: 6.3966\n",
      "Epoch [35/300], Step [12/42], Loss: 13.0573\n",
      "Epoch [35/300], Step [13/42], Loss: 9.6470\n",
      "Epoch [35/300], Step [14/42], Loss: 8.2273\n",
      "Epoch [35/300], Step [15/42], Loss: 8.3424\n",
      "Epoch [35/300], Step [16/42], Loss: 7.1026\n",
      "Epoch [35/300], Step [17/42], Loss: 15.1844\n",
      "Epoch [35/300], Step [18/42], Loss: 7.2470\n",
      "Epoch [35/300], Step [19/42], Loss: 9.7686\n",
      "Epoch [35/300], Step [20/42], Loss: 8.8843\n",
      "Epoch [35/300], Step [21/42], Loss: 8.9782\n",
      "Epoch [35/300], Step [22/42], Loss: 9.1477\n",
      "Epoch [35/300], Step [23/42], Loss: 10.0727\n",
      "Epoch [35/300], Step [24/42], Loss: 9.5627\n",
      "Epoch [35/300], Step [25/42], Loss: 10.8885\n",
      "Epoch [35/300], Step [26/42], Loss: 12.1827\n",
      "Epoch [35/300], Step [27/42], Loss: 7.6301\n",
      "Epoch [35/300], Step [28/42], Loss: 12.3791\n",
      "Epoch [35/300], Step [29/42], Loss: 9.9990\n",
      "Epoch [35/300], Step [30/42], Loss: 14.8186\n",
      "Epoch [35/300], Step [31/42], Loss: 11.6069\n",
      "Epoch [35/300], Step [32/42], Loss: 7.3269\n",
      "Epoch [35/300], Step [33/42], Loss: 8.4928\n",
      "Epoch [35/300], Step [34/42], Loss: 15.9452\n",
      "Epoch [35/300], Step [35/42], Loss: 10.4908\n",
      "Epoch [35/300], Step [36/42], Loss: 10.1581\n",
      "Epoch [35/300], Step [37/42], Loss: 9.7897\n",
      "Epoch [35/300], Step [38/42], Loss: 9.2100\n",
      "Epoch [35/300], Step [39/42], Loss: 11.5869\n",
      "Epoch [35/300], Step [40/42], Loss: 10.5655\n",
      "Epoch [35/300], Step [41/42], Loss: 9.9793\n",
      "Epoch [35/300], Step [42/42], Loss: 5.1968\n",
      "Val. loss :10.3073\n",
      "Epoch [36/300], Step [1/42], Loss: 9.7931\n",
      "Epoch [36/300], Step [2/42], Loss: 9.2485\n",
      "Epoch [36/300], Step [3/42], Loss: 7.5625\n",
      "Epoch [36/300], Step [4/42], Loss: 8.8179\n",
      "Epoch [36/300], Step [5/42], Loss: 8.8035\n",
      "Epoch [36/300], Step [6/42], Loss: 10.9210\n",
      "Epoch [36/300], Step [7/42], Loss: 8.2420\n",
      "Epoch [36/300], Step [8/42], Loss: 9.0599\n",
      "Epoch [36/300], Step [9/42], Loss: 11.0265\n",
      "Epoch [36/300], Step [10/42], Loss: 10.9491\n",
      "Epoch [36/300], Step [11/42], Loss: 6.9269\n",
      "Epoch [36/300], Step [12/42], Loss: 7.2535\n",
      "Epoch [36/300], Step [13/42], Loss: 6.5463\n",
      "Epoch [36/300], Step [14/42], Loss: 9.0693\n",
      "Epoch [36/300], Step [15/42], Loss: 10.5663\n",
      "Epoch [36/300], Step [16/42], Loss: 11.4095\n",
      "Epoch [36/300], Step [17/42], Loss: 5.6192\n",
      "Epoch [36/300], Step [18/42], Loss: 7.4043\n",
      "Epoch [36/300], Step [19/42], Loss: 10.0684\n",
      "Epoch [36/300], Step [20/42], Loss: 9.7761\n",
      "Epoch [36/300], Step [21/42], Loss: 6.7143\n",
      "Epoch [36/300], Step [22/42], Loss: 10.9364\n",
      "Epoch [36/300], Step [23/42], Loss: 8.4391\n",
      "Epoch [36/300], Step [24/42], Loss: 10.2387\n",
      "Epoch [36/300], Step [25/42], Loss: 5.8843\n",
      "Epoch [36/300], Step [26/42], Loss: 11.5101\n",
      "Epoch [36/300], Step [27/42], Loss: 11.9690\n",
      "Epoch [36/300], Step [28/42], Loss: 9.7870\n",
      "Epoch [36/300], Step [29/42], Loss: 9.2448\n",
      "Epoch [36/300], Step [30/42], Loss: 11.5628\n",
      "Epoch [36/300], Step [31/42], Loss: 11.6552\n",
      "Epoch [36/300], Step [32/42], Loss: 9.2906\n",
      "Epoch [36/300], Step [33/42], Loss: 9.5564\n",
      "Epoch [36/300], Step [34/42], Loss: 7.6563\n",
      "Epoch [36/300], Step [35/42], Loss: 8.4930\n",
      "Epoch [36/300], Step [36/42], Loss: 9.0497\n",
      "Epoch [36/300], Step [37/42], Loss: 9.1046\n",
      "Epoch [36/300], Step [38/42], Loss: 11.0668\n",
      "Epoch [36/300], Step [39/42], Loss: 10.3382\n",
      "Epoch [36/300], Step [40/42], Loss: 9.0402\n",
      "Epoch [36/300], Step [41/42], Loss: 11.1156\n",
      "Epoch [36/300], Step [42/42], Loss: 4.4188\n",
      "Val. loss :9.7969\n",
      "Epoch [37/300], Step [1/42], Loss: 6.4512\n",
      "Epoch [37/300], Step [2/42], Loss: 6.5180\n",
      "Epoch [37/300], Step [3/42], Loss: 8.2964\n",
      "Epoch [37/300], Step [4/42], Loss: 10.6066\n",
      "Epoch [37/300], Step [5/42], Loss: 9.0021\n",
      "Epoch [37/300], Step [6/42], Loss: 10.1865\n",
      "Epoch [37/300], Step [7/42], Loss: 11.1819\n",
      "Epoch [37/300], Step [8/42], Loss: 7.2885\n",
      "Epoch [37/300], Step [9/42], Loss: 7.8962\n",
      "Epoch [37/300], Step [10/42], Loss: 6.8067\n",
      "Epoch [37/300], Step [11/42], Loss: 10.1131\n",
      "Epoch [37/300], Step [12/42], Loss: 10.7610\n",
      "Epoch [37/300], Step [13/42], Loss: 7.0992\n",
      "Epoch [37/300], Step [14/42], Loss: 10.9309\n",
      "Epoch [37/300], Step [15/42], Loss: 9.1054\n",
      "Epoch [37/300], Step [16/42], Loss: 9.2213\n",
      "Epoch [37/300], Step [17/42], Loss: 8.7046\n",
      "Epoch [37/300], Step [18/42], Loss: 9.0021\n",
      "Epoch [37/300], Step [19/42], Loss: 5.6468\n",
      "Epoch [37/300], Step [20/42], Loss: 8.3260\n",
      "Epoch [37/300], Step [21/42], Loss: 8.7483\n",
      "Epoch [37/300], Step [22/42], Loss: 11.0002\n",
      "Epoch [37/300], Step [23/42], Loss: 7.5620\n",
      "Epoch [37/300], Step [24/42], Loss: 15.6885\n",
      "Epoch [37/300], Step [25/42], Loss: 9.2753\n",
      "Epoch [37/300], Step [26/42], Loss: 10.7999\n",
      "Epoch [37/300], Step [27/42], Loss: 9.1018\n",
      "Epoch [37/300], Step [28/42], Loss: 9.4318\n",
      "Epoch [37/300], Step [29/42], Loss: 15.3261\n",
      "Epoch [37/300], Step [30/42], Loss: 6.9182\n",
      "Epoch [37/300], Step [31/42], Loss: 11.8081\n",
      "Epoch [37/300], Step [32/42], Loss: 9.8007\n",
      "Epoch [37/300], Step [33/42], Loss: 8.7779\n",
      "Epoch [37/300], Step [34/42], Loss: 8.9412\n",
      "Epoch [37/300], Step [35/42], Loss: 8.8892\n",
      "Epoch [37/300], Step [36/42], Loss: 9.0941\n",
      "Epoch [37/300], Step [37/42], Loss: 5.5116\n",
      "Epoch [37/300], Step [38/42], Loss: 9.3589\n",
      "Epoch [37/300], Step [39/42], Loss: 6.3740\n",
      "Epoch [37/300], Step [40/42], Loss: 7.1411\n",
      "Epoch [37/300], Step [41/42], Loss: 10.6156\n",
      "Epoch [37/300], Step [42/42], Loss: 2.1832\n",
      "Val. loss :9.1485\n",
      "Epoch [38/300], Step [1/42], Loss: 8.5597\n",
      "Epoch [38/300], Step [2/42], Loss: 9.4538\n",
      "Epoch [38/300], Step [3/42], Loss: 7.7822\n",
      "Epoch [38/300], Step [4/42], Loss: 9.3381\n",
      "Epoch [38/300], Step [5/42], Loss: 5.8559\n",
      "Epoch [38/300], Step [6/42], Loss: 5.5402\n",
      "Epoch [38/300], Step [7/42], Loss: 6.3130\n",
      "Epoch [38/300], Step [8/42], Loss: 7.5650\n",
      "Epoch [38/300], Step [9/42], Loss: 9.5053\n",
      "Epoch [38/300], Step [10/42], Loss: 8.8463\n",
      "Epoch [38/300], Step [11/42], Loss: 9.5484\n",
      "Epoch [38/300], Step [12/42], Loss: 9.8959\n",
      "Epoch [38/300], Step [13/42], Loss: 5.7148\n",
      "Epoch [38/300], Step [14/42], Loss: 10.6843\n",
      "Epoch [38/300], Step [15/42], Loss: 9.9710\n",
      "Epoch [38/300], Step [16/42], Loss: 6.7485\n",
      "Epoch [38/300], Step [17/42], Loss: 8.6663\n",
      "Epoch [38/300], Step [18/42], Loss: 9.6781\n",
      "Epoch [38/300], Step [19/42], Loss: 9.1843\n",
      "Epoch [38/300], Step [20/42], Loss: 7.9078\n",
      "Epoch [38/300], Step [21/42], Loss: 9.5656\n",
      "Epoch [38/300], Step [22/42], Loss: 6.7591\n",
      "Epoch [38/300], Step [23/42], Loss: 10.4674\n",
      "Epoch [38/300], Step [24/42], Loss: 8.5074\n",
      "Epoch [38/300], Step [25/42], Loss: 9.9299\n",
      "Epoch [38/300], Step [26/42], Loss: 9.2867\n",
      "Epoch [38/300], Step [27/42], Loss: 5.7231\n",
      "Epoch [38/300], Step [28/42], Loss: 8.3413\n",
      "Epoch [38/300], Step [29/42], Loss: 9.1605\n",
      "Epoch [38/300], Step [30/42], Loss: 11.1470\n",
      "Epoch [38/300], Step [31/42], Loss: 7.9034\n",
      "Epoch [38/300], Step [32/42], Loss: 11.5500\n",
      "Epoch [38/300], Step [33/42], Loss: 7.4108\n",
      "Epoch [38/300], Step [34/42], Loss: 8.0379\n",
      "Epoch [38/300], Step [35/42], Loss: 11.0111\n",
      "Epoch [38/300], Step [36/42], Loss: 7.0621\n",
      "Epoch [38/300], Step [37/42], Loss: 7.3849\n",
      "Epoch [38/300], Step [38/42], Loss: 10.0910\n",
      "Epoch [38/300], Step [39/42], Loss: 10.7237\n",
      "Epoch [38/300], Step [40/42], Loss: 8.2098\n",
      "Epoch [38/300], Step [41/42], Loss: 7.3073\n",
      "Epoch [38/300], Step [42/42], Loss: 3.4207\n",
      "Val. loss :9.3952\n",
      "Epoch [39/300], Step [1/42], Loss: 9.5189\n",
      "Epoch [39/300], Step [2/42], Loss: 11.3979\n",
      "Epoch [39/300], Step [3/42], Loss: 14.1236\n",
      "Epoch [39/300], Step [4/42], Loss: 6.3094\n",
      "Epoch [39/300], Step [5/42], Loss: 10.7110\n",
      "Epoch [39/300], Step [6/42], Loss: 9.1481\n",
      "Epoch [39/300], Step [7/42], Loss: 11.3912\n",
      "Epoch [39/300], Step [8/42], Loss: 8.9789\n",
      "Epoch [39/300], Step [9/42], Loss: 7.4264\n",
      "Epoch [39/300], Step [10/42], Loss: 6.5752\n",
      "Epoch [39/300], Step [11/42], Loss: 6.8345\n",
      "Epoch [39/300], Step [12/42], Loss: 7.1333\n",
      "Epoch [39/300], Step [13/42], Loss: 4.7673\n",
      "Epoch [39/300], Step [14/42], Loss: 6.3187\n",
      "Epoch [39/300], Step [15/42], Loss: 7.4379\n",
      "Epoch [39/300], Step [16/42], Loss: 5.2184\n",
      "Epoch [39/300], Step [17/42], Loss: 7.4682\n",
      "Epoch [39/300], Step [18/42], Loss: 9.3618\n",
      "Epoch [39/300], Step [19/42], Loss: 7.1474\n",
      "Epoch [39/300], Step [20/42], Loss: 5.6217\n",
      "Epoch [39/300], Step [21/42], Loss: 11.2051\n",
      "Epoch [39/300], Step [22/42], Loss: 6.1871\n",
      "Epoch [39/300], Step [23/42], Loss: 8.0322\n",
      "Epoch [39/300], Step [24/42], Loss: 6.2048\n",
      "Epoch [39/300], Step [25/42], Loss: 6.7483\n",
      "Epoch [39/300], Step [26/42], Loss: 10.5143\n",
      "Epoch [39/300], Step [27/42], Loss: 12.4097\n",
      "Epoch [39/300], Step [28/42], Loss: 3.5014\n",
      "Epoch [39/300], Step [29/42], Loss: 7.8546\n",
      "Epoch [39/300], Step [30/42], Loss: 5.5615\n",
      "Epoch [39/300], Step [31/42], Loss: 6.8024\n",
      "Epoch [39/300], Step [32/42], Loss: 8.0445\n",
      "Epoch [39/300], Step [33/42], Loss: 4.9106\n",
      "Epoch [39/300], Step [34/42], Loss: 7.9727\n",
      "Epoch [39/300], Step [35/42], Loss: 8.6530\n",
      "Epoch [39/300], Step [36/42], Loss: 5.7622\n",
      "Epoch [39/300], Step [37/42], Loss: 11.4506\n",
      "Epoch [39/300], Step [38/42], Loss: 5.4644\n",
      "Epoch [39/300], Step [39/42], Loss: 6.6589\n",
      "Epoch [39/300], Step [40/42], Loss: 5.9855\n",
      "Epoch [39/300], Step [41/42], Loss: 11.8955\n",
      "Epoch [39/300], Step [42/42], Loss: 2.0204\n",
      "Val. loss :8.3062\n",
      "Epoch [40/300], Step [1/42], Loss: 5.0258\n",
      "Epoch [40/300], Step [2/42], Loss: 5.2728\n",
      "Epoch [40/300], Step [3/42], Loss: 8.1870\n",
      "Epoch [40/300], Step [4/42], Loss: 8.3999\n",
      "Epoch [40/300], Step [5/42], Loss: 7.0520\n",
      "Epoch [40/300], Step [6/42], Loss: 6.7048\n",
      "Epoch [40/300], Step [7/42], Loss: 8.0527\n",
      "Epoch [40/300], Step [8/42], Loss: 5.9878\n",
      "Epoch [40/300], Step [9/42], Loss: 6.7852\n",
      "Epoch [40/300], Step [10/42], Loss: 5.9987\n",
      "Epoch [40/300], Step [11/42], Loss: 8.6574\n",
      "Epoch [40/300], Step [12/42], Loss: 7.2508\n",
      "Epoch [40/300], Step [13/42], Loss: 6.2564\n",
      "Epoch [40/300], Step [14/42], Loss: 11.1473\n",
      "Epoch [40/300], Step [15/42], Loss: 9.6114\n",
      "Epoch [40/300], Step [16/42], Loss: 6.3909\n",
      "Epoch [40/300], Step [17/42], Loss: 5.9214\n",
      "Epoch [40/300], Step [18/42], Loss: 5.3819\n",
      "Epoch [40/300], Step [19/42], Loss: 6.6084\n",
      "Epoch [40/300], Step [20/42], Loss: 6.2291\n",
      "Epoch [40/300], Step [21/42], Loss: 8.0832\n",
      "Epoch [40/300], Step [22/42], Loss: 6.3343\n",
      "Epoch [40/300], Step [23/42], Loss: 9.8636\n",
      "Epoch [40/300], Step [24/42], Loss: 7.0471\n",
      "Epoch [40/300], Step [25/42], Loss: 7.8324\n",
      "Epoch [40/300], Step [26/42], Loss: 9.8158\n",
      "Epoch [40/300], Step [27/42], Loss: 4.5437\n",
      "Epoch [40/300], Step [28/42], Loss: 3.9186\n",
      "Epoch [40/300], Step [29/42], Loss: 5.4254\n",
      "Epoch [40/300], Step [30/42], Loss: 8.6113\n",
      "Epoch [40/300], Step [31/42], Loss: 10.6093\n",
      "Epoch [40/300], Step [32/42], Loss: 8.7201\n",
      "Epoch [40/300], Step [33/42], Loss: 9.7332\n",
      "Epoch [40/300], Step [34/42], Loss: 5.3548\n",
      "Epoch [40/300], Step [35/42], Loss: 9.1033\n",
      "Epoch [40/300], Step [36/42], Loss: 6.7753\n",
      "Epoch [40/300], Step [37/42], Loss: 9.7150\n",
      "Epoch [40/300], Step [38/42], Loss: 3.9994\n",
      "Epoch [40/300], Step [39/42], Loss: 8.1860\n",
      "Epoch [40/300], Step [40/42], Loss: 6.3702\n",
      "Epoch [40/300], Step [41/42], Loss: 7.1079\n",
      "Epoch [40/300], Step [42/42], Loss: 1.4965\n",
      "Val. loss :8.0174\n",
      "Epoch [41/300], Step [1/42], Loss: 5.0866\n",
      "Epoch [41/300], Step [2/42], Loss: 8.1617\n",
      "Epoch [41/300], Step [3/42], Loss: 7.1376\n",
      "Epoch [41/300], Step [4/42], Loss: 9.0351\n",
      "Epoch [41/300], Step [5/42], Loss: 5.6446\n",
      "Epoch [41/300], Step [6/42], Loss: 7.1708\n",
      "Epoch [41/300], Step [7/42], Loss: 7.5058\n",
      "Epoch [41/300], Step [8/42], Loss: 8.9020\n",
      "Epoch [41/300], Step [9/42], Loss: 8.5654\n",
      "Epoch [41/300], Step [10/42], Loss: 5.9991\n",
      "Epoch [41/300], Step [11/42], Loss: 7.8068\n",
      "Epoch [41/300], Step [12/42], Loss: 4.9731\n",
      "Epoch [41/300], Step [13/42], Loss: 6.7072\n",
      "Epoch [41/300], Step [14/42], Loss: 6.2949\n",
      "Epoch [41/300], Step [15/42], Loss: 8.8821\n",
      "Epoch [41/300], Step [16/42], Loss: 6.2938\n",
      "Epoch [41/300], Step [17/42], Loss: 11.3536\n",
      "Epoch [41/300], Step [18/42], Loss: 7.6165\n",
      "Epoch [41/300], Step [19/42], Loss: 9.0229\n",
      "Epoch [41/300], Step [20/42], Loss: 4.3290\n",
      "Epoch [41/300], Step [21/42], Loss: 8.0959\n",
      "Epoch [41/300], Step [22/42], Loss: 12.4820\n",
      "Epoch [41/300], Step [23/42], Loss: 6.8895\n",
      "Epoch [41/300], Step [24/42], Loss: 4.3698\n",
      "Epoch [41/300], Step [25/42], Loss: 9.2566\n",
      "Epoch [41/300], Step [26/42], Loss: 8.4871\n",
      "Epoch [41/300], Step [27/42], Loss: 5.2476\n",
      "Epoch [41/300], Step [28/42], Loss: 5.1658\n",
      "Epoch [41/300], Step [29/42], Loss: 8.2285\n",
      "Epoch [41/300], Step [30/42], Loss: 6.8391\n",
      "Epoch [41/300], Step [31/42], Loss: 7.1648\n",
      "Epoch [41/300], Step [32/42], Loss: 6.4448\n",
      "Epoch [41/300], Step [33/42], Loss: 5.4033\n",
      "Epoch [41/300], Step [34/42], Loss: 5.8934\n",
      "Epoch [41/300], Step [35/42], Loss: 7.2012\n",
      "Epoch [41/300], Step [36/42], Loss: 7.0539\n",
      "Epoch [41/300], Step [37/42], Loss: 5.2831\n",
      "Epoch [41/300], Step [38/42], Loss: 4.1723\n",
      "Epoch [41/300], Step [39/42], Loss: 4.2253\n",
      "Epoch [41/300], Step [40/42], Loss: 7.6173\n",
      "Epoch [41/300], Step [41/42], Loss: 7.8412\n",
      "Epoch [41/300], Step [42/42], Loss: 3.6012\n",
      "Val. loss :7.5369\n",
      "Epoch [42/300], Step [1/42], Loss: 5.7493\n",
      "Epoch [42/300], Step [2/42], Loss: 5.0099\n",
      "Epoch [42/300], Step [3/42], Loss: 7.8186\n",
      "Epoch [42/300], Step [4/42], Loss: 6.4349\n",
      "Epoch [42/300], Step [5/42], Loss: 4.3168\n",
      "Epoch [42/300], Step [6/42], Loss: 6.3325\n",
      "Epoch [42/300], Step [7/42], Loss: 5.1105\n",
      "Epoch [42/300], Step [8/42], Loss: 9.9271\n",
      "Epoch [42/300], Step [9/42], Loss: 6.2169\n",
      "Epoch [42/300], Step [10/42], Loss: 4.7218\n",
      "Epoch [42/300], Step [11/42], Loss: 7.7092\n",
      "Epoch [42/300], Step [12/42], Loss: 24.3854\n",
      "Epoch [42/300], Step [13/42], Loss: 5.0522\n",
      "Epoch [42/300], Step [14/42], Loss: 50.3889\n",
      "Epoch [42/300], Step [15/42], Loss: 6.9237\n",
      "Epoch [42/300], Step [16/42], Loss: 23.3634\n",
      "Epoch [42/300], Step [17/42], Loss: 36.1466\n",
      "Epoch [42/300], Step [18/42], Loss: 70.1889\n",
      "Epoch [42/300], Step [19/42], Loss: 51.1629\n",
      "Epoch [42/300], Step [20/42], Loss: 48.6677\n",
      "Epoch [42/300], Step [21/42], Loss: 45.3922\n",
      "Epoch [42/300], Step [22/42], Loss: 48.6447\n",
      "Epoch [42/300], Step [23/42], Loss: 46.8288\n",
      "Epoch [42/300], Step [24/42], Loss: 40.2022\n",
      "Epoch [42/300], Step [25/42], Loss: 31.4068\n",
      "Epoch [42/300], Step [26/42], Loss: 44.9894\n",
      "Epoch [42/300], Step [27/42], Loss: 24.7867\n",
      "Epoch [42/300], Step [28/42], Loss: 33.1352\n",
      "Epoch [42/300], Step [29/42], Loss: 24.7724\n",
      "Epoch [42/300], Step [30/42], Loss: 40.7635\n",
      "Epoch [42/300], Step [31/42], Loss: 36.8484\n",
      "Epoch [42/300], Step [32/42], Loss: 11.2086\n",
      "Epoch [42/300], Step [33/42], Loss: 9.0965\n",
      "Epoch [42/300], Step [34/42], Loss: 30.0439\n",
      "Epoch [42/300], Step [35/42], Loss: 32.3226\n",
      "Epoch [42/300], Step [36/42], Loss: 29.1440\n",
      "Epoch [42/300], Step [37/42], Loss: 32.2068\n",
      "Epoch [42/300], Step [38/42], Loss: 28.4727\n",
      "Epoch [42/300], Step [39/42], Loss: 8.8581\n",
      "Epoch [42/300], Step [40/42], Loss: 18.4795\n",
      "Epoch [42/300], Step [41/42], Loss: 42.4721\n",
      "Epoch [42/300], Step [42/42], Loss: 16.0230\n",
      "Val. loss :22.3644\n",
      "Epoch [43/300], Step [1/42], Loss: 19.3650\n",
      "Epoch [43/300], Step [2/42], Loss: 26.2862\n",
      "Epoch [43/300], Step [3/42], Loss: 21.3079\n",
      "Epoch [43/300], Step [4/42], Loss: 23.7807\n",
      "Epoch [43/300], Step [5/42], Loss: 6.3920\n",
      "Epoch [43/300], Step [6/42], Loss: 19.0055\n",
      "Epoch [43/300], Step [7/42], Loss: 15.0004\n",
      "Epoch [43/300], Step [8/42], Loss: 21.0513\n",
      "Epoch [43/300], Step [9/42], Loss: 16.0140\n",
      "Epoch [43/300], Step [10/42], Loss: 12.6004\n",
      "Epoch [43/300], Step [11/42], Loss: 12.3757\n",
      "Epoch [43/300], Step [12/42], Loss: 23.9005\n",
      "Epoch [43/300], Step [13/42], Loss: 17.3026\n",
      "Epoch [43/300], Step [14/42], Loss: 11.8777\n",
      "Epoch [43/300], Step [15/42], Loss: 7.2555\n",
      "Epoch [43/300], Step [16/42], Loss: 8.3311\n",
      "Epoch [43/300], Step [17/42], Loss: 6.6754\n",
      "Epoch [43/300], Step [18/42], Loss: 10.0824\n",
      "Epoch [43/300], Step [19/42], Loss: 7.0586\n",
      "Epoch [43/300], Step [20/42], Loss: 5.4578\n",
      "Epoch [43/300], Step [21/42], Loss: 6.1886\n",
      "Epoch [43/300], Step [22/42], Loss: 7.4182\n",
      "Epoch [43/300], Step [23/42], Loss: 8.2417\n",
      "Epoch [43/300], Step [24/42], Loss: 4.6881\n",
      "Epoch [43/300], Step [25/42], Loss: 7.0131\n",
      "Epoch [43/300], Step [26/42], Loss: 5.3185\n",
      "Epoch [43/300], Step [27/42], Loss: 9.5600\n",
      "Epoch [43/300], Step [28/42], Loss: 8.0315\n",
      "Epoch [43/300], Step [29/42], Loss: 6.2659\n",
      "Epoch [43/300], Step [30/42], Loss: 5.7079\n",
      "Epoch [43/300], Step [31/42], Loss: 9.1200\n",
      "Epoch [43/300], Step [32/42], Loss: 4.6993\n",
      "Epoch [43/300], Step [33/42], Loss: 4.8932\n",
      "Epoch [43/300], Step [34/42], Loss: 6.5013\n",
      "Epoch [43/300], Step [35/42], Loss: 5.9636\n",
      "Epoch [43/300], Step [36/42], Loss: 8.6963\n",
      "Epoch [43/300], Step [37/42], Loss: 4.8713\n",
      "Epoch [43/300], Step [38/42], Loss: 5.2419\n",
      "Epoch [43/300], Step [39/42], Loss: 7.4694\n",
      "Epoch [43/300], Step [40/42], Loss: 5.5824\n",
      "Epoch [43/300], Step [41/42], Loss: 5.2183\n",
      "Epoch [43/300], Step [42/42], Loss: 1.5435\n",
      "Val. loss :7.2032\n",
      "Epoch [44/300], Step [1/42], Loss: 7.0632\n",
      "Epoch [44/300], Step [2/42], Loss: 6.1727\n",
      "Epoch [44/300], Step [3/42], Loss: 7.1624\n",
      "Epoch [44/300], Step [4/42], Loss: 4.9785\n",
      "Epoch [44/300], Step [5/42], Loss: 7.9706\n",
      "Epoch [44/300], Step [6/42], Loss: 6.0113\n",
      "Epoch [44/300], Step [7/42], Loss: 4.4573\n",
      "Epoch [44/300], Step [8/42], Loss: 4.4151\n",
      "Epoch [44/300], Step [9/42], Loss: 4.9687\n",
      "Epoch [44/300], Step [10/42], Loss: 6.7150\n",
      "Epoch [44/300], Step [11/42], Loss: 8.4190\n",
      "Epoch [44/300], Step [12/42], Loss: 4.7401\n",
      "Epoch [44/300], Step [13/42], Loss: 6.1190\n",
      "Epoch [44/300], Step [14/42], Loss: 6.0242\n",
      "Epoch [44/300], Step [15/42], Loss: 7.1934\n",
      "Epoch [44/300], Step [16/42], Loss: 6.0067\n",
      "Epoch [44/300], Step [17/42], Loss: 6.0133\n",
      "Epoch [44/300], Step [18/42], Loss: 7.6248\n",
      "Epoch [44/300], Step [19/42], Loss: 5.7360\n",
      "Epoch [44/300], Step [20/42], Loss: 6.1220\n",
      "Epoch [44/300], Step [21/42], Loss: 8.0213\n",
      "Epoch [44/300], Step [22/42], Loss: 5.5965\n",
      "Epoch [44/300], Step [23/42], Loss: 4.7513\n",
      "Epoch [44/300], Step [24/42], Loss: 7.3428\n",
      "Epoch [44/300], Step [25/42], Loss: 4.1663\n",
      "Epoch [44/300], Step [26/42], Loss: 6.8197\n",
      "Epoch [44/300], Step [27/42], Loss: 5.9792\n",
      "Epoch [44/300], Step [28/42], Loss: 7.4211\n",
      "Epoch [44/300], Step [29/42], Loss: 8.9067\n",
      "Epoch [44/300], Step [30/42], Loss: 6.4132\n",
      "Epoch [44/300], Step [31/42], Loss: 7.5196\n",
      "Epoch [44/300], Step [32/42], Loss: 13.5287\n",
      "Epoch [44/300], Step [33/42], Loss: 4.5857\n",
      "Epoch [44/300], Step [34/42], Loss: 3.9802\n",
      "Epoch [44/300], Step [35/42], Loss: 8.0982\n",
      "Epoch [44/300], Step [36/42], Loss: 6.6009\n",
      "Epoch [44/300], Step [37/42], Loss: 6.1365\n",
      "Epoch [44/300], Step [38/42], Loss: 7.0895\n",
      "Epoch [44/300], Step [39/42], Loss: 6.7546\n",
      "Epoch [44/300], Step [40/42], Loss: 4.8939\n",
      "Epoch [44/300], Step [41/42], Loss: 5.4048\n",
      "Epoch [44/300], Step [42/42], Loss: 2.4147\n",
      "Val. loss :6.8878\n",
      "Epoch [45/300], Step [1/42], Loss: 5.8951\n",
      "Epoch [45/300], Step [2/42], Loss: 5.5962\n",
      "Epoch [45/300], Step [3/42], Loss: 6.5041\n",
      "Epoch [45/300], Step [4/42], Loss: 8.8022\n",
      "Epoch [45/300], Step [5/42], Loss: 4.8320\n",
      "Epoch [45/300], Step [6/42], Loss: 8.2383\n",
      "Epoch [45/300], Step [7/42], Loss: 6.2381\n",
      "Epoch [45/300], Step [8/42], Loss: 5.2963\n",
      "Epoch [45/300], Step [9/42], Loss: 5.9772\n",
      "Epoch [45/300], Step [10/42], Loss: 5.8530\n",
      "Epoch [45/300], Step [11/42], Loss: 5.1263\n",
      "Epoch [45/300], Step [12/42], Loss: 4.6905\n",
      "Epoch [45/300], Step [13/42], Loss: 5.2878\n",
      "Epoch [45/300], Step [14/42], Loss: 5.5105\n",
      "Epoch [45/300], Step [15/42], Loss: 5.9196\n",
      "Epoch [45/300], Step [16/42], Loss: 7.0821\n",
      "Epoch [45/300], Step [17/42], Loss: 6.4056\n",
      "Epoch [45/300], Step [18/42], Loss: 12.1821\n",
      "Epoch [45/300], Step [19/42], Loss: 8.3780\n",
      "Epoch [45/300], Step [20/42], Loss: 4.8112\n",
      "Epoch [45/300], Step [21/42], Loss: 6.1286\n",
      "Epoch [45/300], Step [22/42], Loss: 4.6138\n",
      "Epoch [45/300], Step [23/42], Loss: 5.2423\n",
      "Epoch [45/300], Step [24/42], Loss: 5.5245\n",
      "Epoch [45/300], Step [25/42], Loss: 6.5964\n",
      "Epoch [45/300], Step [26/42], Loss: 4.3383\n",
      "Epoch [45/300], Step [27/42], Loss: 6.1922\n",
      "Epoch [45/300], Step [28/42], Loss: 6.5430\n",
      "Epoch [45/300], Step [29/42], Loss: 4.1730\n",
      "Epoch [45/300], Step [30/42], Loss: 3.7524\n",
      "Epoch [45/300], Step [31/42], Loss: 4.1234\n",
      "Epoch [45/300], Step [32/42], Loss: 5.3343\n",
      "Epoch [45/300], Step [33/42], Loss: 8.2717\n",
      "Epoch [45/300], Step [34/42], Loss: 5.6781\n",
      "Epoch [45/300], Step [35/42], Loss: 6.8309\n",
      "Epoch [45/300], Step [36/42], Loss: 8.9814\n",
      "Epoch [45/300], Step [37/42], Loss: 4.7016\n",
      "Epoch [45/300], Step [38/42], Loss: 4.8704\n",
      "Epoch [45/300], Step [39/42], Loss: 8.5022\n",
      "Epoch [45/300], Step [40/42], Loss: 6.3450\n",
      "Epoch [45/300], Step [41/42], Loss: 5.4441\n",
      "Epoch [45/300], Step [42/42], Loss: 0.9635\n",
      "Val. loss :6.2940\n",
      "Epoch [46/300], Step [1/42], Loss: 4.0682\n",
      "Epoch [46/300], Step [2/42], Loss: 6.8821\n",
      "Epoch [46/300], Step [3/42], Loss: 4.9304\n",
      "Epoch [46/300], Step [4/42], Loss: 7.3991\n",
      "Epoch [46/300], Step [5/42], Loss: 3.7523\n",
      "Epoch [46/300], Step [6/42], Loss: 7.8789\n",
      "Epoch [46/300], Step [7/42], Loss: 6.4101\n",
      "Epoch [46/300], Step [8/42], Loss: 4.4995\n",
      "Epoch [46/300], Step [9/42], Loss: 6.3552\n",
      "Epoch [46/300], Step [10/42], Loss: 3.3459\n",
      "Epoch [46/300], Step [11/42], Loss: 4.3389\n",
      "Epoch [46/300], Step [12/42], Loss: 4.7415\n",
      "Epoch [46/300], Step [13/42], Loss: 6.8557\n",
      "Epoch [46/300], Step [14/42], Loss: 4.5488\n",
      "Epoch [46/300], Step [15/42], Loss: 6.4382\n",
      "Epoch [46/300], Step [16/42], Loss: 5.7322\n",
      "Epoch [46/300], Step [17/42], Loss: 4.6803\n",
      "Epoch [46/300], Step [18/42], Loss: 3.4893\n",
      "Epoch [46/300], Step [19/42], Loss: 6.4213\n",
      "Epoch [46/300], Step [20/42], Loss: 4.3042\n",
      "Epoch [46/300], Step [21/42], Loss: 5.2207\n",
      "Epoch [46/300], Step [22/42], Loss: 6.6751\n",
      "Epoch [46/300], Step [23/42], Loss: 8.3296\n",
      "Epoch [46/300], Step [24/42], Loss: 4.2316\n",
      "Epoch [46/300], Step [25/42], Loss: 7.8474\n",
      "Epoch [46/300], Step [26/42], Loss: 3.5577\n",
      "Epoch [46/300], Step [27/42], Loss: 4.8426\n",
      "Epoch [46/300], Step [28/42], Loss: 5.2564\n",
      "Epoch [46/300], Step [29/42], Loss: 6.2615\n",
      "Epoch [46/300], Step [30/42], Loss: 5.9975\n",
      "Epoch [46/300], Step [31/42], Loss: 4.9206\n",
      "Epoch [46/300], Step [32/42], Loss: 6.3033\n",
      "Epoch [46/300], Step [33/42], Loss: 3.7381\n",
      "Epoch [46/300], Step [34/42], Loss: 4.8004\n",
      "Epoch [46/300], Step [35/42], Loss: 5.1144\n",
      "Epoch [46/300], Step [36/42], Loss: 4.3454\n",
      "Epoch [46/300], Step [37/42], Loss: 4.0704\n",
      "Epoch [46/300], Step [38/42], Loss: 14.7183\n",
      "Epoch [46/300], Step [39/42], Loss: 5.0986\n",
      "Epoch [46/300], Step [40/42], Loss: 4.3964\n",
      "Epoch [46/300], Step [41/42], Loss: 6.2724\n",
      "Epoch [46/300], Step [42/42], Loss: 1.7461\n",
      "Val. loss :6.3994\n",
      "Epoch [47/300], Step [1/42], Loss: 5.9541\n",
      "Epoch [47/300], Step [2/42], Loss: 4.0540\n",
      "Epoch [47/300], Step [3/42], Loss: 4.9684\n",
      "Epoch [47/300], Step [4/42], Loss: 4.9312\n",
      "Epoch [47/300], Step [5/42], Loss: 4.3388\n",
      "Epoch [47/300], Step [6/42], Loss: 3.4320\n",
      "Epoch [47/300], Step [7/42], Loss: 5.4197\n",
      "Epoch [47/300], Step [8/42], Loss: 6.1874\n",
      "Epoch [47/300], Step [9/42], Loss: 8.9855\n",
      "Epoch [47/300], Step [10/42], Loss: 4.9878\n",
      "Epoch [47/300], Step [11/42], Loss: 3.6654\n",
      "Epoch [47/300], Step [12/42], Loss: 5.5197\n",
      "Epoch [47/300], Step [13/42], Loss: 6.3075\n",
      "Epoch [47/300], Step [14/42], Loss: 6.1789\n",
      "Epoch [47/300], Step [15/42], Loss: 5.2628\n",
      "Epoch [47/300], Step [16/42], Loss: 7.8756\n",
      "Epoch [47/300], Step [17/42], Loss: 5.6663\n",
      "Epoch [47/300], Step [18/42], Loss: 4.3488\n",
      "Epoch [47/300], Step [19/42], Loss: 4.9590\n",
      "Epoch [47/300], Step [20/42], Loss: 5.8315\n",
      "Epoch [47/300], Step [21/42], Loss: 4.4752\n",
      "Epoch [47/300], Step [22/42], Loss: 3.6564\n",
      "Epoch [47/300], Step [23/42], Loss: 2.6012\n",
      "Epoch [47/300], Step [24/42], Loss: 4.6035\n",
      "Epoch [47/300], Step [25/42], Loss: 6.3383\n",
      "Epoch [47/300], Step [26/42], Loss: 7.1208\n",
      "Epoch [47/300], Step [27/42], Loss: 7.1905\n",
      "Epoch [47/300], Step [28/42], Loss: 5.0986\n",
      "Epoch [47/300], Step [29/42], Loss: 4.9080\n",
      "Epoch [47/300], Step [30/42], Loss: 7.5367\n",
      "Epoch [47/300], Step [31/42], Loss: 4.0538\n",
      "Epoch [47/300], Step [32/42], Loss: 5.0880\n",
      "Epoch [47/300], Step [33/42], Loss: 5.6197\n",
      "Epoch [47/300], Step [34/42], Loss: 4.7805\n",
      "Epoch [47/300], Step [35/42], Loss: 4.6165\n",
      "Epoch [47/300], Step [36/42], Loss: 5.2151\n",
      "Epoch [47/300], Step [37/42], Loss: 6.5985\n",
      "Epoch [47/300], Step [38/42], Loss: 6.7431\n",
      "Epoch [47/300], Step [39/42], Loss: 4.5251\n",
      "Epoch [47/300], Step [40/42], Loss: 5.7086\n",
      "Epoch [47/300], Step [41/42], Loss: 6.3351\n",
      "Epoch [47/300], Step [42/42], Loss: 1.1096\n",
      "Val. loss :5.8150\n",
      "Epoch [48/300], Step [1/42], Loss: 3.2950\n",
      "Epoch [48/300], Step [2/42], Loss: 3.8978\n",
      "Epoch [48/300], Step [3/42], Loss: 3.4695\n",
      "Epoch [48/300], Step [4/42], Loss: 3.8223\n",
      "Epoch [48/300], Step [5/42], Loss: 5.7308\n",
      "Epoch [48/300], Step [6/42], Loss: 4.7490\n",
      "Epoch [48/300], Step [7/42], Loss: 5.2525\n",
      "Epoch [48/300], Step [8/42], Loss: 5.4364\n",
      "Epoch [48/300], Step [9/42], Loss: 5.5906\n",
      "Epoch [48/300], Step [10/42], Loss: 3.5447\n",
      "Epoch [48/300], Step [11/42], Loss: 4.9899\n",
      "Epoch [48/300], Step [12/42], Loss: 4.0514\n",
      "Epoch [48/300], Step [13/42], Loss: 6.7287\n",
      "Epoch [48/300], Step [14/42], Loss: 3.5190\n",
      "Epoch [48/300], Step [15/42], Loss: 4.0930\n",
      "Epoch [48/300], Step [16/42], Loss: 4.1014\n",
      "Epoch [48/300], Step [17/42], Loss: 34.0360\n",
      "Epoch [48/300], Step [18/42], Loss: 6.5079\n",
      "Epoch [48/300], Step [19/42], Loss: 5.8091\n",
      "Epoch [48/300], Step [20/42], Loss: 8.4438\n",
      "Epoch [48/300], Step [21/42], Loss: 7.4387\n",
      "Epoch [48/300], Step [22/42], Loss: 7.6783\n",
      "Epoch [48/300], Step [23/42], Loss: 6.0603\n",
      "Epoch [48/300], Step [24/42], Loss: 4.8864\n",
      "Epoch [48/300], Step [25/42], Loss: 3.7523\n",
      "Epoch [48/300], Step [26/42], Loss: 11.5910\n",
      "Epoch [48/300], Step [27/42], Loss: 7.1933\n",
      "Epoch [48/300], Step [28/42], Loss: 6.4930\n",
      "Epoch [48/300], Step [29/42], Loss: 5.1799\n",
      "Epoch [48/300], Step [30/42], Loss: 7.0767\n",
      "Epoch [48/300], Step [31/42], Loss: 3.9988\n",
      "Epoch [48/300], Step [32/42], Loss: 4.7240\n",
      "Epoch [48/300], Step [33/42], Loss: 5.2934\n",
      "Epoch [48/300], Step [34/42], Loss: 6.6640\n",
      "Epoch [48/300], Step [35/42], Loss: 7.3014\n",
      "Epoch [48/300], Step [36/42], Loss: 3.8074\n",
      "Epoch [48/300], Step [37/42], Loss: 4.8778\n",
      "Epoch [48/300], Step [38/42], Loss: 5.3995\n",
      "Epoch [48/300], Step [39/42], Loss: 4.4458\n",
      "Epoch [48/300], Step [40/42], Loss: 4.9723\n",
      "Epoch [48/300], Step [41/42], Loss: 5.1347\n",
      "Epoch [48/300], Step [42/42], Loss: 1.0678\n",
      "Val. loss :6.1125\n",
      "Epoch [49/300], Step [1/42], Loss: 5.8330\n",
      "Epoch [49/300], Step [2/42], Loss: 6.5557\n",
      "Epoch [49/300], Step [3/42], Loss: 5.0067\n",
      "Epoch [49/300], Step [4/42], Loss: 3.9040\n",
      "Epoch [49/300], Step [5/42], Loss: 4.9046\n",
      "Epoch [49/300], Step [6/42], Loss: 3.5002\n",
      "Epoch [49/300], Step [7/42], Loss: 7.4424\n",
      "Epoch [49/300], Step [8/42], Loss: 4.8493\n",
      "Epoch [49/300], Step [9/42], Loss: 3.2758\n",
      "Epoch [49/300], Step [10/42], Loss: 4.1645\n",
      "Epoch [49/300], Step [11/42], Loss: 9.5437\n",
      "Epoch [49/300], Step [12/42], Loss: 3.5010\n",
      "Epoch [49/300], Step [13/42], Loss: 3.9440\n",
      "Epoch [49/300], Step [14/42], Loss: 5.9398\n",
      "Epoch [49/300], Step [15/42], Loss: 4.0073\n",
      "Epoch [49/300], Step [16/42], Loss: 5.3784\n",
      "Epoch [49/300], Step [17/42], Loss: 3.9628\n",
      "Epoch [49/300], Step [18/42], Loss: 5.1452\n",
      "Epoch [49/300], Step [19/42], Loss: 5.2733\n",
      "Epoch [49/300], Step [20/42], Loss: 4.1570\n",
      "Epoch [49/300], Step [21/42], Loss: 4.6252\n",
      "Epoch [49/300], Step [22/42], Loss: 4.0236\n",
      "Epoch [49/300], Step [23/42], Loss: 4.7475\n",
      "Epoch [49/300], Step [24/42], Loss: 5.9661\n",
      "Epoch [49/300], Step [25/42], Loss: 5.8190\n",
      "Epoch [49/300], Step [26/42], Loss: 4.3191\n",
      "Epoch [49/300], Step [27/42], Loss: 4.4770\n",
      "Epoch [49/300], Step [28/42], Loss: 6.6763\n",
      "Epoch [49/300], Step [29/42], Loss: 6.5138\n",
      "Epoch [49/300], Step [30/42], Loss: 2.5936\n",
      "Epoch [49/300], Step [31/42], Loss: 2.8582\n",
      "Epoch [49/300], Step [32/42], Loss: 4.0489\n",
      "Epoch [49/300], Step [33/42], Loss: 3.3920\n",
      "Epoch [49/300], Step [34/42], Loss: 3.4270\n",
      "Epoch [49/300], Step [35/42], Loss: 6.9011\n",
      "Epoch [49/300], Step [36/42], Loss: 3.7804\n",
      "Epoch [49/300], Step [37/42], Loss: 4.1952\n",
      "Epoch [49/300], Step [38/42], Loss: 5.1457\n",
      "Epoch [49/300], Step [39/42], Loss: 4.7980\n",
      "Epoch [49/300], Step [40/42], Loss: 4.9427\n",
      "Epoch [49/300], Step [41/42], Loss: 3.8557\n",
      "Epoch [49/300], Step [42/42], Loss: 0.4673\n",
      "Val. loss :4.9406\n",
      "Epoch [50/300], Step [1/42], Loss: 8.4240\n",
      "Epoch [50/300], Step [2/42], Loss: 3.7118\n",
      "Epoch [50/300], Step [3/42], Loss: 3.9596\n",
      "Epoch [50/300], Step [4/42], Loss: 3.7296\n",
      "Epoch [50/300], Step [5/42], Loss: 7.7183\n",
      "Epoch [50/300], Step [6/42], Loss: 3.4696\n",
      "Epoch [50/300], Step [7/42], Loss: 2.9442\n",
      "Epoch [50/300], Step [8/42], Loss: 4.4833\n",
      "Epoch [50/300], Step [9/42], Loss: 5.8997\n",
      "Epoch [50/300], Step [10/42], Loss: 3.3013\n",
      "Epoch [50/300], Step [11/42], Loss: 4.7788\n",
      "Epoch [50/300], Step [12/42], Loss: 4.8543\n",
      "Epoch [50/300], Step [13/42], Loss: 5.3587\n",
      "Epoch [50/300], Step [14/42], Loss: 2.8012\n",
      "Epoch [50/300], Step [15/42], Loss: 5.4679\n",
      "Epoch [50/300], Step [16/42], Loss: 11.0543\n",
      "Epoch [50/300], Step [17/42], Loss: 4.0350\n",
      "Epoch [50/300], Step [18/42], Loss: 5.1557\n",
      "Epoch [50/300], Step [19/42], Loss: 3.9750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 192\u001b[0m\n\u001b[0;32m    189\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    190\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m], Step [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 192\u001b[0m           \u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_epochs, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, nTrain, \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    193\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    194\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(tr_loss \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class facemapdataset(Dataset):\n",
    "    def __init__(self, data_file=\"data/dolensek_facemap_224.pt\", transform=None):\n",
    "        super().__init__()\n",
    "        self.data, self.targets = torch.load(data_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Filter out entries with NaN labels\n",
    "        valid_indices = [i for i, label in enumerate(self.targets) if not np.any(np.isnan(label))]\n",
    "        self.data = self.data[valid_indices]\n",
    "        self.targets = self.targets[valid_indices]\n",
    "        self.targets = torch.Tensor(self.targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) # Increase dataset size by 10x\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_index = index % len(self.data)  # Map index to original data\n",
    "        aug_type = index // len(self.data)   # Determine augmentation type\n",
    "\n",
    "        image, keypoints = self.data[base_index].clone(), self.targets[base_index].clone()\n",
    "\n",
    "        # Apply augmentations based on aug_type\n",
    "        if self.transform is not None:\n",
    "            if aug_type == 1:  # Horizontal flip\n",
    "                image = image.flip([2])\n",
    "                keypoints[::2] = 224 - keypoints[::2]\n",
    "\n",
    "            elif aug_type == 2:  # Random rotation\n",
    "                angle = random.uniform(-30, 30)\n",
    "                image = TF.rotate(image, angle)\n",
    "                keypoints = self.rotate_keypoints(keypoints, angle)\n",
    "\n",
    "            elif aug_type == 3:  # Zoom\n",
    "                scale_factor = random.uniform(0.9, 1.1)\n",
    "                image, keypoints = self.zoom(image, keypoints, scale_factor)\n",
    "\n",
    "            elif aug_type == 4:  # Gaussian blur\n",
    "                image = TF.gaussian_blur(image, kernel_size=3)\n",
    "\n",
    "            elif aug_type == 5:  # Random cutout\n",
    "                image = self.random_cutout(image)\n",
    "\n",
    "            elif aug_type == 6:  # Adjust brightness\n",
    "                image = TF.adjust_brightness(image, random.uniform(0.8, 1.2))\n",
    "\n",
    "            elif aug_type == 7:  # Motion blur\n",
    "                image = self.motion_blur(image)\n",
    "\n",
    "            elif aug_type == 8:  # Random jitter\n",
    "                image = self.random_jitter(image)\n",
    "\n",
    "            elif aug_type == 9:  # Combined transformations\n",
    "                angle = random.uniform(-15, 15)\n",
    "                scale_factor = random.uniform(0.9, 1.1)\n",
    "                image = TF.adjust_brightness(image, random.uniform(0.9, 1.1))\n",
    "                image = self.random_jitter(image)\n",
    "                image = TF.rotate(image, angle)\n",
    "                keypoints = self.rotate_keypoints(keypoints, angle)\n",
    "                image, keypoints = self.zoom(image, keypoints, scale_factor)\n",
    "\n",
    "        return image, keypoints\n",
    "\n",
    "    def rotate_keypoints(self, keypoints, angle):\n",
    "        radians = torch.tensor(angle * np.pi / 180)\n",
    "        rotation_matrix = torch.tensor([\n",
    "            [torch.cos(radians), -torch.sin(radians)],\n",
    "            [torch.sin(radians), torch.cos(radians)]\n",
    "        ])\n",
    "        keypoints_xy = keypoints.view(-1, 2)\n",
    "        keypoints_rotated = torch.matmul(keypoints_xy - 112, rotation_matrix) + 112\n",
    "        return keypoints_rotated.view(-1)\n",
    "\n",
    "    def zoom(self, image, keypoints, scale_factor):\n",
    "        _, h, w = image.shape\n",
    "        new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n",
    "        image = TF.resize(image, [new_h, new_w])\n",
    "        image = TF.center_crop(image, [h, w])\n",
    "        keypoints *= scale_factor\n",
    "        return image, keypoints\n",
    "\n",
    "    def random_cutout(self, image):\n",
    "        _, h, w = image.shape\n",
    "        cutout_h = random.randint(10, 50)\n",
    "        cutout_w = random.randint(10, 50)\n",
    "        top = random.randint(0, h - cutout_h)\n",
    "        left = random.randint(0, w - cutout_w)\n",
    "        image[:, top:top + cutout_h, left:left + cutout_w] = 0\n",
    "        return image\n",
    "\n",
    "    def motion_blur(self, image, kernel_size=5, angle=45):\n",
    "        kernel = torch.zeros((kernel_size, kernel_size))\n",
    "        center = kernel_size // 2\n",
    "        kernel[center, :] = 1\n",
    "        kernel = kernel / kernel.sum()\n",
    "        kernel = kernel.unsqueeze(0).unsqueeze(0)\n",
    "        return F.conv2d(image.unsqueeze(0), kernel, padding=kernel_size // 2).squeeze(0)\n",
    "\n",
    "    def random_jitter(self, image, max_jitter=0.1):\n",
    "        noise = torch.randn_like(image) * max_jitter\n",
    "        image = image + noise\n",
    "        return torch.clamp(image, 0, 1)\n",
    "\n",
    "\n",
    "# Make dataset\n",
    "dataset = facemapdataset()  # This will now automatically filter out entries with NaN values\n",
    "\n",
    "x = dataset[0][0]\n",
    "dim = x.shape[-1]\n",
    "print('Using %d size of images' % dim)\n",
    "\n",
    "N = len(dataset)\n",
    "\n",
    "# Random sampling for train/valid/test splits\n",
    "indices = np.random.permutation(N)\n",
    "train_indices = indices[:int(0.6 * N)]\n",
    "valid_indices = indices[int(0.6 * N):int(0.8 * N)]\n",
    "test_indices = indices[int(0.8 * N):]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Initialize loss and metrics\n",
    "loss_fun = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Initialize input dimensions\n",
    "num_train = len(train_sampler)\n",
    "num_valid = len(valid_sampler)\n",
    "num_test = len(test_sampler)\n",
    "print(\"Num. train = %d, Num. val = %d, Num. test = %d\" % (num_train, num_valid, num_test))\n",
    "\n",
    "# Initialize dataloaders\n",
    "loader_train = DataLoader(dataset=dataset, drop_last=False, num_workers=0,\n",
    "                          batch_size=batch_size, pin_memory=True, sampler=train_sampler)\n",
    "loader_valid = DataLoader(dataset=dataset, drop_last=True, num_workers=0,\n",
    "                          batch_size=batch_size, pin_memory=True, sampler=valid_sampler)\n",
    "loader_test = DataLoader(dataset=dataset, drop_last=True, num_workers=0,\n",
    "                         batch_size=1, pin_memory=True, sampler=test_sampler)\n",
    "\n",
    "nValid = len(loader_valid)\n",
    "nTrain = len(loader_train)\n",
    "nTest = len(loader_test)\n",
    "\n",
    "### Hyperparameters\n",
    "lr = 5e-4\n",
    "num_epochs = 300\n",
    "num_input_channels = 1  # Change this to the desired number of input channels\n",
    "num_output_classes = 24  # Change this to the desired number of output classes\n",
    "\n",
    "model = timm.create_model('vit_base_patch8_224',\n",
    "                          pretrained=True, in_chans=1, num_classes=num_output_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "nParam = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters:%d M\" % (nParam / 1e6))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "minLoss = 1e6\n",
    "convIter = 0\n",
    "patience = 1000\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(loader_train):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        scores = F.softplus(model(inputs))\n",
    "        loss = loss_fun(\n",
    "            torch.log(scores[labels != 0]), torch.log(F.softplus(labels[labels != 0]))\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, num_epochs, i + 1, nTrain, loss.item()))\n",
    "        tr_loss += loss.item()\n",
    "    train_loss.append(tr_loss / (i + 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for i, (inputs, labels) in enumerate(loader_valid):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            scores = F.softplus(model(inputs))\n",
    "            loss = loss_fun(\n",
    "                torch.log(scores[labels != 0]),\n",
    "                torch.log(F.softplus(labels[labels != 0])),\n",
    "\n",
    "            )\n",
    "            val_loss += loss.item()\n",
    "        val_loss = val_loss / (i + 1)\n",
    "        \n",
    "        valid_loss.append(val_loss)\n",
    "\n",
    "        print('Val. loss :%.4f' % val_loss)\n",
    "        \n",
    "        img = inputs.squeeze().detach().cpu().numpy()\n",
    "        pred = scores.squeeze().detach().cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        for i in range(batch_size):\n",
    "            plt.subplot(1, batch_size, i + 1)\n",
    "            plt.imshow(img[i], cmap='gray')\n",
    "            plt.plot(pred[i, ::2], pred[i, 1::2], 'x', c='tab:red', label='pred.')\n",
    "            plt.plot(labels[i, ::2], labels[i, 1::2], 'o', c='tab:green', label='label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('logs/epoch_%03d.jpg' % epoch)\n",
    "        plt.close()\n",
    "            \n",
    "        if minLoss > val_loss:\n",
    "            convEpoch = epoch\n",
    "            minLoss = val_loss\n",
    "            convIter = 0\n",
    "            torch.save(model.state_dict(), 'models/best_model.pt')\n",
    "        else:\n",
    "            convIter += 1\n",
    "\n",
    "        if convIter == patience:\n",
    "            print('Converged at epoch %d with val. loss %.4f' % (convEpoch + 1, minLoss))\n",
    "            break\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(train_loss, label='Training')\n",
    "plt.plot(valid_loss, label='Valid')\n",
    "plt.plot(convEpoch, valid_loss[convEpoch], 'x', label='Final Model')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curve.pdf')\n",
    "\n",
    "### Load best model for inference\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader_test):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        scores = F.softplus(model(inputs))\n",
    "        loss = loss_fun(torch.log(scores), torch.log(F.softplus(labels)))\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        img = inputs.squeeze().detach().cpu().numpy()\n",
    "        pred = scores.squeeze().detach().cpu().numpy()\n",
    "        labels = labels.squeeze().cpu().numpy()\n",
    "        plt.clf()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.plot(pred[::2], pred[1::2], 'x', c='tab:red')\n",
    "        plt.plot(labels[::2], labels[1::2], 'o', c='tab:green')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('preds/test_%03d.jpg' % i)\n",
    "        plt.close()\n",
    "\n",
    "    val_loss = val_loss / (i + 1)\n",
    "    print('Test. loss :%.4f' % val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 224 size of images\n",
      "Num. train = 165, Num. val = 55, Num. test = 55\n",
      "Number of parameters:85 M\n",
      "Epoch [1/300], Step [1/42], Loss: 2366.9194\n",
      "Epoch [1/300], Step [2/42], Loss: 2034.3782\n",
      "Epoch [1/300], Step [3/42], Loss: 1308.1287\n",
      "Epoch [1/300], Step [4/42], Loss: 1051.7107\n",
      "Epoch [1/300], Step [5/42], Loss: 759.5363\n",
      "Epoch [1/300], Step [6/42], Loss: 717.7383\n",
      "Epoch [1/300], Step [7/42], Loss: 553.8943\n",
      "Epoch [1/300], Step [8/42], Loss: 561.1935\n",
      "Epoch [1/300], Step [9/42], Loss: 480.5566\n",
      "Epoch [1/300], Step [10/42], Loss: 466.3644\n",
      "Epoch [1/300], Step [11/42], Loss: 421.8497\n",
      "Epoch [1/300], Step [12/42], Loss: 460.6293\n",
      "Epoch [1/300], Step [13/42], Loss: 349.2500\n",
      "Epoch [1/300], Step [14/42], Loss: 419.2104\n",
      "Epoch [1/300], Step [15/42], Loss: 295.9288\n",
      "Epoch [1/300], Step [16/42], Loss: 340.0952\n",
      "Epoch [1/300], Step [17/42], Loss: 374.8594\n",
      "Epoch [1/300], Step [18/42], Loss: 316.1287\n",
      "Epoch [1/300], Step [19/42], Loss: 340.6282\n",
      "Epoch [1/300], Step [20/42], Loss: 346.9501\n",
      "Epoch [1/300], Step [21/42], Loss: 295.5897\n",
      "Epoch [1/300], Step [22/42], Loss: 320.8575\n",
      "Epoch [1/300], Step [23/42], Loss: 349.7810\n",
      "Epoch [1/300], Step [24/42], Loss: 273.7501\n",
      "Epoch [1/300], Step [25/42], Loss: 252.8811\n",
      "Epoch [1/300], Step [26/42], Loss: 268.5345\n",
      "Epoch [1/300], Step [27/42], Loss: 275.5772\n",
      "Epoch [1/300], Step [28/42], Loss: 207.4943\n",
      "Epoch [1/300], Step [29/42], Loss: 258.5659\n",
      "Epoch [1/300], Step [30/42], Loss: 220.7022\n",
      "Epoch [1/300], Step [31/42], Loss: 327.4241\n",
      "Epoch [1/300], Step [32/42], Loss: 213.2798\n",
      "Epoch [1/300], Step [33/42], Loss: 208.3811\n",
      "Epoch [1/300], Step [34/42], Loss: 201.6911\n",
      "Epoch [1/300], Step [35/42], Loss: 238.0256\n",
      "Epoch [1/300], Step [36/42], Loss: 177.6465\n",
      "Epoch [1/300], Step [37/42], Loss: 232.9856\n",
      "Epoch [1/300], Step [38/42], Loss: 210.0414\n",
      "Epoch [1/300], Step [39/42], Loss: 173.2690\n",
      "Epoch [1/300], Step [40/42], Loss: 196.6861\n",
      "Epoch [1/300], Step [41/42], Loss: 127.5524\n",
      "Epoch [1/300], Step [42/42], Loss: 32.8455\n",
      "Val. loss :195.4256\n",
      "Epoch [2/300], Step [1/42], Loss: 181.7972\n",
      "Epoch [2/300], Step [2/42], Loss: 139.0043\n",
      "Epoch [2/300], Step [3/42], Loss: 119.0381\n",
      "Epoch [2/300], Step [4/42], Loss: 179.1697\n",
      "Epoch [2/300], Step [5/42], Loss: 164.8374\n",
      "Epoch [2/300], Step [6/42], Loss: 172.1107\n",
      "Epoch [2/300], Step [7/42], Loss: 180.3487\n",
      "Epoch [2/300], Step [8/42], Loss: 170.2453\n",
      "Epoch [2/300], Step [9/42], Loss: 169.3042\n",
      "Epoch [2/300], Step [10/42], Loss: 196.4156\n",
      "Epoch [2/300], Step [11/42], Loss: 211.8931\n",
      "Epoch [2/300], Step [12/42], Loss: 155.3888\n",
      "Epoch [2/300], Step [13/42], Loss: 152.9481\n",
      "Epoch [2/300], Step [14/42], Loss: 154.0793\n",
      "Epoch [2/300], Step [15/42], Loss: 174.0520\n",
      "Epoch [2/300], Step [16/42], Loss: 160.2290\n",
      "Epoch [2/300], Step [17/42], Loss: 144.7636\n",
      "Epoch [2/300], Step [18/42], Loss: 150.0110\n",
      "Epoch [2/300], Step [19/42], Loss: 151.8403\n",
      "Epoch [2/300], Step [20/42], Loss: 156.2386\n",
      "Epoch [2/300], Step [21/42], Loss: 202.9832\n",
      "Epoch [2/300], Step [22/42], Loss: 125.2305\n",
      "Epoch [2/300], Step [23/42], Loss: 169.1642\n",
      "Epoch [2/300], Step [24/42], Loss: 142.9878\n",
      "Epoch [2/300], Step [25/42], Loss: 147.0603\n",
      "Epoch [2/300], Step [26/42], Loss: 135.5043\n",
      "Epoch [2/300], Step [27/42], Loss: 155.1814\n",
      "Epoch [2/300], Step [28/42], Loss: 139.4040\n",
      "Epoch [2/300], Step [29/42], Loss: 107.3139\n",
      "Epoch [2/300], Step [30/42], Loss: 169.3506\n",
      "Epoch [2/300], Step [31/42], Loss: 121.8032\n",
      "Epoch [2/300], Step [32/42], Loss: 109.0852\n",
      "Epoch [2/300], Step [33/42], Loss: 137.1393\n",
      "Epoch [2/300], Step [34/42], Loss: 140.1311\n",
      "Epoch [2/300], Step [35/42], Loss: 153.3131\n",
      "Epoch [2/300], Step [36/42], Loss: 148.1949\n",
      "Epoch [2/300], Step [37/42], Loss: 136.0868\n",
      "Epoch [2/300], Step [38/42], Loss: 141.7843\n",
      "Epoch [2/300], Step [39/42], Loss: 181.7415\n",
      "Epoch [2/300], Step [40/42], Loss: 103.6946\n",
      "Epoch [2/300], Step [41/42], Loss: 132.1339\n",
      "Epoch [2/300], Step [42/42], Loss: 23.3952\n",
      "Val. loss :123.8470\n",
      "Epoch [3/300], Step [1/42], Loss: 135.9761\n",
      "Epoch [3/300], Step [2/42], Loss: 105.3733\n",
      "Epoch [3/300], Step [3/42], Loss: 136.3551\n",
      "Epoch [3/300], Step [4/42], Loss: 149.3072\n",
      "Epoch [3/300], Step [5/42], Loss: 115.3873\n",
      "Epoch [3/300], Step [6/42], Loss: 137.6975\n",
      "Epoch [3/300], Step [7/42], Loss: 130.1733\n",
      "Epoch [3/300], Step [8/42], Loss: 126.4885\n",
      "Epoch [3/300], Step [9/42], Loss: 141.7715\n",
      "Epoch [3/300], Step [10/42], Loss: 111.1302\n",
      "Epoch [3/300], Step [11/42], Loss: 104.8702\n",
      "Epoch [3/300], Step [12/42], Loss: 112.8794\n",
      "Epoch [3/300], Step [13/42], Loss: 97.1670\n",
      "Epoch [3/300], Step [14/42], Loss: 126.6875\n",
      "Epoch [3/300], Step [15/42], Loss: 93.9773\n",
      "Epoch [3/300], Step [16/42], Loss: 92.3282\n",
      "Epoch [3/300], Step [17/42], Loss: 103.1842\n",
      "Epoch [3/300], Step [18/42], Loss: 99.9230\n",
      "Epoch [3/300], Step [19/42], Loss: 120.8701\n",
      "Epoch [3/300], Step [20/42], Loss: 99.1723\n",
      "Epoch [3/300], Step [21/42], Loss: 82.2276\n",
      "Epoch [3/300], Step [22/42], Loss: 119.9560\n",
      "Epoch [3/300], Step [23/42], Loss: 96.2144\n",
      "Epoch [3/300], Step [24/42], Loss: 104.8038\n",
      "Epoch [3/300], Step [25/42], Loss: 67.3423\n",
      "Epoch [3/300], Step [26/42], Loss: 71.2171\n",
      "Epoch [3/300], Step [27/42], Loss: 119.0305\n",
      "Epoch [3/300], Step [28/42], Loss: 85.9392\n",
      "Epoch [3/300], Step [29/42], Loss: 79.7293\n",
      "Epoch [3/300], Step [30/42], Loss: 102.0250\n",
      "Epoch [3/300], Step [31/42], Loss: 86.8656\n",
      "Epoch [3/300], Step [32/42], Loss: 94.0566\n",
      "Epoch [3/300], Step [33/42], Loss: 104.7809\n",
      "Epoch [3/300], Step [34/42], Loss: 110.3589\n",
      "Epoch [3/300], Step [35/42], Loss: 112.4361\n",
      "Epoch [3/300], Step [36/42], Loss: 99.2408\n",
      "Epoch [3/300], Step [37/42], Loss: 89.3790\n",
      "Epoch [3/300], Step [38/42], Loss: 71.7736\n",
      "Epoch [3/300], Step [39/42], Loss: 83.7464\n",
      "Epoch [3/300], Step [40/42], Loss: 92.2264\n",
      "Epoch [3/300], Step [41/42], Loss: 90.1982\n",
      "Epoch [3/300], Step [42/42], Loss: 27.3668\n",
      "Val. loss :93.4122\n",
      "Epoch [4/300], Step [1/42], Loss: 96.5331\n",
      "Epoch [4/300], Step [2/42], Loss: 127.8575\n",
      "Epoch [4/300], Step [3/42], Loss: 91.7419\n",
      "Epoch [4/300], Step [4/42], Loss: 76.9688\n",
      "Epoch [4/300], Step [5/42], Loss: 100.0553\n",
      "Epoch [4/300], Step [6/42], Loss: 92.3823\n",
      "Epoch [4/300], Step [7/42], Loss: 76.9993\n",
      "Epoch [4/300], Step [8/42], Loss: 76.9930\n",
      "Epoch [4/300], Step [9/42], Loss: 88.5593\n",
      "Epoch [4/300], Step [10/42], Loss: 106.6907\n",
      "Epoch [4/300], Step [11/42], Loss: 86.9711\n",
      "Epoch [4/300], Step [12/42], Loss: 93.5852\n",
      "Epoch [4/300], Step [13/42], Loss: 76.4805\n",
      "Epoch [4/300], Step [14/42], Loss: 62.5856\n",
      "Epoch [4/300], Step [15/42], Loss: 85.0267\n",
      "Epoch [4/300], Step [16/42], Loss: 65.9315\n",
      "Epoch [4/300], Step [17/42], Loss: 101.7059\n",
      "Epoch [4/300], Step [18/42], Loss: 66.0542\n",
      "Epoch [4/300], Step [19/42], Loss: 87.0205\n",
      "Epoch [4/300], Step [20/42], Loss: 71.7079\n",
      "Epoch [4/300], Step [21/42], Loss: 82.5848\n",
      "Epoch [4/300], Step [22/42], Loss: 70.5634\n",
      "Epoch [4/300], Step [23/42], Loss: 91.6115\n",
      "Epoch [4/300], Step [24/42], Loss: 86.0489\n",
      "Epoch [4/300], Step [25/42], Loss: 75.3301\n",
      "Epoch [4/300], Step [26/42], Loss: 77.2309\n",
      "Epoch [4/300], Step [27/42], Loss: 70.6992\n",
      "Epoch [4/300], Step [28/42], Loss: 67.3021\n",
      "Epoch [4/300], Step [29/42], Loss: 73.8168\n",
      "Epoch [4/300], Step [30/42], Loss: 91.5231\n",
      "Epoch [4/300], Step [31/42], Loss: 76.2922\n",
      "Epoch [4/300], Step [32/42], Loss: 71.2112\n",
      "Epoch [4/300], Step [33/42], Loss: 49.3386\n",
      "Epoch [4/300], Step [34/42], Loss: 76.6432\n",
      "Epoch [4/300], Step [35/42], Loss: 75.3286\n",
      "Epoch [4/300], Step [36/42], Loss: 75.8995\n",
      "Epoch [4/300], Step [37/42], Loss: 61.7343\n",
      "Epoch [4/300], Step [38/42], Loss: 76.4163\n",
      "Epoch [4/300], Step [39/42], Loss: 78.7385\n",
      "Epoch [4/300], Step [40/42], Loss: 74.0204\n",
      "Epoch [4/300], Step [41/42], Loss: 75.8596\n",
      "Epoch [4/300], Step [42/42], Loss: 15.7230\n",
      "Val. loss :73.1347\n",
      "Epoch [5/300], Step [1/42], Loss: 78.2722\n",
      "Epoch [5/300], Step [2/42], Loss: 85.2649\n",
      "Epoch [5/300], Step [3/42], Loss: 71.1437\n",
      "Epoch [5/300], Step [4/42], Loss: 62.2628\n",
      "Epoch [5/300], Step [5/42], Loss: 83.4586\n",
      "Epoch [5/300], Step [6/42], Loss: 58.5002\n",
      "Epoch [5/300], Step [7/42], Loss: 73.3192\n",
      "Epoch [5/300], Step [8/42], Loss: 78.1778\n",
      "Epoch [5/300], Step [9/42], Loss: 65.2096\n",
      "Epoch [5/300], Step [10/42], Loss: 68.7333\n",
      "Epoch [5/300], Step [11/42], Loss: 67.8691\n",
      "Epoch [5/300], Step [12/42], Loss: 73.0509\n",
      "Epoch [5/300], Step [13/42], Loss: 67.9695\n",
      "Epoch [5/300], Step [14/42], Loss: 81.8918\n",
      "Epoch [5/300], Step [15/42], Loss: 60.6108\n",
      "Epoch [5/300], Step [16/42], Loss: 84.4270\n",
      "Epoch [5/300], Step [17/42], Loss: 72.7477\n",
      "Epoch [5/300], Step [18/42], Loss: 71.4730\n",
      "Epoch [5/300], Step [19/42], Loss: 64.9444\n",
      "Epoch [5/300], Step [20/42], Loss: 65.6168\n",
      "Epoch [5/300], Step [21/42], Loss: 55.6602\n",
      "Epoch [5/300], Step [22/42], Loss: 60.6717\n",
      "Epoch [5/300], Step [23/42], Loss: 71.6773\n",
      "Epoch [5/300], Step [24/42], Loss: 68.3598\n",
      "Epoch [5/300], Step [25/42], Loss: 64.5230\n",
      "Epoch [5/300], Step [26/42], Loss: 59.9002\n",
      "Epoch [5/300], Step [27/42], Loss: 59.2455\n",
      "Epoch [5/300], Step [28/42], Loss: 59.9178\n",
      "Epoch [5/300], Step [29/42], Loss: 56.1289\n",
      "Epoch [5/300], Step [30/42], Loss: 69.2956\n",
      "Epoch [5/300], Step [31/42], Loss: 61.2216\n",
      "Epoch [5/300], Step [32/42], Loss: 67.1326\n",
      "Epoch [5/300], Step [33/42], Loss: 62.2489\n",
      "Epoch [5/300], Step [34/42], Loss: 60.5530\n",
      "Epoch [5/300], Step [35/42], Loss: 49.5532\n",
      "Epoch [5/300], Step [36/42], Loss: 62.0435\n",
      "Epoch [5/300], Step [37/42], Loss: 64.6935\n",
      "Epoch [5/300], Step [38/42], Loss: 64.7077\n",
      "Epoch [5/300], Step [39/42], Loss: 59.3710\n",
      "Epoch [5/300], Step [40/42], Loss: 55.1948\n",
      "Epoch [5/300], Step [41/42], Loss: 66.3625\n",
      "Epoch [5/300], Step [42/42], Loss: 9.2007\n",
      "Val. loss :63.0733\n",
      "Epoch [6/300], Step [1/42], Loss: 68.0749\n",
      "Epoch [6/300], Step [2/42], Loss: 57.4724\n",
      "Epoch [6/300], Step [3/42], Loss: 61.1136\n",
      "Epoch [6/300], Step [4/42], Loss: 58.1218\n",
      "Epoch [6/300], Step [5/42], Loss: 48.5410\n",
      "Epoch [6/300], Step [6/42], Loss: 72.5994\n",
      "Epoch [6/300], Step [7/42], Loss: 48.5102\n",
      "Epoch [6/300], Step [8/42], Loss: 58.5368\n",
      "Epoch [6/300], Step [9/42], Loss: 62.2985\n",
      "Epoch [6/300], Step [10/42], Loss: 59.7435\n",
      "Epoch [6/300], Step [11/42], Loss: 57.7445\n",
      "Epoch [6/300], Step [12/42], Loss: 59.9049\n",
      "Epoch [6/300], Step [13/42], Loss: 62.8872\n",
      "Epoch [6/300], Step [14/42], Loss: 60.8398\n",
      "Epoch [6/300], Step [15/42], Loss: 72.8203\n",
      "Epoch [6/300], Step [16/42], Loss: 62.4570\n",
      "Epoch [6/300], Step [17/42], Loss: 54.7220\n",
      "Epoch [6/300], Step [18/42], Loss: 51.0810\n",
      "Epoch [6/300], Step [19/42], Loss: 62.4140\n",
      "Epoch [6/300], Step [20/42], Loss: 56.6540\n",
      "Epoch [6/300], Step [21/42], Loss: 54.6744\n",
      "Epoch [6/300], Step [22/42], Loss: 60.4473\n",
      "Epoch [6/300], Step [23/42], Loss: 44.7951\n",
      "Epoch [6/300], Step [24/42], Loss: 51.8024\n",
      "Epoch [6/300], Step [25/42], Loss: 66.4678\n",
      "Epoch [6/300], Step [26/42], Loss: 65.3359\n",
      "Epoch [6/300], Step [27/42], Loss: 48.6656\n",
      "Epoch [6/300], Step [28/42], Loss: 61.2138\n",
      "Epoch [6/300], Step [29/42], Loss: 51.5863\n",
      "Epoch [6/300], Step [30/42], Loss: 54.3115\n",
      "Epoch [6/300], Step [31/42], Loss: 52.3415\n",
      "Epoch [6/300], Step [32/42], Loss: 53.9007\n",
      "Epoch [6/300], Step [33/42], Loss: 52.1919\n",
      "Epoch [6/300], Step [34/42], Loss: 66.4997\n",
      "Epoch [6/300], Step [35/42], Loss: 58.8495\n",
      "Epoch [6/300], Step [36/42], Loss: 48.2008\n",
      "Epoch [6/300], Step [37/42], Loss: 55.7995\n",
      "Epoch [6/300], Step [38/42], Loss: 55.7966\n",
      "Epoch [6/300], Step [39/42], Loss: 52.3914\n",
      "Epoch [6/300], Step [40/42], Loss: 56.4177\n",
      "Epoch [6/300], Step [41/42], Loss: 46.4473\n",
      "Epoch [6/300], Step [42/42], Loss: 11.9745\n",
      "Val. loss :55.5503\n",
      "Epoch [7/300], Step [1/42], Loss: 50.2457\n",
      "Epoch [7/300], Step [2/42], Loss: 57.2425\n",
      "Epoch [7/300], Step [3/42], Loss: 49.2951\n",
      "Epoch [7/300], Step [4/42], Loss: 44.3677\n",
      "Epoch [7/300], Step [5/42], Loss: 53.6322\n",
      "Epoch [7/300], Step [6/42], Loss: 42.6628\n",
      "Epoch [7/300], Step [7/42], Loss: 57.9556\n",
      "Epoch [7/300], Step [8/42], Loss: 54.6525\n",
      "Epoch [7/300], Step [9/42], Loss: 57.5006\n",
      "Epoch [7/300], Step [10/42], Loss: 53.1866\n",
      "Epoch [7/300], Step [11/42], Loss: 56.1508\n",
      "Epoch [7/300], Step [12/42], Loss: 47.4189\n",
      "Epoch [7/300], Step [13/42], Loss: 51.7840\n",
      "Epoch [7/300], Step [14/42], Loss: 55.5253\n",
      "Epoch [7/300], Step [15/42], Loss: 58.8822\n",
      "Epoch [7/300], Step [16/42], Loss: 55.6634\n",
      "Epoch [7/300], Step [17/42], Loss: 48.7499\n",
      "Epoch [7/300], Step [18/42], Loss: 55.7872\n",
      "Epoch [7/300], Step [19/42], Loss: 47.1019\n",
      "Epoch [7/300], Step [20/42], Loss: 56.3619\n",
      "Epoch [7/300], Step [21/42], Loss: 41.2627\n",
      "Epoch [7/300], Step [22/42], Loss: 43.4793\n",
      "Epoch [7/300], Step [23/42], Loss: 50.5883\n",
      "Epoch [7/300], Step [24/42], Loss: 48.3992\n",
      "Epoch [7/300], Step [25/42], Loss: 63.6408\n",
      "Epoch [7/300], Step [26/42], Loss: 55.3049\n",
      "Epoch [7/300], Step [27/42], Loss: 67.3189\n",
      "Epoch [7/300], Step [28/42], Loss: 43.5290\n",
      "Epoch [7/300], Step [29/42], Loss: 53.7504\n",
      "Epoch [7/300], Step [30/42], Loss: 48.4617\n",
      "Epoch [7/300], Step [31/42], Loss: 45.7484\n",
      "Epoch [7/300], Step [32/42], Loss: 45.3208\n",
      "Epoch [7/300], Step [33/42], Loss: 52.4145\n",
      "Epoch [7/300], Step [34/42], Loss: 49.4676\n",
      "Epoch [7/300], Step [35/42], Loss: 63.6192\n",
      "Epoch [7/300], Step [36/42], Loss: 49.5537\n",
      "Epoch [7/300], Step [37/42], Loss: 42.4428\n",
      "Epoch [7/300], Step [38/42], Loss: 49.1308\n",
      "Epoch [7/300], Step [39/42], Loss: 46.5207\n",
      "Epoch [7/300], Step [40/42], Loss: 46.9814\n",
      "Epoch [7/300], Step [41/42], Loss: 45.8134\n",
      "Epoch [7/300], Step [42/42], Loss: 11.9377\n",
      "Val. loss :50.1469\n",
      "Epoch [8/300], Step [1/42], Loss: 49.3076\n",
      "Epoch [8/300], Step [2/42], Loss: 56.5273\n",
      "Epoch [8/300], Step [3/42], Loss: 54.1847\n",
      "Epoch [8/300], Step [4/42], Loss: 58.6287\n",
      "Epoch [8/300], Step [5/42], Loss: 47.1401\n",
      "Epoch [8/300], Step [6/42], Loss: 50.4968\n",
      "Epoch [8/300], Step [7/42], Loss: 38.9483\n",
      "Epoch [8/300], Step [8/42], Loss: 54.3330\n",
      "Epoch [8/300], Step [9/42], Loss: 43.0619\n",
      "Epoch [8/300], Step [10/42], Loss: 50.0398\n",
      "Epoch [8/300], Step [11/42], Loss: 49.3080\n",
      "Epoch [8/300], Step [12/42], Loss: 55.6874\n",
      "Epoch [8/300], Step [13/42], Loss: 52.6154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 194\u001b[0m\n\u001b[0;32m    191\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    192\u001b[0m scores \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(model(inputs))\n\u001b[0;32m    193\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fun(\n\u001b[1;32m--> 194\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mlog(F\u001b[38;5;241m.\u001b[39msoftplus(labels[labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    196\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    197\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class facemapdataset(Dataset):\n",
    "    def __init__(self, data_file=\"data/dolensek_facemap_224.pt\", transform=None):\n",
    "        super().__init__()\n",
    "        self.data, self.targets = torch.load(data_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Filter out entries with NaN labels\n",
    "        valid_indices = [i for i, label in enumerate(self.targets) if not np.any(np.isnan(label))]\n",
    "        self.data = self.data[valid_indices]\n",
    "        self.targets = self.targets[valid_indices]\n",
    "        self.targets = torch.Tensor(self.targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 10x augmentation as in the segmentation script\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_index = index % len(self.data)  # Map index to original data\n",
    "        aug_type = index // len(self.data)   # Determine augmentation type\n",
    "\n",
    "        image, keypoints = self.data[base_index].clone(), self.targets[base_index].clone()\n",
    "\n",
    "        # Apply the augmentation based on the segmentation script\n",
    "        if aug_type == 1:  # Flipping\n",
    "            image = image.flip([2])  # Horizontal flip\n",
    "            keypoints[::2] = 224 - keypoints[::2]  # Adjust x-coordinates for flip\n",
    "\n",
    "        elif aug_type == 2:  # Rotation\n",
    "            angle = random.uniform(-30, 30)\n",
    "            image = TF.rotate(image, angle)\n",
    "            keypoints = self.rotate_keypoints(keypoints, angle)\n",
    "\n",
    "        elif aug_type == 3:  # Zoom\n",
    "            scale_factor = random.uniform(0.8, 1.5)\n",
    "            image, keypoints = self.zoom(image, keypoints, scale_factor)\n",
    "\n",
    "        elif aug_type == 4:  # Gaussian Blur\n",
    "            image = TF.gaussian_blur(image, kernel_size=3)\n",
    "\n",
    "        elif aug_type == 5:  # Random cutout\n",
    "            image = self.random_cutout(image)\n",
    "\n",
    "        elif aug_type == 6:  # Adjust brightness\n",
    "            image = TF.adjust_brightness(image, random.uniform(0.8, 1.2))\n",
    "\n",
    "        elif aug_type == 7:  # Motion blur\n",
    "            image = self.motion_blur(image)\n",
    "\n",
    "        elif aug_type == 8:  # Random jitter\n",
    "            image = self.random_jitter(image)\n",
    "\n",
    "        elif aug_type == 9:  # Combined transformations\n",
    "            angle = random.uniform(-15, 15)\n",
    "            scale_factor = random.uniform(0.9, 1.1)\n",
    "            image = TF.adjust_brightness(image, random.uniform(0.9, 1.1))\n",
    "            image = self.random_jitter(image)\n",
    "            image = TF.rotate(image, angle)\n",
    "            keypoints = self.rotate_keypoints(keypoints, angle)\n",
    "            image, keypoints = self.zoom(image, keypoints, scale_factor)\n",
    "\n",
    "        return image, keypoints\n",
    "\n",
    "    def rotate_keypoints(self, keypoints, angle):\n",
    "        \"\"\"Rotate keypoints using a rotation matrix.\"\"\"\n",
    "        radians = torch.tensor(angle * np.pi / 180)\n",
    "        rotation_matrix = torch.tensor([\n",
    "            [torch.cos(radians), -torch.sin(radians)],\n",
    "            [torch.sin(radians), torch.cos(radians)]\n",
    "        ])\n",
    "        keypoints_xy = keypoints.view(-1, 2)  # Reshape to N x 2\n",
    "        keypoints_rotated = torch.matmul(keypoints_xy - 112, rotation_matrix) + 112\n",
    "        return keypoints_rotated.view(-1)\n",
    "\n",
    "    def zoom(self, image, keypoints, scale_factor):\n",
    "        \"\"\"Apply zoom to both image and keypoints.\"\"\"\n",
    "        _, h, w = image.shape\n",
    "        new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n",
    "        image = TF.resize(image, [new_h, new_w])\n",
    "        image = TF.center_crop(image, [h, w])\n",
    "        keypoints *= scale_factor  # Scale keypoints proportionally\n",
    "        return image, keypoints\n",
    "\n",
    "    def random_cutout(self, image):\n",
    "        \"\"\"Apply random cutout to the image.\"\"\"\n",
    "        _, h, w = image.shape\n",
    "        cutout_h = random.randint(10, 50)\n",
    "        cutout_w = random.randint(10, 50)\n",
    "        top = random.randint(0, h - cutout_h)\n",
    "        left = random.randint(0, w - cutout_w)\n",
    "        image[:, top:top + cutout_h, left:left + cutout_w] = 0\n",
    "        return image\n",
    "\n",
    "    def motion_blur(self, image, kernel_size=5, angle=45):\n",
    "        \"\"\"Apply motion blur to the image.\"\"\"\n",
    "        kernel = torch.zeros((kernel_size, kernel_size))\n",
    "        center = kernel_size // 2\n",
    "        kernel[center, :] = 1\n",
    "        kernel = kernel / kernel.sum()\n",
    "        kernel = kernel.unsqueeze(0).unsqueeze(0)\n",
    "        return F.conv2d(image.unsqueeze(0), kernel, padding=kernel_size // 2).squeeze(0)\n",
    "\n",
    "    def random_jitter(self, image, max_jitter=0.1):\n",
    "        \"\"\"Apply random jitter to the image.\"\"\"\n",
    "        noise = torch.randn_like(image) * max_jitter\n",
    "        image = image + noise\n",
    "        return torch.clamp(image, 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Make dataset\n",
    "dataset = facemapdataset()  # This will now automatically filter out entries with NaN values\n",
    "\n",
    "x = dataset[0][0]\n",
    "dim = x.shape[-1]\n",
    "print('Using %d size of images' % dim)\n",
    "\n",
    "N = len(dataset)\n",
    "\n",
    "# Random sampling for train/valid/test splits\n",
    "indices = np.random.permutation(N)\n",
    "train_indices = indices[:int(0.6 * N)]\n",
    "valid_indices = indices[int(0.6 * N):int(0.8 * N)]\n",
    "test_indices = indices[int(0.8 * N):]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Initialize loss and metrics\n",
    "loss_fun = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Initialize input dimensions\n",
    "num_train = len(train_sampler)\n",
    "num_valid = len(valid_sampler)\n",
    "num_test = len(test_sampler)\n",
    "print(\"Num. train = %d, Num. val = %d, Num. test = %d\" % (num_train, num_valid, num_test))\n",
    "\n",
    "# Initialize dataloaders\n",
    "loader_train = DataLoader(dataset=dataset, drop_last=False, num_workers=0,\n",
    "                          batch_size=batch_size, pin_memory=True, sampler=train_sampler)\n",
    "loader_valid = DataLoader(dataset=dataset, drop_last=True, num_workers=0,\n",
    "                          batch_size=batch_size, pin_memory=True, sampler=valid_sampler)\n",
    "loader_test = DataLoader(dataset=dataset, drop_last=True, num_workers=0,\n",
    "                         batch_size=1, pin_memory=True, sampler=test_sampler)\n",
    "\n",
    "nValid = len(loader_valid)\n",
    "nTrain = len(loader_train)\n",
    "nTest = len(loader_test)\n",
    "\n",
    "### Hyperparameters\n",
    "lr = 5e-4\n",
    "num_epochs = 300\n",
    "num_input_channels = 1  # Change this to the desired number of input channels\n",
    "num_output_classes = 24  # Change this to the desired number of output classes\n",
    "\n",
    "model = timm.create_model('vit_base_patch8_224',\n",
    "                          pretrained=True, in_chans=1, num_classes=num_output_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "nParam = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters:%d M\" % (nParam / 1e6))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "minLoss = 1e6\n",
    "convIter = 0\n",
    "patience = 1000\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(loader_train):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        scores = F.softplus(model(inputs))\n",
    "        loss = loss_fun(\n",
    "            torch.log(scores[labels != 0]), torch.log(F.softplus(labels[labels != 0]))\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, num_epochs, i + 1, nTrain, loss.item()))\n",
    "        tr_loss += loss.item()\n",
    "    train_loss.append(tr_loss / (i + 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for i, (inputs, labels) in enumerate(loader_valid):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            scores = F.softplus(model(inputs))\n",
    "            loss = loss_fun(\n",
    "                torch.log(scores[labels != 0]),\n",
    "                torch.log(F.softplus(labels[labels != 0])),\n",
    "\n",
    "            )\n",
    "            val_loss += loss.item()\n",
    "        val_loss = val_loss / (i + 1)\n",
    "        \n",
    "        valid_loss.append(val_loss)\n",
    "\n",
    "        print('Val. loss :%.4f' % val_loss)\n",
    "        \n",
    "        img = inputs.squeeze().detach().cpu().numpy()\n",
    "        pred = scores.squeeze().detach().cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        for i in range(batch_size):\n",
    "            plt.subplot(1, batch_size, i + 1)\n",
    "            plt.imshow(img[i], cmap='gray')\n",
    "            plt.plot(pred[i, ::2], pred[i, 1::2], 'x', c='tab:red', label='pred.')\n",
    "            plt.plot(labels[i, ::2], labels[i, 1::2], 'o', c='tab:green', label='label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('logs/epoch_%03d.jpg' % epoch)\n",
    "        plt.close()\n",
    "            \n",
    "        if minLoss > val_loss:\n",
    "            convEpoch = epoch\n",
    "            minLoss = val_loss\n",
    "            convIter = 0\n",
    "            torch.save(model.state_dict(), 'models/best_model.pt')\n",
    "        else:\n",
    "            convIter += 1\n",
    "\n",
    "        if convIter == patience:\n",
    "            print('Converged at epoch %d with val. loss %.4f' % (convEpoch + 1, minLoss))\n",
    "            break\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(train_loss, label='Training')\n",
    "plt.plot(valid_loss, label='Valid')\n",
    "plt.plot(convEpoch, valid_loss[convEpoch], 'x', label='Final Model')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curve.pdf')\n",
    "\n",
    "### Load best model for inference\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader_test):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        scores = F.softplus(model(inputs))\n",
    "        loss = loss_fun(torch.log(scores), torch.log(F.softplus(labels)))\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        img = inputs.squeeze().detach().cpu().numpy()\n",
    "        pred = scores.squeeze().detach().cpu().numpy()\n",
    "        labels = labels.squeeze().cpu().numpy()\n",
    "        plt.clf()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.plot(pred[::2], pred[1::2], 'x', c='tab:red')\n",
    "        plt.plot(labels[::2], labels[1::2], 'o', c='tab:green')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('preds/test_%03d.jpg' % i)\n",
    "        plt.close()\n",
    "\n",
    "    val_loss = val_loss / (i + 1)\n",
    "    print('Test. loss :%.4f' % val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 224 size of images\n",
      "Num. train = 165, Num. val = 55, Num. test = 55\n",
      "Number of parameters:85 M\n",
      "Epoch [1/300], Step [1/42], Loss: 2366.9194\n",
      "Epoch [1/300], Step [2/42], Loss: 2034.3792\n",
      "Epoch [1/300], Step [3/42], Loss: 1308.1312\n",
      "Epoch [1/300], Step [4/42], Loss: 1051.7091\n",
      "Epoch [1/300], Step [5/42], Loss: 759.5433\n",
      "Epoch [1/300], Step [6/42], Loss: 717.7389\n",
      "Epoch [1/300], Step [7/42], Loss: 553.8943\n",
      "Epoch [1/300], Step [8/42], Loss: 561.1953\n",
      "Epoch [1/300], Step [9/42], Loss: 480.5583\n",
      "Epoch [1/300], Step [10/42], Loss: 466.3651\n",
      "Epoch [1/300], Step [11/42], Loss: 421.8498\n",
      "Epoch [1/300], Step [12/42], Loss: 460.6294\n",
      "Epoch [1/300], Step [13/42], Loss: 349.2504\n",
      "Epoch [1/300], Step [14/42], Loss: 419.2108\n",
      "Epoch [1/300], Step [15/42], Loss: 295.9291\n",
      "Epoch [1/300], Step [16/42], Loss: 340.0952\n",
      "Epoch [1/300], Step [17/42], Loss: 374.8594\n",
      "Epoch [1/300], Step [18/42], Loss: 316.1288\n",
      "Epoch [1/300], Step [19/42], Loss: 340.6282\n",
      "Epoch [1/300], Step [20/42], Loss: 346.9500\n",
      "Epoch [1/300], Step [21/42], Loss: 295.5895\n",
      "Epoch [1/300], Step [22/42], Loss: 320.8575\n",
      "Epoch [1/300], Step [23/42], Loss: 349.7809\n",
      "Epoch [1/300], Step [24/42], Loss: 273.7501\n",
      "Epoch [1/300], Step [25/42], Loss: 252.8811\n",
      "Epoch [1/300], Step [26/42], Loss: 268.5345\n",
      "Epoch [1/300], Step [27/42], Loss: 275.5761\n",
      "Epoch [1/300], Step [28/42], Loss: 207.4943\n",
      "Epoch [1/300], Step [29/42], Loss: 258.5657\n",
      "Epoch [1/300], Step [30/42], Loss: 220.7021\n",
      "Epoch [1/300], Step [31/42], Loss: 327.4267\n",
      "Epoch [1/300], Step [32/42], Loss: 213.2797\n",
      "Epoch [1/300], Step [33/42], Loss: 208.3809\n",
      "Epoch [1/300], Step [34/42], Loss: 201.6908\n",
      "Epoch [1/300], Step [35/42], Loss: 238.0253\n",
      "Epoch [1/300], Step [36/42], Loss: 177.6467\n",
      "Epoch [1/300], Step [37/42], Loss: 232.9854\n",
      "Epoch [1/300], Step [38/42], Loss: 210.0415\n",
      "Epoch [1/300], Step [39/42], Loss: 173.2693\n",
      "Epoch [1/300], Step [40/42], Loss: 196.6861\n",
      "Epoch [1/300], Step [41/42], Loss: 127.5528\n",
      "Epoch [1/300], Step [42/42], Loss: 32.8456\n",
      "Val. loss :195.4254\n",
      "Epoch [2/300], Step [1/42], Loss: 181.7972\n",
      "Epoch [2/300], Step [2/42], Loss: 139.0045\n",
      "Epoch [2/300], Step [3/42], Loss: 119.0386\n",
      "Epoch [2/300], Step [4/42], Loss: 179.1694\n",
      "Epoch [2/300], Step [5/42], Loss: 164.8372\n",
      "Epoch [2/300], Step [6/42], Loss: 172.1104\n",
      "Epoch [2/300], Step [7/42], Loss: 180.3484\n",
      "Epoch [2/300], Step [8/42], Loss: 170.2453\n",
      "Epoch [2/300], Step [9/42], Loss: 169.3042\n",
      "Epoch [2/300], Step [10/42], Loss: 196.4158\n",
      "Epoch [2/300], Step [11/42], Loss: 211.8936\n",
      "Epoch [2/300], Step [12/42], Loss: 155.3887\n",
      "Epoch [2/300], Step [13/42], Loss: 152.9480\n",
      "Epoch [2/300], Step [14/42], Loss: 154.0790\n",
      "Epoch [2/300], Step [15/42], Loss: 174.0524\n",
      "Epoch [2/300], Step [16/42], Loss: 160.2295\n",
      "Epoch [2/300], Step [17/42], Loss: 144.7632\n",
      "Epoch [2/300], Step [18/42], Loss: 150.0107\n",
      "Epoch [2/300], Step [19/42], Loss: 151.8401\n",
      "Epoch [2/300], Step [20/42], Loss: 156.2387\n",
      "Epoch [2/300], Step [21/42], Loss: 202.9834\n",
      "Epoch [2/300], Step [22/42], Loss: 125.2302\n",
      "Epoch [2/300], Step [23/42], Loss: 169.1643\n",
      "Epoch [2/300], Step [24/42], Loss: 142.9872\n",
      "Epoch [2/300], Step [25/42], Loss: 147.0600\n",
      "Epoch [2/300], Step [26/42], Loss: 135.5040\n",
      "Epoch [2/300], Step [27/42], Loss: 155.1815\n",
      "Epoch [2/300], Step [28/42], Loss: 139.4039\n",
      "Epoch [2/300], Step [29/42], Loss: 107.3142\n",
      "Epoch [2/300], Step [30/42], Loss: 169.3499\n",
      "Epoch [2/300], Step [31/42], Loss: 121.8034\n",
      "Epoch [2/300], Step [32/42], Loss: 109.0864\n",
      "Epoch [2/300], Step [33/42], Loss: 137.1398\n",
      "Epoch [2/300], Step [34/42], Loss: 140.1310\n",
      "Epoch [2/300], Step [35/42], Loss: 153.3122\n",
      "Epoch [2/300], Step [36/42], Loss: 148.1945\n",
      "Epoch [2/300], Step [37/42], Loss: 136.0865\n",
      "Epoch [2/300], Step [38/42], Loss: 141.7840\n",
      "Epoch [2/300], Step [39/42], Loss: 181.7407\n",
      "Epoch [2/300], Step [40/42], Loss: 103.6955\n",
      "Epoch [2/300], Step [41/42], Loss: 132.1337\n",
      "Epoch [2/300], Step [42/42], Loss: 23.3955\n",
      "Val. loss :123.8471\n",
      "Epoch [3/300], Step [1/42], Loss: 135.9760\n",
      "Epoch [3/300], Step [2/42], Loss: 105.3735\n",
      "Epoch [3/300], Step [3/42], Loss: 136.3550\n",
      "Epoch [3/300], Step [4/42], Loss: 149.3069\n",
      "Epoch [3/300], Step [5/42], Loss: 115.3873\n",
      "Epoch [3/300], Step [6/42], Loss: 137.6975\n",
      "Epoch [3/300], Step [7/42], Loss: 130.1734\n",
      "Epoch [3/300], Step [8/42], Loss: 126.4885\n",
      "Epoch [3/300], Step [9/42], Loss: 141.7716\n",
      "Epoch [3/300], Step [10/42], Loss: 111.1301\n",
      "Epoch [3/300], Step [11/42], Loss: 104.8702\n",
      "Epoch [3/300], Step [12/42], Loss: 112.8793\n",
      "Epoch [3/300], Step [13/42], Loss: 97.1669\n",
      "Epoch [3/300], Step [14/42], Loss: 126.6875\n",
      "Epoch [3/300], Step [15/42], Loss: 93.9772\n",
      "Epoch [3/300], Step [16/42], Loss: 92.3278\n",
      "Epoch [3/300], Step [17/42], Loss: 103.1840\n",
      "Epoch [3/300], Step [18/42], Loss: 99.9226\n",
      "Epoch [3/300], Step [19/42], Loss: 120.8701\n",
      "Epoch [3/300], Step [20/42], Loss: 99.1722\n",
      "Epoch [3/300], Step [21/42], Loss: 82.2273\n",
      "Epoch [3/300], Step [22/42], Loss: 119.9563\n",
      "Epoch [3/300], Step [23/42], Loss: 96.2146\n",
      "Epoch [3/300], Step [24/42], Loss: 104.8037\n",
      "Epoch [3/300], Step [25/42], Loss: 67.3420\n",
      "Epoch [3/300], Step [26/42], Loss: 71.2168\n",
      "Epoch [3/300], Step [27/42], Loss: 119.0308\n",
      "Epoch [3/300], Step [28/42], Loss: 85.9392\n",
      "Epoch [3/300], Step [29/42], Loss: 79.7293\n",
      "Epoch [3/300], Step [30/42], Loss: 102.0251\n",
      "Epoch [3/300], Step [31/42], Loss: 86.8656\n",
      "Epoch [3/300], Step [32/42], Loss: 94.0567\n",
      "Epoch [3/300], Step [33/42], Loss: 104.7811\n",
      "Epoch [3/300], Step [34/42], Loss: 110.3590\n",
      "Epoch [3/300], Step [35/42], Loss: 112.4362\n",
      "Epoch [3/300], Step [36/42], Loss: 99.2409\n",
      "Epoch [3/300], Step [37/42], Loss: 89.3790\n",
      "Epoch [3/300], Step [38/42], Loss: 71.7736\n",
      "Epoch [3/300], Step [39/42], Loss: 83.7464\n",
      "Epoch [3/300], Step [40/42], Loss: 92.2264\n",
      "Epoch [3/300], Step [41/42], Loss: 90.1982\n",
      "Epoch [3/300], Step [42/42], Loss: 27.3668\n",
      "Val. loss :93.4122\n",
      "Epoch [4/300], Step [1/42], Loss: 96.5331\n",
      "Epoch [4/300], Step [2/42], Loss: 127.8575\n",
      "Epoch [4/300], Step [3/42], Loss: 91.7420\n",
      "Epoch [4/300], Step [4/42], Loss: 76.9689\n",
      "Epoch [4/300], Step [5/42], Loss: 100.0554\n",
      "Epoch [4/300], Step [6/42], Loss: 92.3823\n",
      "Epoch [4/300], Step [7/42], Loss: 76.9994\n",
      "Epoch [4/300], Step [8/42], Loss: 76.9931\n",
      "Epoch [4/300], Step [9/42], Loss: 88.5593\n",
      "Epoch [4/300], Step [10/42], Loss: 106.6907\n",
      "Epoch [4/300], Step [11/42], Loss: 86.9712\n",
      "Epoch [4/300], Step [12/42], Loss: 93.5853\n",
      "Epoch [4/300], Step [13/42], Loss: 76.4805\n",
      "Epoch [4/300], Step [14/42], Loss: 62.5857\n",
      "Epoch [4/300], Step [15/42], Loss: 85.0268\n",
      "Epoch [4/300], Step [16/42], Loss: 65.9315\n",
      "Epoch [4/300], Step [17/42], Loss: 101.7060\n",
      "Epoch [4/300], Step [18/42], Loss: 66.0542\n",
      "Epoch [4/300], Step [19/42], Loss: 87.0205\n",
      "Epoch [4/300], Step [20/42], Loss: 71.7080\n",
      "Epoch [4/300], Step [21/42], Loss: 82.5849\n",
      "Epoch [4/300], Step [22/42], Loss: 70.5634\n",
      "Epoch [4/300], Step [23/42], Loss: 91.6116\n",
      "Epoch [4/300], Step [24/42], Loss: 86.0490\n",
      "Epoch [4/300], Step [25/42], Loss: 75.3302\n",
      "Epoch [4/300], Step [26/42], Loss: 77.2310\n",
      "Epoch [4/300], Step [27/42], Loss: 70.6993\n",
      "Epoch [4/300], Step [28/42], Loss: 67.3020\n",
      "Epoch [4/300], Step [29/42], Loss: 73.8169\n",
      "Epoch [4/300], Step [30/42], Loss: 91.5233\n",
      "Epoch [4/300], Step [31/42], Loss: 76.2923\n",
      "Epoch [4/300], Step [32/42], Loss: 71.2112\n",
      "Epoch [4/300], Step [33/42], Loss: 49.3385\n",
      "Epoch [4/300], Step [34/42], Loss: 76.6432\n",
      "Epoch [4/300], Step [35/42], Loss: 75.3287\n",
      "Epoch [4/300], Step [36/42], Loss: 75.8996\n",
      "Epoch [4/300], Step [37/42], Loss: 61.7344\n",
      "Epoch [4/300], Step [38/42], Loss: 76.4164\n",
      "Epoch [4/300], Step [39/42], Loss: 78.7386\n",
      "Epoch [4/300], Step [40/42], Loss: 74.0205\n",
      "Epoch [4/300], Step [41/42], Loss: 75.8597\n",
      "Epoch [4/300], Step [42/42], Loss: 15.7230\n",
      "Val. loss :73.1348\n",
      "Epoch [5/300], Step [1/42], Loss: 78.2722\n",
      "Epoch [5/300], Step [2/42], Loss: 85.2650\n",
      "Epoch [5/300], Step [3/42], Loss: 71.1438\n",
      "Epoch [5/300], Step [4/42], Loss: 62.2629\n",
      "Epoch [5/300], Step [5/42], Loss: 83.4587\n",
      "Epoch [5/300], Step [6/42], Loss: 58.5002\n",
      "Epoch [5/300], Step [7/42], Loss: 73.3193\n",
      "Epoch [5/300], Step [8/42], Loss: 78.1779\n",
      "Epoch [5/300], Step [9/42], Loss: 65.2097\n",
      "Epoch [5/300], Step [10/42], Loss: 68.7334\n",
      "Epoch [5/300], Step [11/42], Loss: 67.8692\n",
      "Epoch [5/300], Step [12/42], Loss: 73.0510\n",
      "Epoch [5/300], Step [13/42], Loss: 67.9696\n",
      "Epoch [5/300], Step [14/42], Loss: 81.8919\n",
      "Epoch [5/300], Step [15/42], Loss: 60.6109\n",
      "Epoch [5/300], Step [16/42], Loss: 84.4271\n",
      "Epoch [5/300], Step [17/42], Loss: 72.7478\n",
      "Epoch [5/300], Step [18/42], Loss: 71.4731\n",
      "Epoch [5/300], Step [19/42], Loss: 64.9445\n",
      "Epoch [5/300], Step [20/42], Loss: 65.6168\n",
      "Epoch [5/300], Step [21/42], Loss: 55.6602\n",
      "Epoch [5/300], Step [22/42], Loss: 60.6717\n",
      "Epoch [5/300], Step [23/42], Loss: 71.6774\n",
      "Epoch [5/300], Step [24/42], Loss: 68.3598\n",
      "Epoch [5/300], Step [25/42], Loss: 64.5231\n",
      "Epoch [5/300], Step [26/42], Loss: 59.9002\n",
      "Epoch [5/300], Step [27/42], Loss: 59.2455\n",
      "Epoch [5/300], Step [28/42], Loss: 59.9178\n",
      "Epoch [5/300], Step [29/42], Loss: 56.1290\n",
      "Epoch [5/300], Step [30/42], Loss: 69.2957\n",
      "Epoch [5/300], Step [31/42], Loss: 61.2216\n",
      "Epoch [5/300], Step [32/42], Loss: 67.1327\n",
      "Epoch [5/300], Step [33/42], Loss: 62.2489\n",
      "Epoch [5/300], Step [34/42], Loss: 60.5531\n",
      "Epoch [5/300], Step [35/42], Loss: 49.5533\n",
      "Epoch [5/300], Step [36/42], Loss: 62.0435\n",
      "Epoch [5/300], Step [37/42], Loss: 64.6935\n",
      "Epoch [5/300], Step [38/42], Loss: 64.7078\n",
      "Epoch [5/300], Step [39/42], Loss: 59.3711\n",
      "Epoch [5/300], Step [40/42], Loss: 55.1948\n",
      "Epoch [5/300], Step [41/42], Loss: 66.3626\n",
      "Epoch [5/300], Step [42/42], Loss: 9.2007\n",
      "Val. loss :63.0734\n",
      "Epoch [6/300], Step [1/42], Loss: 68.0749\n",
      "Epoch [6/300], Step [2/42], Loss: 57.4724\n",
      "Epoch [6/300], Step [3/42], Loss: 61.1137\n",
      "Epoch [6/300], Step [4/42], Loss: 58.1218\n",
      "Epoch [6/300], Step [5/42], Loss: 48.5410\n",
      "Epoch [6/300], Step [6/42], Loss: 72.5995\n",
      "Epoch [6/300], Step [7/42], Loss: 48.5103\n",
      "Epoch [6/300], Step [8/42], Loss: 58.5368\n",
      "Epoch [6/300], Step [9/42], Loss: 62.2986\n",
      "Epoch [6/300], Step [10/42], Loss: 59.7435\n",
      "Epoch [6/300], Step [11/42], Loss: 57.7446\n",
      "Epoch [6/300], Step [12/42], Loss: 59.9050\n",
      "Epoch [6/300], Step [13/42], Loss: 62.8873\n",
      "Epoch [6/300], Step [14/42], Loss: 60.8398\n",
      "Epoch [6/300], Step [15/42], Loss: 72.8204\n",
      "Epoch [6/300], Step [16/42], Loss: 62.4570\n",
      "Epoch [6/300], Step [17/42], Loss: 54.7221\n",
      "Epoch [6/300], Step [18/42], Loss: 51.0811\n",
      "Epoch [6/300], Step [19/42], Loss: 62.4141\n",
      "Epoch [6/300], Step [20/42], Loss: 56.6540\n",
      "Epoch [6/300], Step [21/42], Loss: 54.6744\n",
      "Epoch [6/300], Step [22/42], Loss: 60.4473\n",
      "Epoch [6/300], Step [23/42], Loss: 44.7951\n",
      "Epoch [6/300], Step [24/42], Loss: 51.8025\n",
      "Epoch [6/300], Step [25/42], Loss: 66.4680\n",
      "Epoch [6/300], Step [26/42], Loss: 65.3361\n",
      "Epoch [6/300], Step [27/42], Loss: 48.6657\n",
      "Epoch [6/300], Step [28/42], Loss: 61.2138\n",
      "Epoch [6/300], Step [29/42], Loss: 51.5863\n",
      "Epoch [6/300], Step [30/42], Loss: 54.3115\n",
      "Epoch [6/300], Step [31/42], Loss: 52.3415\n",
      "Epoch [6/300], Step [32/42], Loss: 53.9008\n",
      "Epoch [6/300], Step [33/42], Loss: 52.1920\n",
      "Epoch [6/300], Step [34/42], Loss: 66.4998\n",
      "Epoch [6/300], Step [35/42], Loss: 58.8496\n",
      "Epoch [6/300], Step [36/42], Loss: 48.2009\n",
      "Epoch [6/300], Step [37/42], Loss: 55.7995\n",
      "Epoch [6/300], Step [38/42], Loss: 55.7967\n",
      "Epoch [6/300], Step [39/42], Loss: 52.3914\n",
      "Epoch [6/300], Step [40/42], Loss: 56.4178\n",
      "Epoch [6/300], Step [41/42], Loss: 46.4473\n",
      "Epoch [6/300], Step [42/42], Loss: 11.9745\n",
      "Val. loss :55.5504\n",
      "Epoch [7/300], Step [1/42], Loss: 50.2458\n",
      "Epoch [7/300], Step [2/42], Loss: 57.2425\n",
      "Epoch [7/300], Step [3/42], Loss: 49.2951\n",
      "Epoch [7/300], Step [4/42], Loss: 44.3678\n",
      "Epoch [7/300], Step [5/42], Loss: 53.6323\n",
      "Epoch [7/300], Step [6/42], Loss: 42.6629\n",
      "Epoch [7/300], Step [7/42], Loss: 57.9557\n",
      "Epoch [7/300], Step [8/42], Loss: 54.6526\n",
      "Epoch [7/300], Step [9/42], Loss: 57.5006\n",
      "Epoch [7/300], Step [10/42], Loss: 53.1866\n",
      "Epoch [7/300], Step [11/42], Loss: 56.1509\n",
      "Epoch [7/300], Step [12/42], Loss: 47.4189\n",
      "Epoch [7/300], Step [13/42], Loss: 51.7841\n",
      "Epoch [7/300], Step [14/42], Loss: 55.5253\n",
      "Epoch [7/300], Step [15/42], Loss: 58.8822\n",
      "Epoch [7/300], Step [16/42], Loss: 55.6634\n",
      "Epoch [7/300], Step [17/42], Loss: 48.7499\n",
      "Epoch [7/300], Step [18/42], Loss: 55.7872\n",
      "Epoch [7/300], Step [19/42], Loss: 47.1019\n",
      "Epoch [7/300], Step [20/42], Loss: 56.3620\n",
      "Epoch [7/300], Step [21/42], Loss: 41.2628\n",
      "Epoch [7/300], Step [22/42], Loss: 43.4794\n",
      "Epoch [7/300], Step [23/42], Loss: 50.5884\n",
      "Epoch [7/300], Step [24/42], Loss: 48.3992\n",
      "Epoch [7/300], Step [25/42], Loss: 63.6408\n",
      "Epoch [7/300], Step [26/42], Loss: 55.3050\n",
      "Epoch [7/300], Step [27/42], Loss: 67.3189\n",
      "Epoch [7/300], Step [28/42], Loss: 43.5291\n",
      "Epoch [7/300], Step [29/42], Loss: 53.7504\n",
      "Epoch [7/300], Step [30/42], Loss: 48.4618\n",
      "Epoch [7/300], Step [31/42], Loss: 45.7485\n",
      "Epoch [7/300], Step [32/42], Loss: 45.3209\n",
      "Epoch [7/300], Step [33/42], Loss: 52.4146\n",
      "Epoch [7/300], Step [34/42], Loss: 49.4676\n",
      "Epoch [7/300], Step [35/42], Loss: 63.6192\n",
      "Epoch [7/300], Step [36/42], Loss: 49.5538\n",
      "Epoch [7/300], Step [37/42], Loss: 42.4429\n",
      "Epoch [7/300], Step [38/42], Loss: 49.1308\n",
      "Epoch [7/300], Step [39/42], Loss: 46.5207\n",
      "Epoch [7/300], Step [40/42], Loss: 46.9814\n",
      "Epoch [7/300], Step [41/42], Loss: 45.8134\n",
      "Epoch [7/300], Step [42/42], Loss: 11.9377\n",
      "Val. loss :50.1470\n",
      "Epoch [8/300], Step [1/42], Loss: 49.3076\n",
      "Epoch [8/300], Step [2/42], Loss: 56.5273\n",
      "Epoch [8/300], Step [3/42], Loss: 54.1847\n",
      "Epoch [8/300], Step [4/42], Loss: 58.6288\n",
      "Epoch [8/300], Step [5/42], Loss: 47.1402\n",
      "Epoch [8/300], Step [6/42], Loss: 50.4969\n",
      "Epoch [8/300], Step [7/42], Loss: 38.9483\n",
      "Epoch [8/300], Step [8/42], Loss: 54.3331\n",
      "Epoch [8/300], Step [9/42], Loss: 43.0619\n",
      "Epoch [8/300], Step [10/42], Loss: 50.0398\n",
      "Epoch [8/300], Step [11/42], Loss: 49.3081\n",
      "Epoch [8/300], Step [12/42], Loss: 55.6874\n",
      "Epoch [8/300], Step [13/42], Loss: 52.6155\n",
      "Epoch [8/300], Step [14/42], Loss: 46.8559\n",
      "Epoch [8/300], Step [15/42], Loss: 48.9590\n",
      "Epoch [8/300], Step [16/42], Loss: 47.7278\n",
      "Epoch [8/300], Step [17/42], Loss: 46.0129\n",
      "Epoch [8/300], Step [18/42], Loss: 42.1718\n",
      "Epoch [8/300], Step [19/42], Loss: 45.0065\n",
      "Epoch [8/300], Step [20/42], Loss: 40.6098\n",
      "Epoch [8/300], Step [21/42], Loss: 39.9433\n",
      "Epoch [8/300], Step [22/42], Loss: 38.2273\n",
      "Epoch [8/300], Step [23/42], Loss: 42.9879\n",
      "Epoch [8/300], Step [24/42], Loss: 50.0029\n",
      "Epoch [8/300], Step [25/42], Loss: 46.4748\n",
      "Epoch [8/300], Step [26/42], Loss: 44.7374\n",
      "Epoch [8/300], Step [27/42], Loss: 47.1771\n",
      "Epoch [8/300], Step [28/42], Loss: 43.3225\n",
      "Epoch [8/300], Step [29/42], Loss: 58.0157\n",
      "Epoch [8/300], Step [30/42], Loss: 43.0361\n",
      "Epoch [8/300], Step [31/42], Loss: 44.2350\n",
      "Epoch [8/300], Step [32/42], Loss: 43.5117\n",
      "Epoch [8/300], Step [33/42], Loss: 50.8187\n",
      "Epoch [8/300], Step [34/42], Loss: 41.5035\n",
      "Epoch [8/300], Step [35/42], Loss: 46.8851\n",
      "Epoch [8/300], Step [36/42], Loss: 41.4074\n",
      "Epoch [8/300], Step [37/42], Loss: 40.1744\n",
      "Epoch [8/300], Step [38/42], Loss: 54.4177\n",
      "Epoch [8/300], Step [39/42], Loss: 33.4098\n",
      "Epoch [8/300], Step [40/42], Loss: 57.5805\n",
      "Epoch [8/300], Step [41/42], Loss: 42.0709\n",
      "Epoch [8/300], Step [42/42], Loss: 10.1559\n",
      "Val. loss :45.6493\n",
      "Epoch [9/300], Step [1/42], Loss: 38.0724\n",
      "Epoch [9/300], Step [2/42], Loss: 44.4882\n",
      "Epoch [9/300], Step [3/42], Loss: 44.6498\n",
      "Epoch [9/300], Step [4/42], Loss: 43.0109\n",
      "Epoch [9/300], Step [5/42], Loss: 52.3090\n",
      "Epoch [9/300], Step [6/42], Loss: 47.0414\n",
      "Epoch [9/300], Step [7/42], Loss: 38.8898\n",
      "Epoch [9/300], Step [8/42], Loss: 38.7211\n",
      "Epoch [9/300], Step [9/42], Loss: 45.2087\n",
      "Epoch [9/300], Step [10/42], Loss: 41.2651\n",
      "Epoch [9/300], Step [11/42], Loss: 34.7700\n",
      "Epoch [9/300], Step [12/42], Loss: 39.5191\n",
      "Epoch [9/300], Step [13/42], Loss: 35.9584\n",
      "Epoch [9/300], Step [14/42], Loss: 44.2745\n",
      "Epoch [9/300], Step [15/42], Loss: 40.3337\n",
      "Epoch [9/300], Step [16/42], Loss: 56.8257\n",
      "Epoch [9/300], Step [17/42], Loss: 51.6113\n",
      "Epoch [9/300], Step [18/42], Loss: 57.0810\n",
      "Epoch [9/300], Step [19/42], Loss: 37.2121\n",
      "Epoch [9/300], Step [20/42], Loss: 45.5440\n",
      "Epoch [9/300], Step [21/42], Loss: 38.9507\n",
      "Epoch [9/300], Step [22/42], Loss: 39.5775\n",
      "Epoch [9/300], Step [23/42], Loss: 41.1746\n",
      "Epoch [9/300], Step [24/42], Loss: 46.5428\n",
      "Epoch [9/300], Step [25/42], Loss: 56.0980\n",
      "Epoch [9/300], Step [26/42], Loss: 44.0406\n",
      "Epoch [9/300], Step [27/42], Loss: 44.1310\n",
      "Epoch [9/300], Step [28/42], Loss: 56.0995\n",
      "Epoch [9/300], Step [29/42], Loss: 44.9465\n",
      "Epoch [9/300], Step [30/42], Loss: 39.3145\n",
      "Epoch [9/300], Step [31/42], Loss: 42.2896\n",
      "Epoch [9/300], Step [32/42], Loss: 40.8084\n",
      "Epoch [9/300], Step [33/42], Loss: 47.4362\n",
      "Epoch [9/300], Step [34/42], Loss: 46.4038\n",
      "Epoch [9/300], Step [35/42], Loss: 52.9822\n",
      "Epoch [9/300], Step [36/42], Loss: 42.7326\n",
      "Epoch [9/300], Step [37/42], Loss: 39.0065\n",
      "Epoch [9/300], Step [38/42], Loss: 40.2057\n",
      "Epoch [9/300], Step [39/42], Loss: 34.1641\n",
      "Epoch [9/300], Step [40/42], Loss: 43.3382\n",
      "Epoch [9/300], Step [41/42], Loss: 42.2876\n",
      "Epoch [9/300], Step [42/42], Loss: 12.2350\n",
      "Val. loss :43.6364\n",
      "Epoch [10/300], Step [1/42], Loss: 45.2108\n",
      "Epoch [10/300], Step [2/42], Loss: 37.0356\n",
      "Epoch [10/300], Step [3/42], Loss: 45.6267\n",
      "Epoch [10/300], Step [4/42], Loss: 48.8307\n",
      "Epoch [10/300], Step [5/42], Loss: 41.2185\n",
      "Epoch [10/300], Step [6/42], Loss: 49.9861\n",
      "Epoch [10/300], Step [7/42], Loss: 41.3280\n",
      "Epoch [10/300], Step [8/42], Loss: 37.2307\n",
      "Epoch [10/300], Step [9/42], Loss: 44.0944\n",
      "Epoch [10/300], Step [10/42], Loss: 36.6033\n",
      "Epoch [10/300], Step [11/42], Loss: 47.9789\n",
      "Epoch [10/300], Step [12/42], Loss: 35.2444\n",
      "Epoch [10/300], Step [13/42], Loss: 35.2182\n",
      "Epoch [10/300], Step [14/42], Loss: 39.9880\n",
      "Epoch [10/300], Step [15/42], Loss: 33.9344\n",
      "Epoch [10/300], Step [16/42], Loss: 38.4858\n",
      "Epoch [10/300], Step [17/42], Loss: 37.3798\n",
      "Epoch [10/300], Step [18/42], Loss: 29.6118\n",
      "Epoch [10/300], Step [19/42], Loss: 47.5070\n",
      "Epoch [10/300], Step [20/42], Loss: 38.7366\n",
      "Epoch [10/300], Step [21/42], Loss: 31.5199\n",
      "Epoch [10/300], Step [22/42], Loss: 35.2417\n",
      "Epoch [10/300], Step [23/42], Loss: 28.2135\n",
      "Epoch [10/300], Step [24/42], Loss: 27.8716\n",
      "Epoch [10/300], Step [25/42], Loss: 28.5501\n",
      "Epoch [10/300], Step [26/42], Loss: 34.8201\n",
      "Epoch [10/300], Step [27/42], Loss: 34.1273\n",
      "Epoch [10/300], Step [28/42], Loss: 33.6969\n",
      "Epoch [10/300], Step [29/42], Loss: 32.3103\n",
      "Epoch [10/300], Step [30/42], Loss: 35.6632\n",
      "Epoch [10/300], Step [31/42], Loss: 37.3436\n",
      "Epoch [10/300], Step [32/42], Loss: 31.1747\n",
      "Epoch [10/300], Step [33/42], Loss: 31.3797\n",
      "Epoch [10/300], Step [34/42], Loss: 37.4643\n",
      "Epoch [10/300], Step [35/42], Loss: 29.1715\n",
      "Epoch [10/300], Step [36/42], Loss: 33.1060\n",
      "Epoch [10/300], Step [37/42], Loss: 36.2307\n",
      "Epoch [10/300], Step [38/42], Loss: 31.1312\n",
      "Epoch [10/300], Step [39/42], Loss: 33.2055\n",
      "Epoch [10/300], Step [40/42], Loss: 36.1666\n",
      "Epoch [10/300], Step [41/42], Loss: 28.5795\n",
      "Epoch [10/300], Step [42/42], Loss: 9.4197\n",
      "Val. loss :33.4874\n",
      "Epoch [11/300], Step [1/42], Loss: 25.3858\n",
      "Epoch [11/300], Step [2/42], Loss: 31.7730\n",
      "Epoch [11/300], Step [3/42], Loss: 26.3165\n",
      "Epoch [11/300], Step [4/42], Loss: 27.6120\n",
      "Epoch [11/300], Step [5/42], Loss: 29.1202\n",
      "Epoch [11/300], Step [6/42], Loss: 28.1608\n",
      "Epoch [11/300], Step [7/42], Loss: 27.1494\n",
      "Epoch [11/300], Step [8/42], Loss: 30.0667\n",
      "Epoch [11/300], Step [9/42], Loss: 35.3561\n",
      "Epoch [11/300], Step [10/42], Loss: 25.8863\n",
      "Epoch [11/300], Step [11/42], Loss: 42.5966\n",
      "Epoch [11/300], Step [12/42], Loss: 25.5681\n",
      "Epoch [11/300], Step [13/42], Loss: 27.1657\n",
      "Epoch [11/300], Step [14/42], Loss: 39.3736\n",
      "Epoch [11/300], Step [15/42], Loss: 31.8288\n",
      "Epoch [11/300], Step [16/42], Loss: 33.1838\n",
      "Epoch [11/300], Step [17/42], Loss: 31.1001\n",
      "Epoch [11/300], Step [18/42], Loss: 32.4965\n",
      "Epoch [11/300], Step [19/42], Loss: 31.1081\n",
      "Epoch [11/300], Step [20/42], Loss: 32.0624\n",
      "Epoch [11/300], Step [21/42], Loss: 39.4546\n",
      "Epoch [11/300], Step [22/42], Loss: 46.5904\n",
      "Epoch [11/300], Step [23/42], Loss: 39.6873\n",
      "Epoch [11/300], Step [24/42], Loss: 36.0812\n",
      "Epoch [11/300], Step [25/42], Loss: 36.9898\n",
      "Epoch [11/300], Step [26/42], Loss: 30.9986\n",
      "Epoch [11/300], Step [27/42], Loss: 28.0815\n",
      "Epoch [11/300], Step [28/42], Loss: 29.9675\n",
      "Epoch [11/300], Step [29/42], Loss: 31.6253\n",
      "Epoch [11/300], Step [30/42], Loss: 36.0350\n",
      "Epoch [11/300], Step [31/42], Loss: 41.0345\n",
      "Epoch [11/300], Step [32/42], Loss: 47.1382\n",
      "Epoch [11/300], Step [33/42], Loss: 30.8857\n",
      "Epoch [11/300], Step [34/42], Loss: 45.8956\n",
      "Epoch [11/300], Step [35/42], Loss: 43.6685\n",
      "Epoch [11/300], Step [36/42], Loss: 43.1306\n",
      "Epoch [11/300], Step [37/42], Loss: 36.7201\n",
      "Epoch [11/300], Step [38/42], Loss: 35.5238\n",
      "Epoch [11/300], Step [39/42], Loss: 38.5499\n",
      "Epoch [11/300], Step [40/42], Loss: 29.0330\n",
      "Epoch [11/300], Step [41/42], Loss: 34.4879\n",
      "Epoch [11/300], Step [42/42], Loss: 4.8245\n",
      "Val. loss :33.7804\n",
      "Epoch [12/300], Step [1/42], Loss: 33.5999\n",
      "Epoch [12/300], Step [2/42], Loss: 29.4757\n",
      "Epoch [12/300], Step [3/42], Loss: 47.2725\n",
      "Epoch [12/300], Step [4/42], Loss: 34.9739\n",
      "Epoch [12/300], Step [5/42], Loss: 22.3986\n",
      "Epoch [12/300], Step [6/42], Loss: 25.3222\n",
      "Epoch [12/300], Step [7/42], Loss: 26.2173\n",
      "Epoch [12/300], Step [8/42], Loss: 24.4859\n",
      "Epoch [12/300], Step [9/42], Loss: 22.4296\n",
      "Epoch [12/300], Step [10/42], Loss: 45.2782\n",
      "Epoch [12/300], Step [11/42], Loss: 32.7570\n",
      "Epoch [12/300], Step [12/42], Loss: 42.5278\n",
      "Epoch [12/300], Step [13/42], Loss: 33.3495\n",
      "Epoch [12/300], Step [14/42], Loss: 45.2331\n",
      "Epoch [12/300], Step [15/42], Loss: 29.4227\n",
      "Epoch [12/300], Step [16/42], Loss: 39.1631\n",
      "Epoch [12/300], Step [17/42], Loss: 52.0938\n",
      "Epoch [12/300], Step [18/42], Loss: 36.8079\n",
      "Epoch [12/300], Step [19/42], Loss: 24.9565\n",
      "Epoch [12/300], Step [20/42], Loss: 29.2795\n",
      "Epoch [12/300], Step [21/42], Loss: 36.5038\n",
      "Epoch [12/300], Step [22/42], Loss: 33.8177\n",
      "Epoch [12/300], Step [23/42], Loss: 43.4642\n",
      "Epoch [12/300], Step [24/42], Loss: 37.7283\n",
      "Epoch [12/300], Step [25/42], Loss: 42.1852\n",
      "Epoch [12/300], Step [26/42], Loss: 35.1928\n",
      "Epoch [12/300], Step [27/42], Loss: 49.7184\n",
      "Epoch [12/300], Step [28/42], Loss: 41.3316\n",
      "Epoch [12/300], Step [29/42], Loss: 31.5040\n",
      "Epoch [12/300], Step [30/42], Loss: 42.8967\n",
      "Epoch [12/300], Step [31/42], Loss: 45.3944\n",
      "Epoch [12/300], Step [32/42], Loss: 30.3423\n",
      "Epoch [12/300], Step [33/42], Loss: 37.7771\n",
      "Epoch [12/300], Step [34/42], Loss: 48.1389\n",
      "Epoch [12/300], Step [35/42], Loss: 47.3911\n",
      "Epoch [12/300], Step [36/42], Loss: 40.9850\n",
      "Epoch [12/300], Step [37/42], Loss: 38.8673\n",
      "Epoch [12/300], Step [38/42], Loss: 31.6115\n",
      "Epoch [12/300], Step [39/42], Loss: 35.4212\n",
      "Epoch [12/300], Step [40/42], Loss: 28.8529\n",
      "Epoch [12/300], Step [41/42], Loss: 30.1029\n",
      "Epoch [12/300], Step [42/42], Loss: 11.1753\n",
      "Val. loss :40.0937\n",
      "Epoch [13/300], Step [1/42], Loss: 49.8985\n",
      "Epoch [13/300], Step [2/42], Loss: 29.6211\n",
      "Epoch [13/300], Step [3/42], Loss: 28.3826\n",
      "Epoch [13/300], Step [4/42], Loss: 39.6348\n",
      "Epoch [13/300], Step [5/42], Loss: 43.7007\n",
      "Epoch [13/300], Step [6/42], Loss: 42.6536\n",
      "Epoch [13/300], Step [7/42], Loss: 41.7885\n",
      "Epoch [13/300], Step [8/42], Loss: 51.4415\n",
      "Epoch [13/300], Step [9/42], Loss: 50.7103\n",
      "Epoch [13/300], Step [10/42], Loss: 30.4130\n",
      "Epoch [13/300], Step [11/42], Loss: 41.1810\n",
      "Epoch [13/300], Step [12/42], Loss: 41.5241\n",
      "Epoch [13/300], Step [13/42], Loss: 37.0119\n",
      "Epoch [13/300], Step [14/42], Loss: 35.4462\n",
      "Epoch [13/300], Step [15/42], Loss: 36.5810\n",
      "Epoch [13/300], Step [16/42], Loss: 31.0801\n",
      "Epoch [13/300], Step [17/42], Loss: 44.3412\n",
      "Epoch [13/300], Step [18/42], Loss: 28.7840\n",
      "Epoch [13/300], Step [19/42], Loss: 42.4235\n",
      "Epoch [13/300], Step [20/42], Loss: 39.6730\n",
      "Epoch [13/300], Step [21/42], Loss: 39.8339\n",
      "Epoch [13/300], Step [22/42], Loss: 29.8776\n",
      "Epoch [13/300], Step [23/42], Loss: 36.5238\n",
      "Epoch [13/300], Step [24/42], Loss: 26.6651\n",
      "Epoch [13/300], Step [25/42], Loss: 33.8315\n",
      "Epoch [13/300], Step [26/42], Loss: 27.2488\n",
      "Epoch [13/300], Step [27/42], Loss: 38.0139\n",
      "Epoch [13/300], Step [28/42], Loss: 36.1433\n",
      "Epoch [13/300], Step [29/42], Loss: 38.4247\n",
      "Epoch [13/300], Step [30/42], Loss: 45.7067\n",
      "Epoch [13/300], Step [31/42], Loss: 56.6628\n",
      "Epoch [13/300], Step [32/42], Loss: 34.2486\n",
      "Epoch [13/300], Step [33/42], Loss: 26.2215\n",
      "Epoch [13/300], Step [34/42], Loss: 45.1824\n",
      "Epoch [13/300], Step [35/42], Loss: 35.9620\n",
      "Epoch [13/300], Step [36/42], Loss: 33.2498\n",
      "Epoch [13/300], Step [37/42], Loss: 38.5580\n",
      "Epoch [13/300], Step [38/42], Loss: 39.0260\n",
      "Epoch [13/300], Step [39/42], Loss: 39.5077\n",
      "Epoch [13/300], Step [40/42], Loss: 29.8771\n",
      "Epoch [13/300], Step [41/42], Loss: 43.7822\n",
      "Epoch [13/300], Step [42/42], Loss: 11.1638\n",
      "Val. loss :37.5494\n",
      "Epoch [14/300], Step [1/42], Loss: 52.7220\n",
      "Epoch [14/300], Step [2/42], Loss: 25.5040\n",
      "Epoch [14/300], Step [3/42], Loss: 32.3130\n",
      "Epoch [14/300], Step [4/42], Loss: 36.1659\n",
      "Epoch [14/300], Step [5/42], Loss: 40.6128\n",
      "Epoch [14/300], Step [6/42], Loss: 33.7325\n",
      "Epoch [14/300], Step [7/42], Loss: 35.2385\n",
      "Epoch [14/300], Step [8/42], Loss: 33.9612\n",
      "Epoch [14/300], Step [9/42], Loss: 32.8773\n",
      "Epoch [14/300], Step [10/42], Loss: 30.2594\n",
      "Epoch [14/300], Step [11/42], Loss: 30.1268\n",
      "Epoch [14/300], Step [12/42], Loss: 30.1553\n",
      "Epoch [14/300], Step [13/42], Loss: 32.7543\n",
      "Epoch [14/300], Step [14/42], Loss: 24.8973\n",
      "Epoch [14/300], Step [15/42], Loss: 25.4924\n",
      "Epoch [14/300], Step [16/42], Loss: 26.2972\n",
      "Epoch [14/300], Step [17/42], Loss: 39.5570\n",
      "Epoch [14/300], Step [18/42], Loss: 57.0731\n",
      "Epoch [14/300], Step [19/42], Loss: 31.2691\n",
      "Epoch [14/300], Step [20/42], Loss: 41.7875\n",
      "Epoch [14/300], Step [21/42], Loss: 35.8641\n",
      "Epoch [14/300], Step [22/42], Loss: 52896.1250\n",
      "Epoch [14/300], Step [23/42], Loss: 33.5945\n",
      "Epoch [14/300], Step [24/42], Loss: 26.4933\n",
      "Epoch [14/300], Step [25/42], Loss: 34.3485\n",
      "Epoch [14/300], Step [26/42], Loss: 36.4820\n",
      "Epoch [14/300], Step [27/42], Loss: 45.6408\n",
      "Epoch [14/300], Step [28/42], Loss: 40.9857\n",
      "Epoch [14/300], Step [29/42], Loss: 40.4584\n",
      "Epoch [14/300], Step [30/42], Loss: 43.3943\n",
      "Epoch [14/300], Step [31/42], Loss: 42.5217\n",
      "Epoch [14/300], Step [32/42], Loss: 31.2764\n",
      "Epoch [14/300], Step [33/42], Loss: 36.3051\n",
      "Epoch [14/300], Step [34/42], Loss: 42.6561\n",
      "Epoch [14/300], Step [35/42], Loss: 42.8917\n",
      "Epoch [14/300], Step [36/42], Loss: 40.4597\n",
      "Epoch [14/300], Step [37/42], Loss: 45.5115\n",
      "Epoch [14/300], Step [38/42], Loss: 32.5763\n",
      "Epoch [14/300], Step [39/42], Loss: 35.6565\n",
      "Epoch [14/300], Step [40/42], Loss: 40.5375\n",
      "Epoch [14/300], Step [41/42], Loss: 25.3896\n",
      "Epoch [14/300], Step [42/42], Loss: 7.9299\n",
      "Val. loss :32.9511\n",
      "Epoch [15/300], Step [1/42], Loss: 35.7985\n",
      "Epoch [15/300], Step [2/42], Loss: 29.5691\n",
      "Epoch [15/300], Step [3/42], Loss: 33.1228\n",
      "Epoch [15/300], Step [4/42], Loss: 35.4902\n",
      "Epoch [15/300], Step [5/42], Loss: 58.6722\n",
      "Epoch [15/300], Step [6/42], Loss: 28.8872\n",
      "Epoch [15/300], Step [7/42], Loss: 38.9164\n",
      "Epoch [15/300], Step [8/42], Loss: 33.1075\n",
      "Epoch [15/300], Step [9/42], Loss: 37.9094\n",
      "Epoch [15/300], Step [10/42], Loss: 43.3518\n",
      "Epoch [15/300], Step [11/42], Loss: 39.5422\n",
      "Epoch [15/300], Step [12/42], Loss: 38.3629\n",
      "Epoch [15/300], Step [13/42], Loss: 36.4171\n",
      "Epoch [15/300], Step [14/42], Loss: 38.3525\n",
      "Epoch [15/300], Step [15/42], Loss: 36.8621\n",
      "Epoch [15/300], Step [16/42], Loss: 39.5027\n",
      "Epoch [15/300], Step [17/42], Loss: 46.2009\n",
      "Epoch [15/300], Step [18/42], Loss: 32.6106\n",
      "Epoch [15/300], Step [19/42], Loss: 43.7925\n",
      "Epoch [15/300], Step [20/42], Loss: 36.5238\n",
      "Epoch [15/300], Step [21/42], Loss: 33.5082\n",
      "Epoch [15/300], Step [22/42], Loss: 33.0220\n",
      "Epoch [15/300], Step [23/42], Loss: 36.9175\n",
      "Epoch [15/300], Step [24/42], Loss: 34.6704\n",
      "Epoch [15/300], Step [25/42], Loss: 37.7013\n",
      "Epoch [15/300], Step [26/42], Loss: 29.9258\n",
      "Epoch [15/300], Step [27/42], Loss: 40.8994\n",
      "Epoch [15/300], Step [28/42], Loss: 49.4864\n",
      "Epoch [15/300], Step [29/42], Loss: 30.3968\n",
      "Epoch [15/300], Step [30/42], Loss: 33.6954\n",
      "Epoch [15/300], Step [31/42], Loss: 25.9233\n",
      "Epoch [15/300], Step [32/42], Loss: 28.3366\n",
      "Epoch [15/300], Step [33/42], Loss: 37.9318\n",
      "Epoch [15/300], Step [34/42], Loss: 33.6809\n",
      "Epoch [15/300], Step [35/42], Loss: 35.2316\n",
      "Epoch [15/300], Step [36/42], Loss: 30.2398\n",
      "Epoch [15/300], Step [37/42], Loss: 30.8611\n",
      "Epoch [15/300], Step [38/42], Loss: 38.1081\n",
      "Epoch [15/300], Step [39/42], Loss: 30.9292\n",
      "Epoch [15/300], Step [40/42], Loss: 36.3546\n",
      "Epoch [15/300], Step [41/42], Loss: 36.9875\n",
      "Epoch [15/300], Step [42/42], Loss: 6.3483\n",
      "Val. loss :36.0474\n",
      "Epoch [16/300], Step [1/42], Loss: 36.1676\n",
      "Epoch [16/300], Step [2/42], Loss: 37.0961\n",
      "Epoch [16/300], Step [3/42], Loss: 33.3323\n",
      "Epoch [16/300], Step [4/42], Loss: 33.9764\n",
      "Epoch [16/300], Step [5/42], Loss: 32.6990\n",
      "Epoch [16/300], Step [6/42], Loss: 32.1361\n",
      "Epoch [16/300], Step [7/42], Loss: 38.9224\n",
      "Epoch [16/300], Step [8/42], Loss: 32.7613\n",
      "Epoch [16/300], Step [9/42], Loss: 31.1265\n",
      "Epoch [16/300], Step [10/42], Loss: 40.6823\n",
      "Epoch [16/300], Step [11/42], Loss: 35.3078\n",
      "Epoch [16/300], Step [12/42], Loss: 30.5980\n",
      "Epoch [16/300], Step [13/42], Loss: 29.9738\n",
      "Epoch [16/300], Step [14/42], Loss: 37.5573\n",
      "Epoch [16/300], Step [15/42], Loss: 34.6348\n",
      "Epoch [16/300], Step [16/42], Loss: 29.8175\n",
      "Epoch [16/300], Step [17/42], Loss: 38.5874\n",
      "Epoch [16/300], Step [18/42], Loss: 29.6420\n",
      "Epoch [16/300], Step [19/42], Loss: 34.2776\n",
      "Epoch [16/300], Step [20/42], Loss: 24.4877\n",
      "Epoch [16/300], Step [21/42], Loss: 27.5691\n",
      "Epoch [16/300], Step [22/42], Loss: 28.6922\n",
      "Epoch [16/300], Step [23/42], Loss: 21.6717\n",
      "Epoch [16/300], Step [24/42], Loss: 24.4186\n",
      "Epoch [16/300], Step [25/42], Loss: 30.4022\n",
      "Epoch [16/300], Step [26/42], Loss: 30.9942\n",
      "Epoch [16/300], Step [27/42], Loss: 22.4956\n",
      "Epoch [16/300], Step [28/42], Loss: 28.8179\n",
      "Epoch [16/300], Step [29/42], Loss: 28.8954\n",
      "Epoch [16/300], Step [30/42], Loss: 23.5897\n",
      "Epoch [16/300], Step [31/42], Loss: 20.7292\n",
      "Epoch [16/300], Step [32/42], Loss: 22.5581\n",
      "Epoch [16/300], Step [33/42], Loss: 25.4598\n",
      "Epoch [16/300], Step [34/42], Loss: 25.1359\n",
      "Epoch [16/300], Step [35/42], Loss: 24.4259\n",
      "Epoch [16/300], Step [36/42], Loss: 25.7410\n",
      "Epoch [16/300], Step [37/42], Loss: 23.1737\n",
      "Epoch [16/300], Step [38/42], Loss: 31.4043\n",
      "Epoch [16/300], Step [39/42], Loss: 33.9962\n",
      "Epoch [16/300], Step [40/42], Loss: 28.4800\n",
      "Epoch [16/300], Step [41/42], Loss: 44.4078\n",
      "Epoch [16/300], Step [42/42], Loss: 6.6053\n",
      "Val. loss :35.4025\n",
      "Epoch [17/300], Step [1/42], Loss: 42.1230\n",
      "Epoch [17/300], Step [2/42], Loss: 32.0284\n",
      "Epoch [17/300], Step [3/42], Loss: 40.4814\n",
      "Epoch [17/300], Step [4/42], Loss: 25.9527\n",
      "Epoch [17/300], Step [5/42], Loss: 27.2393\n",
      "Epoch [17/300], Step [6/42], Loss: 42.2541\n",
      "Epoch [17/300], Step [7/42], Loss: 34.1325\n",
      "Epoch [17/300], Step [8/42], Loss: 40.8381\n",
      "Epoch [17/300], Step [9/42], Loss: 35.2648\n",
      "Epoch [17/300], Step [10/42], Loss: 24.8092\n",
      "Epoch [17/300], Step [11/42], Loss: 28.5697\n",
      "Epoch [17/300], Step [12/42], Loss: 32.0885\n",
      "Epoch [17/300], Step [13/42], Loss: 30.0008\n",
      "Epoch [17/300], Step [14/42], Loss: 31.3422\n",
      "Epoch [17/300], Step [15/42], Loss: 28.5069\n",
      "Epoch [17/300], Step [16/42], Loss: 42.0597\n",
      "Epoch [17/300], Step [17/42], Loss: 25.0664\n",
      "Epoch [17/300], Step [18/42], Loss: 32.6124\n",
      "Epoch [17/300], Step [19/42], Loss: 34.2372\n",
      "Epoch [17/300], Step [20/42], Loss: 34.8099\n",
      "Epoch [17/300], Step [21/42], Loss: 24.2478\n",
      "Epoch [17/300], Step [22/42], Loss: 28.7440\n",
      "Epoch [17/300], Step [23/42], Loss: 31.5775\n",
      "Epoch [17/300], Step [24/42], Loss: 32.3115\n",
      "Epoch [17/300], Step [25/42], Loss: 27.1639\n",
      "Epoch [17/300], Step [26/42], Loss: 27.0993\n",
      "Epoch [17/300], Step [27/42], Loss: 28.4532\n",
      "Epoch [17/300], Step [28/42], Loss: 31.8800\n",
      "Epoch [17/300], Step [29/42], Loss: 35.7547\n",
      "Epoch [17/300], Step [30/42], Loss: 31.7857\n",
      "Epoch [17/300], Step [31/42], Loss: 24.8552\n",
      "Epoch [17/300], Step [32/42], Loss: 26.7557\n",
      "Epoch [17/300], Step [33/42], Loss: 32.4668\n",
      "Epoch [17/300], Step [34/42], Loss: 26.1902\n",
      "Epoch [17/300], Step [35/42], Loss: 22.2576\n",
      "Epoch [17/300], Step [36/42], Loss: 24.9669\n",
      "Epoch [17/300], Step [37/42], Loss: 19.9128\n",
      "Epoch [17/300], Step [38/42], Loss: 20.3187\n",
      "Epoch [17/300], Step [39/42], Loss: 20.6159\n",
      "Epoch [17/300], Step [40/42], Loss: 26.8703\n",
      "Epoch [17/300], Step [41/42], Loss: 22.2929\n",
      "Epoch [17/300], Step [42/42], Loss: 5.0734\n",
      "Val. loss :24.6990\n",
      "Epoch [18/300], Step [1/42], Loss: 24.9199\n",
      "Epoch [18/300], Step [2/42], Loss: 24.3835\n",
      "Epoch [18/300], Step [3/42], Loss: 24.5785\n",
      "Epoch [18/300], Step [4/42], Loss: 20.0275\n",
      "Epoch [18/300], Step [5/42], Loss: 19.9192\n",
      "Epoch [18/300], Step [6/42], Loss: 25.2948\n",
      "Epoch [18/300], Step [7/42], Loss: 22.7018\n",
      "Epoch [18/300], Step [8/42], Loss: 21.8010\n",
      "Epoch [18/300], Step [9/42], Loss: 22.0927\n",
      "Epoch [18/300], Step [10/42], Loss: 23.3010\n",
      "Epoch [18/300], Step [11/42], Loss: 19.3914\n",
      "Epoch [18/300], Step [12/42], Loss: 30.2477\n",
      "Epoch [18/300], Step [13/42], Loss: 21.9256\n",
      "Epoch [18/300], Step [14/42], Loss: 28.1614\n",
      "Epoch [18/300], Step [15/42], Loss: 18.3578\n",
      "Epoch [18/300], Step [16/42], Loss: 25.3155\n",
      "Epoch [18/300], Step [17/42], Loss: 23.1916\n",
      "Epoch [18/300], Step [18/42], Loss: 25.4614\n",
      "Epoch [18/300], Step [19/42], Loss: 27.2672\n",
      "Epoch [18/300], Step [20/42], Loss: 20.1764\n",
      "Epoch [18/300], Step [21/42], Loss: 21.7308\n",
      "Epoch [18/300], Step [22/42], Loss: 23.8208\n",
      "Epoch [18/300], Step [23/42], Loss: 21.2171\n",
      "Epoch [18/300], Step [24/42], Loss: 19.1170\n",
      "Epoch [18/300], Step [25/42], Loss: 19.5430\n",
      "Epoch [18/300], Step [26/42], Loss: 18.0125\n",
      "Epoch [18/300], Step [27/42], Loss: 24.5409\n",
      "Epoch [18/300], Step [28/42], Loss: 25.8599\n",
      "Epoch [18/300], Step [29/42], Loss: 22.6960\n",
      "Epoch [18/300], Step [30/42], Loss: 19.5380\n",
      "Epoch [18/300], Step [31/42], Loss: 24.1669\n",
      "Epoch [18/300], Step [32/42], Loss: 23.4613\n",
      "Epoch [18/300], Step [33/42], Loss: 21.1157\n",
      "Epoch [18/300], Step [34/42], Loss: 23.9289\n",
      "Epoch [18/300], Step [35/42], Loss: 31.6476\n",
      "Epoch [18/300], Step [36/42], Loss: 22.2335\n",
      "Epoch [18/300], Step [37/42], Loss: 19.7691\n",
      "Epoch [18/300], Step [38/42], Loss: 18.9591\n",
      "Epoch [18/300], Step [39/42], Loss: 24.8247\n",
      "Epoch [18/300], Step [40/42], Loss: 17.2507\n",
      "Epoch [18/300], Step [41/42], Loss: 19.0659\n",
      "Epoch [18/300], Step [42/42], Loss: 5.5647\n",
      "Val. loss :22.3827\n",
      "Epoch [19/300], Step [1/42], Loss: 27.0621\n",
      "Epoch [19/300], Step [2/42], Loss: 20.5194\n",
      "Epoch [19/300], Step [3/42], Loss: 26.9199\n",
      "Epoch [19/300], Step [4/42], Loss: 26.6075\n",
      "Epoch [19/300], Step [5/42], Loss: 23.1865\n",
      "Epoch [19/300], Step [6/42], Loss: 24.4102\n",
      "Epoch [19/300], Step [7/42], Loss: 19.2429\n",
      "Epoch [19/300], Step [8/42], Loss: 22.4315\n",
      "Epoch [19/300], Step [9/42], Loss: 22.2979\n",
      "Epoch [19/300], Step [10/42], Loss: 18.5378\n",
      "Epoch [19/300], Step [11/42], Loss: 15.9595\n",
      "Epoch [19/300], Step [12/42], Loss: 16.3028\n",
      "Epoch [19/300], Step [13/42], Loss: 18.2830\n",
      "Epoch [19/300], Step [14/42], Loss: 22.9395\n",
      "Epoch [19/300], Step [15/42], Loss: 14.9097\n",
      "Epoch [19/300], Step [16/42], Loss: 16.3575\n",
      "Epoch [19/300], Step [17/42], Loss: 26.0045\n",
      "Epoch [19/300], Step [18/42], Loss: 15.7526\n",
      "Epoch [19/300], Step [19/42], Loss: 26.1564\n",
      "Epoch [19/300], Step [20/42], Loss: 17.6865\n",
      "Epoch [19/300], Step [21/42], Loss: 15.4672\n",
      "Epoch [19/300], Step [22/42], Loss: 15.5682\n",
      "Epoch [19/300], Step [23/42], Loss: 17.9548\n",
      "Epoch [19/300], Step [24/42], Loss: 25.2722\n",
      "Epoch [19/300], Step [25/42], Loss: 24.2076\n",
      "Epoch [19/300], Step [26/42], Loss: 24.3223\n",
      "Epoch [19/300], Step [27/42], Loss: 17.9451\n",
      "Epoch [19/300], Step [28/42], Loss: 17.5523\n",
      "Epoch [19/300], Step [29/42], Loss: 25.1010\n",
      "Epoch [19/300], Step [30/42], Loss: 20.2246\n",
      "Epoch [19/300], Step [31/42], Loss: 23.7890\n",
      "Epoch [19/300], Step [32/42], Loss: 17.1723\n",
      "Epoch [19/300], Step [33/42], Loss: 24.3903\n",
      "Epoch [19/300], Step [34/42], Loss: 19.4009\n",
      "Epoch [19/300], Step [35/42], Loss: 23.1272\n",
      "Epoch [19/300], Step [36/42], Loss: 16.7903\n",
      "Epoch [19/300], Step [37/42], Loss: 18.9547\n",
      "Epoch [19/300], Step [38/42], Loss: 20.7695\n",
      "Epoch [19/300], Step [39/42], Loss: 26.0589\n",
      "Epoch [19/300], Step [40/42], Loss: 19.8854\n",
      "Epoch [19/300], Step [41/42], Loss: 16.3395\n",
      "Epoch [19/300], Step [42/42], Loss: 4.7992\n",
      "Val. loss :21.2399\n",
      "Epoch [20/300], Step [1/42], Loss: 23.4933\n",
      "Epoch [20/300], Step [2/42], Loss: 19.8341\n",
      "Epoch [20/300], Step [3/42], Loss: 24.0111\n",
      "Epoch [20/300], Step [4/42], Loss: 22.6795\n",
      "Epoch [20/300], Step [5/42], Loss: 16.1598\n",
      "Epoch [20/300], Step [6/42], Loss: 15.3978\n",
      "Epoch [20/300], Step [7/42], Loss: 24.3887\n",
      "Epoch [20/300], Step [8/42], Loss: 22.0549\n",
      "Epoch [20/300], Step [9/42], Loss: 22.0146\n",
      "Epoch [20/300], Step [10/42], Loss: 15.2791\n",
      "Epoch [20/300], Step [11/42], Loss: 23.2689\n",
      "Epoch [20/300], Step [12/42], Loss: 19.6118\n",
      "Epoch [20/300], Step [13/42], Loss: 17.4666\n",
      "Epoch [20/300], Step [14/42], Loss: 16.6961\n",
      "Epoch [20/300], Step [15/42], Loss: 19.2719\n",
      "Epoch [20/300], Step [16/42], Loss: 18.9782\n",
      "Epoch [20/300], Step [17/42], Loss: 22.6913\n",
      "Epoch [20/300], Step [18/42], Loss: 16.2838\n",
      "Epoch [20/300], Step [19/42], Loss: 16.7656\n",
      "Epoch [20/300], Step [20/42], Loss: 25.2928\n",
      "Epoch [20/300], Step [21/42], Loss: 20.6361\n",
      "Epoch [20/300], Step [22/42], Loss: 18.1732\n",
      "Epoch [20/300], Step [23/42], Loss: 16.5070\n",
      "Epoch [20/300], Step [24/42], Loss: 22.8169\n",
      "Epoch [20/300], Step [25/42], Loss: 18.3637\n",
      "Epoch [20/300], Step [26/42], Loss: 19.7065\n",
      "Epoch [20/300], Step [27/42], Loss: 16.6974\n",
      "Epoch [20/300], Step [28/42], Loss: 19.8014\n",
      "Epoch [20/300], Step [29/42], Loss: 18.0998\n",
      "Epoch [20/300], Step [30/42], Loss: 21.9667\n",
      "Epoch [20/300], Step [31/42], Loss: 20.3652\n",
      "Epoch [20/300], Step [32/42], Loss: 16.3791\n",
      "Epoch [20/300], Step [33/42], Loss: 21.6471\n",
      "Epoch [20/300], Step [34/42], Loss: 19.6588\n",
      "Epoch [20/300], Step [35/42], Loss: 22.1137\n",
      "Epoch [20/300], Step [36/42], Loss: 22.8494\n",
      "Epoch [20/300], Step [37/42], Loss: 23.9775\n",
      "Epoch [20/300], Step [38/42], Loss: 15.5283\n",
      "Epoch [20/300], Step [39/42], Loss: 15.8576\n",
      "Epoch [20/300], Step [40/42], Loss: 16.7655\n",
      "Epoch [20/300], Step [41/42], Loss: 18.4207\n",
      "Epoch [20/300], Step [42/42], Loss: 2.6663\n",
      "Val. loss :20.4168\n",
      "Epoch [21/300], Step [1/42], Loss: 20.1126\n",
      "Epoch [21/300], Step [2/42], Loss: 15.8359\n",
      "Epoch [21/300], Step [3/42], Loss: 19.0625\n",
      "Epoch [21/300], Step [4/42], Loss: 18.8332\n",
      "Epoch [21/300], Step [5/42], Loss: 18.9174\n",
      "Epoch [21/300], Step [6/42], Loss: 17.2901\n",
      "Epoch [21/300], Step [7/42], Loss: 15.3965\n",
      "Epoch [21/300], Step [8/42], Loss: 17.3972\n",
      "Epoch [21/300], Step [9/42], Loss: 23.1183\n",
      "Epoch [21/300], Step [10/42], Loss: 20.4112\n",
      "Epoch [21/300], Step [11/42], Loss: 18.4538\n",
      "Epoch [21/300], Step [12/42], Loss: 20.9501\n",
      "Epoch [21/300], Step [13/42], Loss: 14.6378\n",
      "Epoch [21/300], Step [14/42], Loss: 19.4758\n",
      "Epoch [21/300], Step [15/42], Loss: 15.1683\n",
      "Epoch [21/300], Step [16/42], Loss: 22.2268\n",
      "Epoch [21/300], Step [17/42], Loss: 20.2808\n",
      "Epoch [21/300], Step [18/42], Loss: 12.3651\n",
      "Epoch [21/300], Step [19/42], Loss: 15.9537\n",
      "Epoch [21/300], Step [20/42], Loss: 18.7376\n",
      "Epoch [21/300], Step [21/42], Loss: 21.2629\n",
      "Epoch [21/300], Step [22/42], Loss: 21.1644\n",
      "Epoch [21/300], Step [23/42], Loss: 29.3510\n",
      "Epoch [21/300], Step [24/42], Loss: 18.2364\n",
      "Epoch [21/300], Step [25/42], Loss: 25.7938\n",
      "Epoch [21/300], Step [26/42], Loss: 19.7397\n",
      "Epoch [21/300], Step [27/42], Loss: 19.9568\n",
      "Epoch [21/300], Step [28/42], Loss: 20.0924\n",
      "Epoch [21/300], Step [29/42], Loss: 21.4773\n",
      "Epoch [21/300], Step [30/42], Loss: 15.2268\n",
      "Epoch [21/300], Step [31/42], Loss: 18.6685\n",
      "Epoch [21/300], Step [32/42], Loss: 19.5944\n",
      "Epoch [21/300], Step [33/42], Loss: 17.0379\n",
      "Epoch [21/300], Step [34/42], Loss: 18.1511\n",
      "Epoch [21/300], Step [35/42], Loss: 14.8398\n",
      "Epoch [21/300], Step [36/42], Loss: 14.1975\n",
      "Epoch [21/300], Step [37/42], Loss: 15.0423\n",
      "Epoch [21/300], Step [38/42], Loss: 22.3259\n",
      "Epoch [21/300], Step [39/42], Loss: 16.2759\n",
      "Epoch [21/300], Step [40/42], Loss: 18.3507\n",
      "Epoch [21/300], Step [41/42], Loss: 18.8471\n",
      "Epoch [21/300], Step [42/42], Loss: 3.7038\n",
      "Val. loss :19.1767\n",
      "Epoch [22/300], Step [1/42], Loss: 18.6709\n",
      "Epoch [22/300], Step [2/42], Loss: 17.1806\n",
      "Epoch [22/300], Step [3/42], Loss: 25.1027\n",
      "Epoch [22/300], Step [4/42], Loss: 17.0370\n",
      "Epoch [22/300], Step [5/42], Loss: 18.4722\n",
      "Epoch [22/300], Step [6/42], Loss: 21.1964\n",
      "Epoch [22/300], Step [7/42], Loss: 22.2043\n",
      "Epoch [22/300], Step [8/42], Loss: 16.1208\n",
      "Epoch [22/300], Step [9/42], Loss: 16.7144\n",
      "Epoch [22/300], Step [10/42], Loss: 16.6523\n",
      "Epoch [22/300], Step [11/42], Loss: 16.2137\n",
      "Epoch [22/300], Step [12/42], Loss: 16.5901\n",
      "Epoch [22/300], Step [13/42], Loss: 16.5956\n",
      "Epoch [22/300], Step [14/42], Loss: 20.8193\n",
      "Epoch [22/300], Step [15/42], Loss: 19.9717\n",
      "Epoch [22/300], Step [16/42], Loss: 15.3669\n",
      "Epoch [22/300], Step [17/42], Loss: 22.7045\n",
      "Epoch [22/300], Step [18/42], Loss: 13.5751\n",
      "Epoch [22/300], Step [19/42], Loss: 18.0239\n",
      "Epoch [22/300], Step [20/42], Loss: 11.3198\n",
      "Epoch [22/300], Step [21/42], Loss: 14.4905\n",
      "Epoch [22/300], Step [22/42], Loss: 18.6469\n",
      "Epoch [22/300], Step [23/42], Loss: 14.7842\n",
      "Epoch [22/300], Step [24/42], Loss: 14.7716\n",
      "Epoch [22/300], Step [25/42], Loss: 22.2095\n",
      "Epoch [22/300], Step [26/42], Loss: 18.9621\n",
      "Epoch [22/300], Step [27/42], Loss: 15.7044\n",
      "Epoch [22/300], Step [28/42], Loss: 16.7560\n",
      "Epoch [22/300], Step [29/42], Loss: 13.5561\n",
      "Epoch [22/300], Step [30/42], Loss: 18.6914\n",
      "Epoch [22/300], Step [31/42], Loss: 16.1559\n",
      "Epoch [22/300], Step [32/42], Loss: 18.1496\n",
      "Epoch [22/300], Step [33/42], Loss: 19.9210\n",
      "Epoch [22/300], Step [34/42], Loss: 16.2716\n",
      "Epoch [22/300], Step [35/42], Loss: 20.4245\n",
      "Epoch [22/300], Step [36/42], Loss: 21.3761\n",
      "Epoch [22/300], Step [37/42], Loss: 15.1901\n",
      "Epoch [22/300], Step [38/42], Loss: 13.3419\n",
      "Epoch [22/300], Step [39/42], Loss: 13.4145\n",
      "Epoch [22/300], Step [40/42], Loss: 14.9866\n",
      "Epoch [22/300], Step [41/42], Loss: 17.7811\n",
      "Epoch [22/300], Step [42/42], Loss: 3.9125\n",
      "Val. loss :17.5409\n",
      "Epoch [23/300], Step [1/42], Loss: 14.0798\n",
      "Epoch [23/300], Step [2/42], Loss: 16.4273\n",
      "Epoch [23/300], Step [3/42], Loss: 25.0510\n",
      "Epoch [23/300], Step [4/42], Loss: 13.6713\n",
      "Epoch [23/300], Step [5/42], Loss: 16.5080\n",
      "Epoch [23/300], Step [6/42], Loss: 10.8661\n",
      "Epoch [23/300], Step [7/42], Loss: 22.6339\n",
      "Epoch [23/300], Step [8/42], Loss: 16.2753\n",
      "Epoch [23/300], Step [9/42], Loss: 19.0092\n",
      "Epoch [23/300], Step [10/42], Loss: 18.0288\n",
      "Epoch [23/300], Step [11/42], Loss: 19.5298\n",
      "Epoch [23/300], Step [12/42], Loss: 17.6332\n",
      "Epoch [23/300], Step [13/42], Loss: 17.8045\n",
      "Epoch [23/300], Step [14/42], Loss: 16.2365\n",
      "Epoch [23/300], Step [15/42], Loss: 19.7044\n",
      "Epoch [23/300], Step [16/42], Loss: 13.3951\n",
      "Epoch [23/300], Step [17/42], Loss: 17.2009\n",
      "Epoch [23/300], Step [18/42], Loss: 17.8087\n",
      "Epoch [23/300], Step [19/42], Loss: 12.7303\n",
      "Epoch [23/300], Step [20/42], Loss: 17.2959\n",
      "Epoch [23/300], Step [21/42], Loss: 18.2523\n",
      "Epoch [23/300], Step [22/42], Loss: 13.4335\n",
      "Epoch [23/300], Step [23/42], Loss: 15.3369\n",
      "Epoch [23/300], Step [24/42], Loss: 12.8176\n",
      "Epoch [23/300], Step [25/42], Loss: 15.9184\n",
      "Epoch [23/300], Step [26/42], Loss: 12.9801\n",
      "Epoch [23/300], Step [27/42], Loss: 16.4051\n",
      "Epoch [23/300], Step [28/42], Loss: 15.4536\n",
      "Epoch [23/300], Step [29/42], Loss: 11.5274\n",
      "Epoch [23/300], Step [30/42], Loss: 13.3146\n",
      "Epoch [23/300], Step [31/42], Loss: 19.0091\n",
      "Epoch [23/300], Step [32/42], Loss: 17.7551\n",
      "Epoch [23/300], Step [33/42], Loss: 14.5713\n",
      "Epoch [23/300], Step [34/42], Loss: 13.4616\n",
      "Epoch [23/300], Step [35/42], Loss: 21.5496\n",
      "Epoch [23/300], Step [36/42], Loss: 17.0208\n",
      "Epoch [23/300], Step [37/42], Loss: 18.6368\n",
      "Epoch [23/300], Step [38/42], Loss: 17.0443\n",
      "Epoch [23/300], Step [39/42], Loss: 20.9960\n",
      "Epoch [23/300], Step [40/42], Loss: 11.2572\n",
      "Epoch [23/300], Step [41/42], Loss: 12.2420\n",
      "Epoch [23/300], Step [42/42], Loss: 3.2150\n",
      "Val. loss :16.8377\n",
      "Epoch [24/300], Step [1/42], Loss: 15.8206\n",
      "Epoch [24/300], Step [2/42], Loss: 14.4182\n",
      "Epoch [24/300], Step [3/42], Loss: 17.3781\n",
      "Epoch [24/300], Step [4/42], Loss: 12.7845\n",
      "Epoch [24/300], Step [5/42], Loss: 14.6768\n",
      "Epoch [24/300], Step [6/42], Loss: 13.1317\n",
      "Epoch [24/300], Step [7/42], Loss: 16.6801\n",
      "Epoch [24/300], Step [8/42], Loss: 20.1795\n",
      "Epoch [24/300], Step [9/42], Loss: 19.2884\n",
      "Epoch [24/300], Step [10/42], Loss: 18.3470\n",
      "Epoch [24/300], Step [11/42], Loss: 9.1143\n",
      "Epoch [24/300], Step [12/42], Loss: 16.6088\n",
      "Epoch [24/300], Step [13/42], Loss: 15.9592\n",
      "Epoch [24/300], Step [14/42], Loss: 21.6221\n",
      "Epoch [24/300], Step [15/42], Loss: 15.1583\n",
      "Epoch [24/300], Step [16/42], Loss: 14.9272\n",
      "Epoch [24/300], Step [17/42], Loss: 14.7766\n",
      "Epoch [24/300], Step [18/42], Loss: 11.1300\n",
      "Epoch [24/300], Step [19/42], Loss: 16.2909\n",
      "Epoch [24/300], Step [20/42], Loss: 13.4700\n",
      "Epoch [24/300], Step [21/42], Loss: 13.0065\n",
      "Epoch [24/300], Step [22/42], Loss: 14.1323\n",
      "Epoch [24/300], Step [23/42], Loss: 22.5040\n",
      "Epoch [24/300], Step [24/42], Loss: 14.5113\n",
      "Epoch [24/300], Step [25/42], Loss: 8.6410\n",
      "Epoch [24/300], Step [26/42], Loss: 15.1776\n",
      "Epoch [24/300], Step [27/42], Loss: 19.0760\n",
      "Epoch [24/300], Step [28/42], Loss: 12.0133\n",
      "Epoch [24/300], Step [29/42], Loss: 9.7803\n",
      "Epoch [24/300], Step [30/42], Loss: 22.3349\n",
      "Epoch [24/300], Step [31/42], Loss: 11.9081\n",
      "Epoch [24/300], Step [32/42], Loss: 19.2201\n",
      "Epoch [24/300], Step [33/42], Loss: 10.3777\n",
      "Epoch [24/300], Step [34/42], Loss: 20.7478\n",
      "Epoch [24/300], Step [35/42], Loss: 13.6686\n",
      "Epoch [24/300], Step [36/42], Loss: 15.8820\n",
      "Epoch [24/300], Step [37/42], Loss: 16.1666\n",
      "Epoch [24/300], Step [38/42], Loss: 14.1389\n",
      "Epoch [24/300], Step [39/42], Loss: 15.0740\n",
      "Epoch [24/300], Step [40/42], Loss: 21.5183\n",
      "Epoch [24/300], Step [41/42], Loss: 13.5983\n",
      "Epoch [24/300], Step [42/42], Loss: 4.0634\n",
      "Val. loss :15.7133\n",
      "Epoch [25/300], Step [1/42], Loss: 15.3180\n",
      "Epoch [25/300], Step [2/42], Loss: 12.8212\n",
      "Epoch [25/300], Step [3/42], Loss: 14.3762\n",
      "Epoch [25/300], Step [4/42], Loss: 14.7032\n",
      "Epoch [25/300], Step [5/42], Loss: 12.3036\n",
      "Epoch [25/300], Step [6/42], Loss: 14.8733\n",
      "Epoch [25/300], Step [7/42], Loss: 19.4182\n",
      "Epoch [25/300], Step [8/42], Loss: 19.1944\n",
      "Epoch [25/300], Step [9/42], Loss: 15.7464\n",
      "Epoch [25/300], Step [10/42], Loss: 12.6534\n",
      "Epoch [25/300], Step [11/42], Loss: 12.3665\n",
      "Epoch [25/300], Step [12/42], Loss: 18.4588\n",
      "Epoch [25/300], Step [13/42], Loss: 11.8007\n",
      "Epoch [25/300], Step [14/42], Loss: 14.7870\n",
      "Epoch [25/300], Step [15/42], Loss: 16.4646\n",
      "Epoch [25/300], Step [16/42], Loss: 17.8257\n",
      "Epoch [25/300], Step [17/42], Loss: 17.0711\n",
      "Epoch [25/300], Step [18/42], Loss: 15.9187\n",
      "Epoch [25/300], Step [19/42], Loss: 10.8871\n",
      "Epoch [25/300], Step [20/42], Loss: 18.0879\n",
      "Epoch [25/300], Step [21/42], Loss: 17.0726\n",
      "Epoch [25/300], Step [22/42], Loss: 13.4502\n",
      "Epoch [25/300], Step [23/42], Loss: 13.6311\n",
      "Epoch [25/300], Step [24/42], Loss: 15.3953\n",
      "Epoch [25/300], Step [25/42], Loss: 14.0380\n",
      "Epoch [25/300], Step [26/42], Loss: 16.3641\n",
      "Epoch [25/300], Step [27/42], Loss: 11.9212\n",
      "Epoch [25/300], Step [28/42], Loss: 13.7315\n",
      "Epoch [25/300], Step [29/42], Loss: 12.2572\n",
      "Epoch [25/300], Step [30/42], Loss: 16.7625\n",
      "Epoch [25/300], Step [31/42], Loss: 15.0466\n",
      "Epoch [25/300], Step [32/42], Loss: 13.7382\n",
      "Epoch [25/300], Step [33/42], Loss: 10.9455\n",
      "Epoch [25/300], Step [34/42], Loss: 14.8515\n",
      "Epoch [25/300], Step [35/42], Loss: 9.2199\n",
      "Epoch [25/300], Step [36/42], Loss: 18.7279\n",
      "Epoch [25/300], Step [37/42], Loss: 14.8105\n",
      "Epoch [25/300], Step [38/42], Loss: 16.9676\n",
      "Epoch [25/300], Step [39/42], Loss: 10.1324\n",
      "Epoch [25/300], Step [40/42], Loss: 12.4178\n",
      "Epoch [25/300], Step [41/42], Loss: 17.9060\n",
      "Epoch [25/300], Step [42/42], Loss: 3.6354\n",
      "Val. loss :14.5843\n",
      "Epoch [26/300], Step [1/42], Loss: 16.3994\n",
      "Epoch [26/300], Step [2/42], Loss: 14.5953\n",
      "Epoch [26/300], Step [3/42], Loss: 9.8647\n",
      "Epoch [26/300], Step [4/42], Loss: 14.8675\n",
      "Epoch [26/300], Step [5/42], Loss: 14.7006\n",
      "Epoch [26/300], Step [6/42], Loss: 12.4538\n",
      "Epoch [26/300], Step [7/42], Loss: 14.0928\n",
      "Epoch [26/300], Step [8/42], Loss: 13.6262\n",
      "Epoch [26/300], Step [9/42], Loss: 13.3186\n",
      "Epoch [26/300], Step [10/42], Loss: 14.0849\n",
      "Epoch [26/300], Step [11/42], Loss: 15.7333\n",
      "Epoch [26/300], Step [12/42], Loss: 14.8789\n",
      "Epoch [26/300], Step [13/42], Loss: 11.7785\n",
      "Epoch [26/300], Step [14/42], Loss: 20.2492\n",
      "Epoch [26/300], Step [15/42], Loss: 13.2029\n",
      "Epoch [26/300], Step [16/42], Loss: 11.1590\n",
      "Epoch [26/300], Step [17/42], Loss: 9.1191\n",
      "Epoch [26/300], Step [18/42], Loss: 14.9724\n",
      "Epoch [26/300], Step [19/42], Loss: 12.9215\n",
      "Epoch [26/300], Step [20/42], Loss: 13.1528\n",
      "Epoch [26/300], Step [21/42], Loss: 16.8316\n",
      "Epoch [26/300], Step [22/42], Loss: 19.1667\n",
      "Epoch [26/300], Step [23/42], Loss: 14.9631\n",
      "Epoch [26/300], Step [24/42], Loss: 9.7822\n",
      "Epoch [26/300], Step [25/42], Loss: 17.6145\n",
      "Epoch [26/300], Step [26/42], Loss: 11.7486\n",
      "Epoch [26/300], Step [27/42], Loss: 11.2686\n",
      "Epoch [26/300], Step [28/42], Loss: 11.8792\n",
      "Epoch [26/300], Step [29/42], Loss: 15.7505\n",
      "Epoch [26/300], Step [30/42], Loss: 15.8125\n",
      "Epoch [26/300], Step [31/42], Loss: 15.5044\n",
      "Epoch [26/300], Step [32/42], Loss: 12.7134\n",
      "Epoch [26/300], Step [33/42], Loss: 16.5511\n",
      "Epoch [26/300], Step [34/42], Loss: 12.8839\n",
      "Epoch [26/300], Step [35/42], Loss: 10.3833\n",
      "Epoch [26/300], Step [36/42], Loss: 16.3629\n",
      "Epoch [26/300], Step [37/42], Loss: 16.4046\n",
      "Epoch [26/300], Step [38/42], Loss: 12.0009\n",
      "Epoch [26/300], Step [39/42], Loss: 8.7417\n",
      "Epoch [26/300], Step [40/42], Loss: 11.7728\n",
      "Epoch [26/300], Step [41/42], Loss: 11.3406\n",
      "Epoch [26/300], Step [42/42], Loss: 3.5252\n",
      "Val. loss :14.1793\n",
      "Epoch [27/300], Step [1/42], Loss: 9.5193\n",
      "Epoch [27/300], Step [2/42], Loss: 9.7130\n",
      "Epoch [27/300], Step [3/42], Loss: 13.4758\n",
      "Epoch [27/300], Step [4/42], Loss: 12.9779\n",
      "Epoch [27/300], Step [5/42], Loss: 11.4305\n",
      "Epoch [27/300], Step [6/42], Loss: 11.2459\n",
      "Epoch [27/300], Step [7/42], Loss: 13.8718\n",
      "Epoch [27/300], Step [8/42], Loss: 13.0868\n",
      "Epoch [27/300], Step [9/42], Loss: 15.4612\n",
      "Epoch [27/300], Step [10/42], Loss: 13.6008\n",
      "Epoch [27/300], Step [11/42], Loss: 12.3148\n",
      "Epoch [27/300], Step [12/42], Loss: 13.0322\n",
      "Epoch [27/300], Step [13/42], Loss: 11.1716\n",
      "Epoch [27/300], Step [14/42], Loss: 12.3495\n",
      "Epoch [27/300], Step [15/42], Loss: 13.3586\n",
      "Epoch [27/300], Step [16/42], Loss: 16.2201\n",
      "Epoch [27/300], Step [17/42], Loss: 18.4809\n",
      "Epoch [27/300], Step [18/42], Loss: 16.5493\n",
      "Epoch [27/300], Step [19/42], Loss: 12.3873\n",
      "Epoch [27/300], Step [20/42], Loss: 18.0570\n",
      "Epoch [27/300], Step [21/42], Loss: 8.8158\n",
      "Epoch [27/300], Step [22/42], Loss: 14.1124\n",
      "Epoch [27/300], Step [23/42], Loss: 13.5960\n",
      "Epoch [27/300], Step [24/42], Loss: 11.9507\n",
      "Epoch [27/300], Step [25/42], Loss: 15.5986\n",
      "Epoch [27/300], Step [26/42], Loss: 15.1306\n",
      "Epoch [27/300], Step [27/42], Loss: 12.4738\n",
      "Epoch [27/300], Step [28/42], Loss: 13.6383\n",
      "Epoch [27/300], Step [29/42], Loss: 11.8894\n",
      "Epoch [27/300], Step [30/42], Loss: 10.6480\n",
      "Epoch [27/300], Step [31/42], Loss: 14.8455\n",
      "Epoch [27/300], Step [32/42], Loss: 9.3562\n",
      "Epoch [27/300], Step [33/42], Loss: 14.6693\n",
      "Epoch [27/300], Step [34/42], Loss: 14.8456\n",
      "Epoch [27/300], Step [35/42], Loss: 13.2389\n",
      "Epoch [27/300], Step [36/42], Loss: 15.6961\n",
      "Epoch [27/300], Step [37/42], Loss: 12.6523\n",
      "Epoch [27/300], Step [38/42], Loss: 9.8170\n",
      "Epoch [27/300], Step [39/42], Loss: 11.9761\n",
      "Epoch [27/300], Step [40/42], Loss: 14.9979\n",
      "Epoch [27/300], Step [41/42], Loss: 12.0198\n",
      "Epoch [27/300], Step [42/42], Loss: 3.1951\n",
      "Val. loss :12.8891\n",
      "Epoch [28/300], Step [1/42], Loss: 10.9441\n",
      "Epoch [28/300], Step [2/42], Loss: 13.6733\n",
      "Epoch [28/300], Step [3/42], Loss: 15.8705\n",
      "Epoch [28/300], Step [4/42], Loss: 9.8223\n",
      "Epoch [28/300], Step [5/42], Loss: 12.4913\n",
      "Epoch [28/300], Step [6/42], Loss: 8.3314\n",
      "Epoch [28/300], Step [7/42], Loss: 15.3543\n",
      "Epoch [28/300], Step [8/42], Loss: 10.5412\n",
      "Epoch [28/300], Step [9/42], Loss: 12.7044\n",
      "Epoch [28/300], Step [10/42], Loss: 11.4324\n",
      "Epoch [28/300], Step [11/42], Loss: 6.2605\n",
      "Epoch [28/300], Step [12/42], Loss: 9.6355\n",
      "Epoch [28/300], Step [13/42], Loss: 12.4954\n",
      "Epoch [28/300], Step [14/42], Loss: 14.2061\n",
      "Epoch [28/300], Step [15/42], Loss: 10.9052\n",
      "Epoch [28/300], Step [16/42], Loss: 8.2869\n",
      "Epoch [28/300], Step [17/42], Loss: 11.7033\n",
      "Epoch [28/300], Step [18/42], Loss: 15.6711\n",
      "Epoch [28/300], Step [19/42], Loss: 17.9192\n",
      "Epoch [28/300], Step [20/42], Loss: 11.1519\n",
      "Epoch [28/300], Step [21/42], Loss: 8.9710\n",
      "Epoch [28/300], Step [22/42], Loss: 8.1253\n",
      "Epoch [28/300], Step [23/42], Loss: 14.7454\n",
      "Epoch [28/300], Step [24/42], Loss: 11.1328\n",
      "Epoch [28/300], Step [25/42], Loss: 8.8488\n",
      "Epoch [28/300], Step [26/42], Loss: 16.1852\n",
      "Epoch [28/300], Step [27/42], Loss: 9.0733\n",
      "Epoch [28/300], Step [28/42], Loss: 13.8015\n",
      "Epoch [28/300], Step [29/42], Loss: 11.6945\n",
      "Epoch [28/300], Step [30/42], Loss: 12.4830\n",
      "Epoch [28/300], Step [31/42], Loss: 13.4850\n",
      "Epoch [28/300], Step [32/42], Loss: 14.1753\n",
      "Epoch [28/300], Step [33/42], Loss: 13.4517\n",
      "Epoch [28/300], Step [34/42], Loss: 14.1292\n",
      "Epoch [28/300], Step [35/42], Loss: 13.5707\n",
      "Epoch [28/300], Step [36/42], Loss: 15.5579\n",
      "Epoch [28/300], Step [37/42], Loss: 16.4076\n",
      "Epoch [28/300], Step [38/42], Loss: 9.1854\n",
      "Epoch [28/300], Step [39/42], Loss: 10.4116\n",
      "Epoch [28/300], Step [40/42], Loss: 11.0465\n",
      "Epoch [28/300], Step [41/42], Loss: 12.4473\n",
      "Epoch [28/300], Step [42/42], Loss: 2.8396\n",
      "Val. loss :12.7406\n",
      "Epoch [29/300], Step [1/42], Loss: 12.3271\n",
      "Epoch [29/300], Step [2/42], Loss: 11.4352\n",
      "Epoch [29/300], Step [3/42], Loss: 8.1484\n",
      "Epoch [29/300], Step [4/42], Loss: 7.8922\n",
      "Epoch [29/300], Step [5/42], Loss: 16.1201\n",
      "Epoch [29/300], Step [6/42], Loss: 6.9541\n",
      "Epoch [29/300], Step [7/42], Loss: 11.1199\n",
      "Epoch [29/300], Step [8/42], Loss: 15.6690\n",
      "Epoch [29/300], Step [9/42], Loss: 12.3130\n",
      "Epoch [29/300], Step [10/42], Loss: 9.5811\n",
      "Epoch [29/300], Step [11/42], Loss: 13.2554\n",
      "Epoch [29/300], Step [12/42], Loss: 12.4272\n",
      "Epoch [29/300], Step [13/42], Loss: 13.4796\n",
      "Epoch [29/300], Step [14/42], Loss: 11.3594\n",
      "Epoch [29/300], Step [15/42], Loss: 12.6059\n",
      "Epoch [29/300], Step [16/42], Loss: 11.1092\n",
      "Epoch [29/300], Step [17/42], Loss: 9.4240\n",
      "Epoch [29/300], Step [18/42], Loss: 10.9398\n",
      "Epoch [29/300], Step [19/42], Loss: 10.7617\n",
      "Epoch [29/300], Step [20/42], Loss: 10.2807\n",
      "Epoch [29/300], Step [21/42], Loss: 14.2464\n",
      "Epoch [29/300], Step [22/42], Loss: 11.2777\n",
      "Epoch [29/300], Step [23/42], Loss: 12.9865\n",
      "Epoch [29/300], Step [24/42], Loss: 8.3095\n",
      "Epoch [29/300], Step [25/42], Loss: 13.2109\n",
      "Epoch [29/300], Step [26/42], Loss: 11.3999\n",
      "Epoch [29/300], Step [27/42], Loss: 15.1979\n",
      "Epoch [29/300], Step [28/42], Loss: 13.6274\n",
      "Epoch [29/300], Step [29/42], Loss: 11.9525\n",
      "Epoch [29/300], Step [30/42], Loss: 15.6684\n",
      "Epoch [29/300], Step [31/42], Loss: 10.5458\n",
      "Epoch [29/300], Step [32/42], Loss: 7.5343\n",
      "Epoch [29/300], Step [33/42], Loss: 11.6152\n",
      "Epoch [29/300], Step [34/42], Loss: 10.6846\n",
      "Epoch [29/300], Step [35/42], Loss: 10.4631\n",
      "Epoch [29/300], Step [36/42], Loss: 10.8219\n",
      "Epoch [29/300], Step [37/42], Loss: 9.3544\n",
      "Epoch [29/300], Step [38/42], Loss: 12.8005\n",
      "Epoch [29/300], Step [39/42], Loss: 15.2026\n",
      "Epoch [29/300], Step [40/42], Loss: 9.3934\n",
      "Epoch [29/300], Step [41/42], Loss: 10.0719\n",
      "Epoch [29/300], Step [42/42], Loss: 2.7246\n",
      "Val. loss :11.4534\n",
      "Epoch [30/300], Step [1/42], Loss: 8.6473\n",
      "Epoch [30/300], Step [2/42], Loss: 13.5332\n",
      "Epoch [30/300], Step [3/42], Loss: 7.4578\n",
      "Epoch [30/300], Step [4/42], Loss: 9.5004\n",
      "Epoch [30/300], Step [5/42], Loss: 7.1884\n",
      "Epoch [30/300], Step [6/42], Loss: 9.2086\n",
      "Epoch [30/300], Step [7/42], Loss: 11.8806\n",
      "Epoch [30/300], Step [8/42], Loss: 10.4753\n",
      "Epoch [30/300], Step [9/42], Loss: 9.6417\n",
      "Epoch [30/300], Step [10/42], Loss: 12.9136\n",
      "Epoch [30/300], Step [11/42], Loss: 14.6010\n",
      "Epoch [30/300], Step [12/42], Loss: 9.2951\n",
      "Epoch [30/300], Step [13/42], Loss: 11.7336\n",
      "Epoch [30/300], Step [14/42], Loss: 9.7641\n",
      "Epoch [30/300], Step [15/42], Loss: 12.1639\n",
      "Epoch [30/300], Step [16/42], Loss: 8.3509\n",
      "Epoch [30/300], Step [17/42], Loss: 8.7879\n",
      "Epoch [30/300], Step [18/42], Loss: 10.4551\n",
      "Epoch [30/300], Step [19/42], Loss: 11.0939\n",
      "Epoch [30/300], Step [20/42], Loss: 12.8491\n",
      "Epoch [30/300], Step [21/42], Loss: 10.8977\n",
      "Epoch [30/300], Step [22/42], Loss: 10.7649\n",
      "Epoch [30/300], Step [23/42], Loss: 8.3984\n",
      "Epoch [30/300], Step [24/42], Loss: 7.6458\n",
      "Epoch [30/300], Step [25/42], Loss: 8.9702\n",
      "Epoch [30/300], Step [26/42], Loss: 11.3437\n",
      "Epoch [30/300], Step [27/42], Loss: 13.1544\n",
      "Epoch [30/300], Step [28/42], Loss: 11.0602\n",
      "Epoch [30/300], Step [29/42], Loss: 10.0133\n",
      "Epoch [30/300], Step [30/42], Loss: 11.2309\n",
      "Epoch [30/300], Step [31/42], Loss: 14.3117\n",
      "Epoch [30/300], Step [32/42], Loss: 6.8493\n",
      "Epoch [30/300], Step [33/42], Loss: 12.9846\n",
      "Epoch [30/300], Step [34/42], Loss: 10.3988\n",
      "Epoch [30/300], Step [35/42], Loss: 12.2356\n",
      "Epoch [30/300], Step [36/42], Loss: 11.2901\n",
      "Epoch [30/300], Step [37/42], Loss: 9.0842\n",
      "Epoch [30/300], Step [38/42], Loss: 11.0128\n",
      "Epoch [30/300], Step [39/42], Loss: 15.5521\n",
      "Epoch [30/300], Step [40/42], Loss: 8.4123\n",
      "Epoch [30/300], Step [41/42], Loss: 7.9425\n",
      "Epoch [30/300], Step [42/42], Loss: 1.3308\n",
      "Val. loss :11.3553\n",
      "Epoch [31/300], Step [1/42], Loss: 12.0959\n",
      "Epoch [31/300], Step [2/42], Loss: 12.1068\n",
      "Epoch [31/300], Step [3/42], Loss: 10.8018\n",
      "Epoch [31/300], Step [4/42], Loss: 7.7334\n",
      "Epoch [31/300], Step [5/42], Loss: 12.8366\n",
      "Epoch [31/300], Step [6/42], Loss: 10.8124\n",
      "Epoch [31/300], Step [7/42], Loss: 13.2392\n",
      "Epoch [31/300], Step [8/42], Loss: 10.0106\n",
      "Epoch [31/300], Step [9/42], Loss: 10.4306\n",
      "Epoch [31/300], Step [10/42], Loss: 8.5013\n",
      "Epoch [31/300], Step [11/42], Loss: 9.1004\n",
      "Epoch [31/300], Step [12/42], Loss: 10.8712\n",
      "Epoch [31/300], Step [13/42], Loss: 12.0763\n",
      "Epoch [31/300], Step [14/42], Loss: 15.6332\n",
      "Epoch [31/300], Step [15/42], Loss: 10.8948\n",
      "Epoch [31/300], Step [16/42], Loss: 7.7653\n",
      "Epoch [31/300], Step [17/42], Loss: 6.9336\n",
      "Epoch [31/300], Step [18/42], Loss: 9.4048\n",
      "Epoch [31/300], Step [19/42], Loss: 9.4801\n",
      "Epoch [31/300], Step [20/42], Loss: 14.6774\n",
      "Epoch [31/300], Step [21/42], Loss: 10.8882\n",
      "Epoch [31/300], Step [22/42], Loss: 5.9906\n",
      "Epoch [31/300], Step [23/42], Loss: 9.6752\n",
      "Epoch [31/300], Step [24/42], Loss: 11.1064\n",
      "Epoch [31/300], Step [25/42], Loss: 8.7541\n",
      "Epoch [31/300], Step [26/42], Loss: 8.3838\n",
      "Epoch [31/300], Step [27/42], Loss: 12.4670\n",
      "Epoch [31/300], Step [28/42], Loss: 9.7608\n",
      "Epoch [31/300], Step [29/42], Loss: 6.9799\n",
      "Epoch [31/300], Step [30/42], Loss: 7.7329\n",
      "Epoch [31/300], Step [31/42], Loss: 12.0224\n",
      "Epoch [31/300], Step [32/42], Loss: 9.9267\n",
      "Epoch [31/300], Step [33/42], Loss: 14.0016\n",
      "Epoch [31/300], Step [34/42], Loss: 9.4759\n",
      "Epoch [31/300], Step [35/42], Loss: 8.9478\n",
      "Epoch [31/300], Step [36/42], Loss: 10.3951\n",
      "Epoch [31/300], Step [37/42], Loss: 13.5216\n",
      "Epoch [31/300], Step [38/42], Loss: 8.5390\n",
      "Epoch [31/300], Step [39/42], Loss: 13.4421\n",
      "Epoch [31/300], Step [40/42], Loss: 13.4932\n",
      "Epoch [31/300], Step [41/42], Loss: 10.7237\n",
      "Epoch [31/300], Step [42/42], Loss: 2.6016\n",
      "Val. loss :12.1184\n",
      "Epoch [32/300], Step [1/42], Loss: 10.8961\n",
      "Epoch [32/300], Step [2/42], Loss: 13.6649\n",
      "Epoch [32/300], Step [3/42], Loss: 13.7200\n",
      "Epoch [32/300], Step [4/42], Loss: 8.3812\n",
      "Epoch [32/300], Step [5/42], Loss: 9.9940\n",
      "Epoch [32/300], Step [6/42], Loss: 8.7705\n",
      "Epoch [32/300], Step [7/42], Loss: 11.2829\n",
      "Epoch [32/300], Step [8/42], Loss: 10.2079\n",
      "Epoch [32/300], Step [9/42], Loss: 6.6541\n",
      "Epoch [32/300], Step [10/42], Loss: 10.8205\n",
      "Epoch [32/300], Step [11/42], Loss: 13.1638\n",
      "Epoch [32/300], Step [12/42], Loss: 10.9197\n",
      "Epoch [32/300], Step [13/42], Loss: 11.3739\n",
      "Epoch [32/300], Step [14/42], Loss: 10.2023\n",
      "Epoch [32/300], Step [15/42], Loss: 9.3830\n",
      "Epoch [32/300], Step [16/42], Loss: 12.2939\n",
      "Epoch [32/300], Step [17/42], Loss: 8.0415\n",
      "Epoch [32/300], Step [18/42], Loss: 14.3633\n",
      "Epoch [32/300], Step [19/42], Loss: 10.9335\n",
      "Epoch [32/300], Step [20/42], Loss: 10.6158\n",
      "Epoch [32/300], Step [21/42], Loss: 8.0933\n",
      "Epoch [32/300], Step [22/42], Loss: 7.9886\n",
      "Epoch [32/300], Step [23/42], Loss: 14.6985\n",
      "Epoch [32/300], Step [24/42], Loss: 10.6427\n",
      "Epoch [32/300], Step [25/42], Loss: 8.9095\n",
      "Epoch [32/300], Step [26/42], Loss: 7.7491\n",
      "Epoch [32/300], Step [27/42], Loss: 8.4622\n",
      "Epoch [32/300], Step [28/42], Loss: 10.2292\n",
      "Epoch [32/300], Step [29/42], Loss: 7.0877\n",
      "Epoch [32/300], Step [30/42], Loss: 10.6545\n",
      "Epoch [32/300], Step [31/42], Loss: 6.8205\n",
      "Epoch [32/300], Step [32/42], Loss: 10.8832\n",
      "Epoch [32/300], Step [33/42], Loss: 7.9690\n",
      "Epoch [32/300], Step [34/42], Loss: 9.8502\n",
      "Epoch [32/300], Step [35/42], Loss: 5.8508\n",
      "Epoch [32/300], Step [36/42], Loss: 8.3150\n",
      "Epoch [32/300], Step [37/42], Loss: 6.7799\n",
      "Epoch [32/300], Step [38/42], Loss: 8.3745\n",
      "Epoch [32/300], Step [39/42], Loss: 10.4757\n",
      "Epoch [32/300], Step [40/42], Loss: 8.8005\n",
      "Epoch [32/300], Step [41/42], Loss: 10.9279\n",
      "Epoch [32/300], Step [42/42], Loss: 2.1445\n",
      "Val. loss :10.9696\n",
      "Epoch [33/300], Step [1/42], Loss: 7.2375\n",
      "Epoch [33/300], Step [2/42], Loss: 7.8413\n",
      "Epoch [33/300], Step [3/42], Loss: 9.7305\n",
      "Epoch [33/300], Step [4/42], Loss: 8.9865\n",
      "Epoch [33/300], Step [5/42], Loss: 8.9228\n",
      "Epoch [33/300], Step [6/42], Loss: 7.4641\n",
      "Epoch [33/300], Step [7/42], Loss: 8.9136\n",
      "Epoch [33/300], Step [8/42], Loss: 7.0470\n",
      "Epoch [33/300], Step [9/42], Loss: 7.8693\n",
      "Epoch [33/300], Step [10/42], Loss: 8.7731\n",
      "Epoch [33/300], Step [11/42], Loss: 9.3019\n",
      "Epoch [33/300], Step [12/42], Loss: 12.0495\n",
      "Epoch [33/300], Step [13/42], Loss: 7.1009\n",
      "Epoch [33/300], Step [14/42], Loss: 11.7876\n",
      "Epoch [33/300], Step [15/42], Loss: 9.6463\n",
      "Epoch [33/300], Step [16/42], Loss: 11.5767\n",
      "Epoch [33/300], Step [17/42], Loss: 11.3579\n",
      "Epoch [33/300], Step [18/42], Loss: 10.1387\n",
      "Epoch [33/300], Step [19/42], Loss: 10.9808\n",
      "Epoch [33/300], Step [20/42], Loss: 7.0514\n",
      "Epoch [33/300], Step [21/42], Loss: 9.3091\n",
      "Epoch [33/300], Step [22/42], Loss: 8.5469\n",
      "Epoch [33/300], Step [23/42], Loss: 12.3613\n",
      "Epoch [33/300], Step [24/42], Loss: 9.4079\n",
      "Epoch [33/300], Step [25/42], Loss: 7.1591\n",
      "Epoch [33/300], Step [26/42], Loss: 9.3752\n",
      "Epoch [33/300], Step [27/42], Loss: 9.0150\n",
      "Epoch [33/300], Step [28/42], Loss: 13.2521\n",
      "Epoch [33/300], Step [29/42], Loss: 8.9633\n",
      "Epoch [33/300], Step [30/42], Loss: 11.1233\n",
      "Epoch [33/300], Step [31/42], Loss: 10.1239\n",
      "Epoch [33/300], Step [32/42], Loss: 9.7097\n",
      "Epoch [33/300], Step [33/42], Loss: 11.6623\n",
      "Epoch [33/300], Step [34/42], Loss: 10.1754\n",
      "Epoch [33/300], Step [35/42], Loss: 9.7406\n",
      "Epoch [33/300], Step [36/42], Loss: 7.8234\n",
      "Epoch [33/300], Step [37/42], Loss: 10.2356\n",
      "Epoch [33/300], Step [38/42], Loss: 14.5192\n",
      "Epoch [33/300], Step [39/42], Loss: 8.5268\n",
      "Epoch [33/300], Step [40/42], Loss: 6.0465\n",
      "Epoch [33/300], Step [41/42], Loss: 8.8901\n",
      "Epoch [33/300], Step [42/42], Loss: 1.8082\n",
      "Val. loss :9.1371\n",
      "Epoch [34/300], Step [1/42], Loss: 9.3408\n",
      "Epoch [34/300], Step [2/42], Loss: 10.3140\n",
      "Epoch [34/300], Step [3/42], Loss: 10.3148\n",
      "Epoch [34/300], Step [4/42], Loss: 9.1082\n",
      "Epoch [34/300], Step [5/42], Loss: 7.7861\n",
      "Epoch [34/300], Step [6/42], Loss: 8.3022\n",
      "Epoch [34/300], Step [7/42], Loss: 9.7545\n",
      "Epoch [34/300], Step [8/42], Loss: 8.1667\n",
      "Epoch [34/300], Step [9/42], Loss: 8.9291\n",
      "Epoch [34/300], Step [10/42], Loss: 8.5160\n",
      "Epoch [34/300], Step [11/42], Loss: 5.2402\n",
      "Epoch [34/300], Step [12/42], Loss: 9.3724\n",
      "Epoch [34/300], Step [13/42], Loss: 7.3991\n",
      "Epoch [34/300], Step [14/42], Loss: 7.4903\n",
      "Epoch [34/300], Step [15/42], Loss: 5.7197\n",
      "Epoch [34/300], Step [16/42], Loss: 10.3734\n",
      "Epoch [34/300], Step [17/42], Loss: 10.5805\n",
      "Epoch [34/300], Step [18/42], Loss: 7.0858\n",
      "Epoch [34/300], Step [19/42], Loss: 9.3365\n",
      "Epoch [34/300], Step [20/42], Loss: 7.2827\n",
      "Epoch [34/300], Step [21/42], Loss: 5.2773\n",
      "Epoch [34/300], Step [22/42], Loss: 7.4260\n",
      "Epoch [34/300], Step [23/42], Loss: 8.4649\n",
      "Epoch [34/300], Step [24/42], Loss: 9.6232\n",
      "Epoch [34/300], Step [25/42], Loss: 6.8543\n",
      "Epoch [34/300], Step [26/42], Loss: 7.6397\n",
      "Epoch [34/300], Step [27/42], Loss: 4.6154\n",
      "Epoch [34/300], Step [28/42], Loss: 10.3646\n",
      "Epoch [34/300], Step [29/42], Loss: 11.3609\n",
      "Epoch [34/300], Step [30/42], Loss: 7.4194\n",
      "Epoch [34/300], Step [31/42], Loss: 11.3135\n",
      "Epoch [34/300], Step [32/42], Loss: 5.8720\n",
      "Epoch [34/300], Step [33/42], Loss: 6.2000\n",
      "Epoch [34/300], Step [34/42], Loss: 7.6193\n",
      "Epoch [34/300], Step [35/42], Loss: 9.4273\n",
      "Epoch [34/300], Step [36/42], Loss: 12.9122\n",
      "Epoch [34/300], Step [37/42], Loss: 9.0674\n",
      "Epoch [34/300], Step [38/42], Loss: 9.6198\n",
      "Epoch [34/300], Step [39/42], Loss: 5.9320\n",
      "Epoch [34/300], Step [40/42], Loss: 9.3336\n",
      "Epoch [34/300], Step [41/42], Loss: 6.3875\n",
      "Epoch [34/300], Step [42/42], Loss: 0.9738\n",
      "Val. loss :8.7127\n",
      "Epoch [35/300], Step [1/42], Loss: 10.0135\n",
      "Epoch [35/300], Step [2/42], Loss: 6.5783\n",
      "Epoch [35/300], Step [3/42], Loss: 8.7762\n",
      "Epoch [35/300], Step [4/42], Loss: 6.6341\n",
      "Epoch [35/300], Step [5/42], Loss: 5.7501\n",
      "Epoch [35/300], Step [6/42], Loss: 9.3651\n",
      "Epoch [35/300], Step [7/42], Loss: 6.0859\n",
      "Epoch [35/300], Step [8/42], Loss: 9.8081\n",
      "Epoch [35/300], Step [9/42], Loss: 7.0662\n",
      "Epoch [35/300], Step [10/42], Loss: 8.3837\n",
      "Epoch [35/300], Step [11/42], Loss: 4.8474\n",
      "Epoch [35/300], Step [12/42], Loss: 9.9171\n",
      "Epoch [35/300], Step [13/42], Loss: 7.2297\n",
      "Epoch [35/300], Step [14/42], Loss: 6.3005\n",
      "Epoch [35/300], Step [15/42], Loss: 6.3669\n",
      "Epoch [35/300], Step [16/42], Loss: 5.2682\n",
      "Epoch [35/300], Step [17/42], Loss: 12.4747\n",
      "Epoch [35/300], Step [18/42], Loss: 5.1606\n",
      "Epoch [35/300], Step [19/42], Loss: 7.2989\n",
      "Epoch [35/300], Step [20/42], Loss: 6.8665\n",
      "Epoch [35/300], Step [21/42], Loss: 6.6225\n",
      "Epoch [35/300], Step [22/42], Loss: 7.0727\n",
      "Epoch [35/300], Step [23/42], Loss: 7.6282\n",
      "Epoch [35/300], Step [24/42], Loss: 8.8593\n",
      "Epoch [35/300], Step [25/42], Loss: 7.7935\n",
      "Epoch [35/300], Step [26/42], Loss: 9.3186\n",
      "Epoch [35/300], Step [27/42], Loss: 5.7148\n",
      "Epoch [35/300], Step [28/42], Loss: 10.2665\n",
      "Epoch [35/300], Step [29/42], Loss: 8.8076\n",
      "Epoch [35/300], Step [30/42], Loss: 13.5246\n",
      "Epoch [35/300], Step [31/42], Loss: 9.5234\n",
      "Epoch [35/300], Step [32/42], Loss: 6.9875\n",
      "Epoch [35/300], Step [33/42], Loss: 7.3300\n",
      "Epoch [35/300], Step [34/42], Loss: 14.7567\n",
      "Epoch [35/300], Step [35/42], Loss: 7.7689\n",
      "Epoch [35/300], Step [36/42], Loss: 7.5073\n",
      "Epoch [35/300], Step [37/42], Loss: 7.4788\n",
      "Epoch [35/300], Step [38/42], Loss: 6.8363\n",
      "Epoch [35/300], Step [39/42], Loss: 8.8417\n",
      "Epoch [35/300], Step [40/42], Loss: 8.0758\n",
      "Epoch [35/300], Step [41/42], Loss: 7.5764\n",
      "Epoch [35/300], Step [42/42], Loss: 4.2441\n",
      "Val. loss :7.9693\n",
      "Epoch [36/300], Step [1/42], Loss: 7.2852\n",
      "Epoch [36/300], Step [2/42], Loss: 7.5931\n",
      "Epoch [36/300], Step [3/42], Loss: 5.7163\n",
      "Epoch [36/300], Step [4/42], Loss: 7.9692\n",
      "Epoch [36/300], Step [5/42], Loss: 6.6250\n",
      "Epoch [36/300], Step [6/42], Loss: 9.2386\n",
      "Epoch [36/300], Step [7/42], Loss: 6.0698\n",
      "Epoch [36/300], Step [8/42], Loss: 6.8069\n",
      "Epoch [36/300], Step [9/42], Loss: 8.1079\n",
      "Epoch [36/300], Step [10/42], Loss: 8.1889\n",
      "Epoch [36/300], Step [11/42], Loss: 4.9878\n",
      "Epoch [36/300], Step [12/42], Loss: 5.2789\n",
      "Epoch [36/300], Step [13/42], Loss: 4.9553\n",
      "Epoch [36/300], Step [14/42], Loss: 6.8798\n",
      "Epoch [36/300], Step [15/42], Loss: 8.0747\n",
      "Epoch [36/300], Step [16/42], Loss: 8.4139\n",
      "Epoch [36/300], Step [17/42], Loss: 4.2564\n",
      "Epoch [36/300], Step [18/42], Loss: 5.4794\n",
      "Epoch [36/300], Step [19/42], Loss: 7.6038\n",
      "Epoch [36/300], Step [20/42], Loss: 7.3935\n",
      "Epoch [36/300], Step [21/42], Loss: 5.0787\n",
      "Epoch [36/300], Step [22/42], Loss: 8.0713\n",
      "Epoch [36/300], Step [23/42], Loss: 6.1834\n",
      "Epoch [36/300], Step [24/42], Loss: 7.5881\n",
      "Epoch [36/300], Step [25/42], Loss: 4.3422\n",
      "Epoch [36/300], Step [26/42], Loss: 8.7514\n",
      "Epoch [36/300], Step [27/42], Loss: 9.4961\n",
      "Epoch [36/300], Step [28/42], Loss: 7.1144\n",
      "Epoch [36/300], Step [29/42], Loss: 6.7557\n",
      "Epoch [36/300], Step [30/42], Loss: 9.9119\n",
      "Epoch [36/300], Step [31/42], Loss: 8.8039\n",
      "Epoch [36/300], Step [32/42], Loss: 6.9058\n",
      "Epoch [36/300], Step [33/42], Loss: 7.4953\n",
      "Epoch [36/300], Step [34/42], Loss: 6.0515\n",
      "Epoch [36/300], Step [35/42], Loss: 6.2076\n",
      "Epoch [36/300], Step [36/42], Loss: 6.7194\n",
      "Epoch [36/300], Step [37/42], Loss: 6.6609\n",
      "Epoch [36/300], Step [38/42], Loss: 8.3298\n",
      "Epoch [36/300], Step [39/42], Loss: 7.7060\n",
      "Epoch [36/300], Step [40/42], Loss: 6.9603\n",
      "Epoch [36/300], Step [41/42], Loss: 8.8572\n",
      "Epoch [36/300], Step [42/42], Loss: 3.4780\n",
      "Val. loss :7.7329\n",
      "Epoch [37/300], Step [1/42], Loss: 5.1857\n",
      "Epoch [37/300], Step [2/42], Loss: 5.8529\n",
      "Epoch [37/300], Step [3/42], Loss: 6.2607\n",
      "Epoch [37/300], Step [4/42], Loss: 7.9960\n",
      "Epoch [37/300], Step [5/42], Loss: 6.5911\n",
      "Epoch [37/300], Step [6/42], Loss: 7.9397\n",
      "Epoch [37/300], Step [7/42], Loss: 8.4688\n",
      "Epoch [37/300], Step [8/42], Loss: 5.4030\n",
      "Epoch [37/300], Step [9/42], Loss: 6.0535\n",
      "Epoch [37/300], Step [10/42], Loss: 4.9817\n",
      "Epoch [37/300], Step [11/42], Loss: 8.3840\n",
      "Epoch [37/300], Step [12/42], Loss: 8.2788\n",
      "Epoch [37/300], Step [13/42], Loss: 5.4650\n",
      "Epoch [37/300], Step [14/42], Loss: 8.3330\n",
      "Epoch [37/300], Step [15/42], Loss: 7.8470\n",
      "Epoch [37/300], Step [16/42], Loss: 6.5204\n",
      "Epoch [37/300], Step [17/42], Loss: 5.9249\n",
      "Epoch [37/300], Step [18/42], Loss: 6.2497\n",
      "Epoch [37/300], Step [19/42], Loss: 4.1863\n",
      "Epoch [37/300], Step [20/42], Loss: 6.7789\n",
      "Epoch [37/300], Step [21/42], Loss: 7.0819\n",
      "Epoch [37/300], Step [22/42], Loss: 8.2320\n",
      "Epoch [37/300], Step [23/42], Loss: 5.8331\n",
      "Epoch [37/300], Step [24/42], Loss: 12.9124\n",
      "Epoch [37/300], Step [25/42], Loss: 7.0598\n",
      "Epoch [37/300], Step [26/42], Loss: 8.4784\n",
      "Epoch [37/300], Step [27/42], Loss: 6.8928\n",
      "Epoch [37/300], Step [28/42], Loss: 6.6008\n",
      "Epoch [37/300], Step [29/42], Loss: 12.0083\n",
      "Epoch [37/300], Step [30/42], Loss: 5.4516\n",
      "Epoch [37/300], Step [31/42], Loss: 10.8686\n",
      "Epoch [37/300], Step [32/42], Loss: 7.5180\n",
      "Epoch [37/300], Step [33/42], Loss: 6.4716\n",
      "Epoch [37/300], Step [34/42], Loss: 6.3158\n",
      "Epoch [37/300], Step [35/42], Loss: 5.0178\n",
      "Epoch [37/300], Step [36/42], Loss: 7.5670\n",
      "Epoch [37/300], Step [37/42], Loss: 5.0162\n",
      "Epoch [37/300], Step [38/42], Loss: 7.6509\n",
      "Epoch [37/300], Step [39/42], Loss: 4.7274\n",
      "Epoch [37/300], Step [40/42], Loss: 5.3407\n",
      "Epoch [37/300], Step [41/42], Loss: 7.9551\n",
      "Epoch [37/300], Step [42/42], Loss: 1.6388\n",
      "Val. loss :6.8178\n",
      "Epoch [38/300], Step [1/42], Loss: 6.1527\n",
      "Epoch [38/300], Step [2/42], Loss: 7.5409\n",
      "Epoch [38/300], Step [3/42], Loss: 5.6680\n",
      "Epoch [38/300], Step [4/42], Loss: 6.8062\n",
      "Epoch [38/300], Step [5/42], Loss: 4.4577\n",
      "Epoch [38/300], Step [6/42], Loss: 4.4008\n",
      "Epoch [38/300], Step [7/42], Loss: 5.9219\n",
      "Epoch [38/300], Step [8/42], Loss: 6.2229\n",
      "Epoch [38/300], Step [9/42], Loss: 8.2668\n",
      "Epoch [38/300], Step [10/42], Loss: 5.9073\n",
      "Epoch [38/300], Step [11/42], Loss: 6.8088\n",
      "Epoch [38/300], Step [12/42], Loss: 6.9290\n",
      "Epoch [38/300], Step [13/42], Loss: 4.5541\n",
      "Epoch [38/300], Step [14/42], Loss: 8.4905\n",
      "Epoch [38/300], Step [15/42], Loss: 7.9545\n",
      "Epoch [38/300], Step [16/42], Loss: 5.1072\n",
      "Epoch [38/300], Step [17/42], Loss: 6.2771\n",
      "Epoch [38/300], Step [18/42], Loss: 7.2441\n",
      "Epoch [38/300], Step [19/42], Loss: 6.9192\n",
      "Epoch [38/300], Step [20/42], Loss: 5.9970\n",
      "Epoch [38/300], Step [21/42], Loss: 6.5762\n",
      "Epoch [38/300], Step [22/42], Loss: 4.9246\n",
      "Epoch [38/300], Step [23/42], Loss: 7.9381\n",
      "Epoch [38/300], Step [24/42], Loss: 7.7801\n",
      "Epoch [38/300], Step [25/42], Loss: 6.5159\n",
      "Epoch [38/300], Step [26/42], Loss: 6.7282\n",
      "Epoch [38/300], Step [27/42], Loss: 3.5451\n",
      "Epoch [38/300], Step [28/42], Loss: 6.7901\n",
      "Epoch [38/300], Step [29/42], Loss: 6.7414\n",
      "Epoch [38/300], Step [30/42], Loss: 8.7860\n",
      "Epoch [38/300], Step [31/42], Loss: 6.1955\n",
      "Epoch [38/300], Step [32/42], Loss: 10.1986\n",
      "Epoch [38/300], Step [33/42], Loss: 5.4065\n",
      "Epoch [38/300], Step [34/42], Loss: 5.8503\n",
      "Epoch [38/300], Step [35/42], Loss: 8.5046\n",
      "Epoch [38/300], Step [36/42], Loss: 5.0696\n",
      "Epoch [38/300], Step [37/42], Loss: 5.2638\n",
      "Epoch [38/300], Step [38/42], Loss: 7.5252\n",
      "Epoch [38/300], Step [39/42], Loss: 8.1487\n",
      "Epoch [38/300], Step [40/42], Loss: 5.9655\n",
      "Epoch [38/300], Step [41/42], Loss: 5.3162\n",
      "Epoch [38/300], Step [42/42], Loss: 2.6179\n",
      "Val. loss :6.7643\n",
      "Epoch [39/300], Step [1/42], Loss: 7.2034\n",
      "Epoch [39/300], Step [2/42], Loss: 8.4086\n",
      "Epoch [39/300], Step [3/42], Loss: 9.0409\n",
      "Epoch [39/300], Step [4/42], Loss: 4.2404\n",
      "Epoch [39/300], Step [5/42], Loss: 9.0857\n",
      "Epoch [39/300], Step [6/42], Loss: 9.9888\n",
      "Epoch [39/300], Step [7/42], Loss: 7.8713\n",
      "Epoch [39/300], Step [8/42], Loss: 6.7221\n",
      "Epoch [39/300], Step [9/42], Loss: 9.3261\n",
      "Epoch [39/300], Step [10/42], Loss: 8.6402\n",
      "Epoch [39/300], Step [11/42], Loss: 4.6652\n",
      "Epoch [39/300], Step [12/42], Loss: 5.7240\n",
      "Epoch [39/300], Step [13/42], Loss: 4.5990\n",
      "Epoch [39/300], Step [14/42], Loss: 7.1867\n",
      "Epoch [39/300], Step [15/42], Loss: 5.2880\n",
      "Epoch [39/300], Step [16/42], Loss: 3.7799\n",
      "Epoch [39/300], Step [17/42], Loss: 8.0076\n",
      "Epoch [39/300], Step [18/42], Loss: 7.6545\n",
      "Epoch [39/300], Step [19/42], Loss: 5.7084\n",
      "Epoch [39/300], Step [20/42], Loss: 4.7192\n",
      "Epoch [39/300], Step [21/42], Loss: 8.8616\n",
      "Epoch [39/300], Step [22/42], Loss: 4.2987\n",
      "Epoch [39/300], Step [23/42], Loss: 5.7818\n",
      "Epoch [39/300], Step [24/42], Loss: 5.2864\n",
      "Epoch [39/300], Step [25/42], Loss: 5.6419\n",
      "Epoch [39/300], Step [26/42], Loss: 8.1056\n",
      "Epoch [39/300], Step [27/42], Loss: 9.5578\n",
      "Epoch [39/300], Step [28/42], Loss: 3.1609\n",
      "Epoch [39/300], Step [29/42], Loss: 6.3307\n",
      "Epoch [39/300], Step [30/42], Loss: 4.5353\n",
      "Epoch [39/300], Step [31/42], Loss: 5.0168\n",
      "Epoch [39/300], Step [32/42], Loss: 5.9887\n",
      "Epoch [39/300], Step [33/42], Loss: 3.4857\n",
      "Epoch [39/300], Step [34/42], Loss: 5.6355\n",
      "Epoch [39/300], Step [35/42], Loss: 6.3571\n",
      "Epoch [39/300], Step [36/42], Loss: 4.9244\n",
      "Epoch [39/300], Step [37/42], Loss: 8.8162\n",
      "Epoch [39/300], Step [38/42], Loss: 4.3282\n",
      "Epoch [39/300], Step [39/42], Loss: 4.7447\n",
      "Epoch [39/300], Step [40/42], Loss: 4.2127\n",
      "Epoch [39/300], Step [41/42], Loss: 11.5293\n",
      "Epoch [39/300], Step [42/42], Loss: 1.4654\n",
      "Val. loss :6.5637\n",
      "Epoch [40/300], Step [1/42], Loss: 4.0691\n",
      "Epoch [40/300], Step [2/42], Loss: 3.8782\n",
      "Epoch [40/300], Step [3/42], Loss: 7.6249\n",
      "Epoch [40/300], Step [4/42], Loss: 6.3280\n",
      "Epoch [40/300], Step [5/42], Loss: 5.4471\n",
      "Epoch [40/300], Step [6/42], Loss: 5.6873\n",
      "Epoch [40/300], Step [7/42], Loss: 6.0614\n",
      "Epoch [40/300], Step [8/42], Loss: 4.4478\n",
      "Epoch [40/300], Step [9/42], Loss: 5.2739\n",
      "Epoch [40/300], Step [10/42], Loss: 4.1764\n",
      "Epoch [40/300], Step [11/42], Loss: 6.4701\n",
      "Epoch [40/300], Step [12/42], Loss: 5.2363\n",
      "Epoch [40/300], Step [13/42], Loss: 4.5086\n",
      "Epoch [40/300], Step [14/42], Loss: 9.8770\n",
      "Epoch [40/300], Step [15/42], Loss: 7.1242\n",
      "Epoch [40/300], Step [16/42], Loss: 4.8468\n",
      "Epoch [40/300], Step [17/42], Loss: 4.4369\n",
      "Epoch [40/300], Step [18/42], Loss: 3.9583\n",
      "Epoch [40/300], Step [19/42], Loss: 4.8255\n",
      "Epoch [40/300], Step [20/42], Loss: 4.8063\n",
      "Epoch [40/300], Step [21/42], Loss: 6.0987\n",
      "Epoch [40/300], Step [22/42], Loss: 4.5511\n",
      "Epoch [40/300], Step [23/42], Loss: 7.6986\n",
      "Epoch [40/300], Step [24/42], Loss: 5.3280\n",
      "Epoch [40/300], Step [25/42], Loss: 5.6814\n",
      "Epoch [40/300], Step [26/42], Loss: 7.7273\n",
      "Epoch [40/300], Step [27/42], Loss: 3.3929\n",
      "Epoch [40/300], Step [28/42], Loss: 3.1514\n",
      "Epoch [40/300], Step [29/42], Loss: 3.6505\n",
      "Epoch [40/300], Step [30/42], Loss: 6.7172\n",
      "Epoch [40/300], Step [31/42], Loss: 8.0126\n",
      "Epoch [40/300], Step [32/42], Loss: 6.8004\n",
      "Epoch [40/300], Step [33/42], Loss: 7.8562\n",
      "Epoch [40/300], Step [34/42], Loss: 3.6387\n",
      "Epoch [40/300], Step [35/42], Loss: 6.9460\n",
      "Epoch [40/300], Step [36/42], Loss: 5.1451\n",
      "Epoch [40/300], Step [37/42], Loss: 7.9034\n",
      "Epoch [40/300], Step [38/42], Loss: 3.9660\n",
      "Epoch [40/300], Step [39/42], Loss: 6.3632\n",
      "Epoch [40/300], Step [40/42], Loss: 4.4747\n",
      "Epoch [40/300], Step [41/42], Loss: 5.3957\n",
      "Epoch [40/300], Step [42/42], Loss: 0.9946\n",
      "Val. loss :6.7728\n",
      "Epoch [41/300], Step [1/42], Loss: 4.1754\n",
      "Epoch [41/300], Step [2/42], Loss: 6.2483\n",
      "Epoch [41/300], Step [3/42], Loss: 5.9603\n",
      "Epoch [41/300], Step [4/42], Loss: 7.3005\n",
      "Epoch [41/300], Step [5/42], Loss: 4.8619\n",
      "Epoch [41/300], Step [6/42], Loss: 7.4362\n",
      "Epoch [41/300], Step [7/42], Loss: 6.0745\n",
      "Epoch [41/300], Step [8/42], Loss: 6.6462\n",
      "Epoch [41/300], Step [9/42], Loss: 6.5929\n",
      "Epoch [41/300], Step [10/42], Loss: 4.3484\n",
      "Epoch [41/300], Step [11/42], Loss: 5.5384\n",
      "Epoch [41/300], Step [12/42], Loss: 3.5760\n",
      "Epoch [41/300], Step [13/42], Loss: 5.3527\n",
      "Epoch [41/300], Step [14/42], Loss: 5.4742\n",
      "Epoch [41/300], Step [15/42], Loss: 6.8915\n",
      "Epoch [41/300], Step [16/42], Loss: 4.8196\n",
      "Epoch [41/300], Step [17/42], Loss: 8.6789\n",
      "Epoch [41/300], Step [18/42], Loss: 6.5808\n",
      "Epoch [41/300], Step [19/42], Loss: 7.3661\n",
      "Epoch [41/300], Step [20/42], Loss: 3.3511\n",
      "Epoch [41/300], Step [21/42], Loss: 6.9491\n",
      "Epoch [41/300], Step [22/42], Loss: 11.6721\n",
      "Epoch [41/300], Step [23/42], Loss: 5.7944\n",
      "Epoch [41/300], Step [24/42], Loss: 3.6035\n",
      "Epoch [41/300], Step [25/42], Loss: 7.0368\n",
      "Epoch [41/300], Step [26/42], Loss: 7.0000\n",
      "Epoch [41/300], Step [27/42], Loss: 4.1000\n",
      "Epoch [41/300], Step [28/42], Loss: 3.9335\n",
      "Epoch [41/300], Step [29/42], Loss: 6.0354\n",
      "Epoch [41/300], Step [30/42], Loss: 5.4933\n",
      "Epoch [41/300], Step [31/42], Loss: 5.7933\n",
      "Epoch [41/300], Step [32/42], Loss: 4.9123\n",
      "Epoch [41/300], Step [33/42], Loss: 3.9364\n",
      "Epoch [41/300], Step [34/42], Loss: 5.2650\n",
      "Epoch [41/300], Step [35/42], Loss: 5.5580\n",
      "Epoch [41/300], Step [36/42], Loss: 5.2581\n",
      "Epoch [41/300], Step [37/42], Loss: 3.9150\n",
      "Epoch [41/300], Step [38/42], Loss: 3.2446\n",
      "Epoch [41/300], Step [39/42], Loss: 2.9165\n",
      "Epoch [41/300], Step [40/42], Loss: 6.0321\n",
      "Epoch [41/300], Step [41/42], Loss: 6.5108\n",
      "Epoch [41/300], Step [42/42], Loss: 2.6261\n",
      "Val. loss :5.8474\n",
      "Epoch [42/300], Step [1/42], Loss: 4.6374\n",
      "Epoch [42/300], Step [2/42], Loss: 3.6247\n",
      "Epoch [42/300], Step [3/42], Loss: 6.0562\n",
      "Epoch [42/300], Step [4/42], Loss: 5.6703\n",
      "Epoch [42/300], Step [5/42], Loss: 3.1121\n",
      "Epoch [42/300], Step [6/42], Loss: 4.5683\n",
      "Epoch [42/300], Step [7/42], Loss: 3.6868\n",
      "Epoch [42/300], Step [8/42], Loss: 9.0482\n",
      "Epoch [42/300], Step [9/42], Loss: 4.8566\n",
      "Epoch [42/300], Step [10/42], Loss: 3.2697\n",
      "Epoch [42/300], Step [11/42], Loss: 5.6683\n",
      "Epoch [42/300], Step [12/42], Loss: 4.1685\n",
      "Epoch [42/300], Step [13/42], Loss: 3.5517\n",
      "Epoch [42/300], Step [14/42], Loss: 4.3658\n",
      "Epoch [42/300], Step [15/42], Loss: 3.0828\n",
      "Epoch [42/300], Step [16/42], Loss: 4.3692\n",
      "Epoch [42/300], Step [17/42], Loss: 6.9745\n",
      "Epoch [42/300], Step [18/42], Loss: 6.2645\n",
      "Epoch [42/300], Step [19/42], Loss: 3.6373\n",
      "Epoch [42/300], Step [20/42], Loss: 3.8655\n",
      "Epoch [42/300], Step [21/42], Loss: 3.8781\n",
      "Epoch [42/300], Step [22/42], Loss: 5.5907\n",
      "Epoch [42/300], Step [23/42], Loss: 4.8298\n",
      "Epoch [42/300], Step [24/42], Loss: 4.7672\n",
      "Epoch [42/300], Step [25/42], Loss: 3.7543\n",
      "Epoch [42/300], Step [26/42], Loss: 2.8934\n",
      "Epoch [42/300], Step [27/42], Loss: 4.7267\n",
      "Epoch [42/300], Step [28/42], Loss: 4.7056\n",
      "Epoch [42/300], Step [29/42], Loss: 6.1023\n",
      "Epoch [42/300], Step [30/42], Loss: 6.0506\n",
      "Epoch [42/300], Step [31/42], Loss: 4.5380\n",
      "Epoch [42/300], Step [32/42], Loss: 6.7821\n",
      "Epoch [42/300], Step [33/42], Loss: 5.8065\n",
      "Epoch [42/300], Step [34/42], Loss: 4.0307\n",
      "Epoch [42/300], Step [35/42], Loss: 4.7445\n",
      "Epoch [42/300], Step [36/42], Loss: 3.8135\n",
      "Epoch [42/300], Step [37/42], Loss: 3.9383\n",
      "Epoch [42/300], Step [38/42], Loss: 4.4720\n",
      "Epoch [42/300], Step [39/42], Loss: 5.5999\n",
      "Epoch [42/300], Step [40/42], Loss: 7.0541\n",
      "Epoch [42/300], Step [41/42], Loss: 8.0524\n",
      "Epoch [42/300], Step [42/42], Loss: 2.1094\n",
      "Val. loss :5.2012\n",
      "Epoch [43/300], Step [1/42], Loss: 2.8565\n",
      "Epoch [43/300], Step [2/42], Loss: 7.0388\n",
      "Epoch [43/300], Step [3/42], Loss: 5.5035\n",
      "Epoch [43/300], Step [4/42], Loss: 2.4595\n",
      "Epoch [43/300], Step [5/42], Loss: 4.4004\n",
      "Epoch [43/300], Step [6/42], Loss: 3.4293\n",
      "Epoch [43/300], Step [7/42], Loss: 4.8607\n",
      "Epoch [43/300], Step [8/42], Loss: 4.3782\n",
      "Epoch [43/300], Step [9/42], Loss: 3.5664\n",
      "Epoch [43/300], Step [10/42], Loss: 7.6203\n",
      "Epoch [43/300], Step [11/42], Loss: 3.6864\n",
      "Epoch [43/300], Step [12/42], Loss: 9.4665\n",
      "Epoch [43/300], Step [13/42], Loss: 5.7021\n",
      "Epoch [43/300], Step [14/42], Loss: 5.5704\n",
      "Epoch [43/300], Step [15/42], Loss: 3.7263\n",
      "Epoch [43/300], Step [16/42], Loss: 4.6931\n",
      "Epoch [43/300], Step [17/42], Loss: 3.7982\n",
      "Epoch [43/300], Step [18/42], Loss: 6.9501\n",
      "Epoch [43/300], Step [19/42], Loss: 4.5336\n",
      "Epoch [43/300], Step [20/42], Loss: 2.9132\n",
      "Epoch [43/300], Step [21/42], Loss: 3.7227\n",
      "Epoch [43/300], Step [22/42], Loss: 4.1710\n",
      "Epoch [43/300], Step [23/42], Loss: 5.5912\n",
      "Epoch [43/300], Step [24/42], Loss: 3.7222\n",
      "Epoch [43/300], Step [25/42], Loss: 3.9037\n",
      "Epoch [43/300], Step [26/42], Loss: 3.5690\n",
      "Epoch [43/300], Step [27/42], Loss: 5.9130\n",
      "Epoch [43/300], Step [28/42], Loss: 5.4598\n",
      "Epoch [43/300], Step [29/42], Loss: 4.1969\n",
      "Epoch [43/300], Step [30/42], Loss: 3.7597\n",
      "Epoch [43/300], Step [31/42], Loss: 5.9051\n",
      "Epoch [43/300], Step [32/42], Loss: 3.3806\n",
      "Epoch [43/300], Step [33/42], Loss: 2.8883\n",
      "Epoch [43/300], Step [34/42], Loss: 4.1647\n",
      "Epoch [43/300], Step [35/42], Loss: 3.7069\n",
      "Epoch [43/300], Step [36/42], Loss: 5.9975\n",
      "Epoch [43/300], Step [37/42], Loss: 3.1813\n",
      "Epoch [43/300], Step [38/42], Loss: 3.6817\n",
      "Epoch [43/300], Step [39/42], Loss: 5.6672\n",
      "Epoch [43/300], Step [40/42], Loss: 3.5575\n",
      "Epoch [43/300], Step [41/42], Loss: 3.5320\n",
      "Epoch [43/300], Step [42/42], Loss: 1.0393\n",
      "Val. loss :4.8463\n",
      "Epoch [44/300], Step [1/42], Loss: 5.0355\n",
      "Epoch [44/300], Step [2/42], Loss: 4.0500\n",
      "Epoch [44/300], Step [3/42], Loss: 4.8357\n",
      "Epoch [44/300], Step [4/42], Loss: 3.1660\n",
      "Epoch [44/300], Step [5/42], Loss: 5.2986\n",
      "Epoch [44/300], Step [6/42], Loss: 4.1919\n",
      "Epoch [44/300], Step [7/42], Loss: 2.7813\n",
      "Epoch [44/300], Step [8/42], Loss: 2.8261\n",
      "Epoch [44/300], Step [9/42], Loss: 3.0185\n",
      "Epoch [44/300], Step [10/42], Loss: 4.5108\n",
      "Epoch [44/300], Step [11/42], Loss: 5.9576\n",
      "Epoch [44/300], Step [12/42], Loss: 2.9072\n",
      "Epoch [44/300], Step [13/42], Loss: 4.0043\n",
      "Epoch [44/300], Step [14/42], Loss: 3.9008\n",
      "Epoch [44/300], Step [15/42], Loss: 5.1510\n",
      "Epoch [44/300], Step [16/42], Loss: 3.4228\n",
      "Epoch [44/300], Step [17/42], Loss: 4.7973\n",
      "Epoch [44/300], Step [18/42], Loss: 4.9008\n",
      "Epoch [44/300], Step [19/42], Loss: 4.0339\n",
      "Epoch [44/300], Step [20/42], Loss: 4.4064\n",
      "Epoch [44/300], Step [21/42], Loss: 6.3975\n",
      "Epoch [44/300], Step [22/42], Loss: 3.6068\n",
      "Epoch [44/300], Step [23/42], Loss: 3.3977\n",
      "Epoch [44/300], Step [24/42], Loss: 5.2517\n",
      "Epoch [44/300], Step [25/42], Loss: 2.3109\n",
      "Epoch [44/300], Step [26/42], Loss: 4.5277\n",
      "Epoch [44/300], Step [27/42], Loss: 5.0944\n",
      "Epoch [44/300], Step [28/42], Loss: 5.2349\n",
      "Epoch [44/300], Step [29/42], Loss: 6.3311\n",
      "Epoch [44/300], Step [30/42], Loss: 5.5324\n",
      "Epoch [44/300], Step [31/42], Loss: 5.7496\n",
      "Epoch [44/300], Step [32/42], Loss: 11.8073\n",
      "Epoch [44/300], Step [33/42], Loss: 2.8809\n",
      "Epoch [44/300], Step [34/42], Loss: 2.4387\n",
      "Epoch [44/300], Step [35/42], Loss: 5.3065\n",
      "Epoch [44/300], Step [36/42], Loss: 4.9558\n",
      "Epoch [44/300], Step [37/42], Loss: 4.5682\n",
      "Epoch [44/300], Step [38/42], Loss: 4.3754\n",
      "Epoch [44/300], Step [39/42], Loss: 6.1170\n",
      "Epoch [44/300], Step [40/42], Loss: 3.7387\n",
      "Epoch [44/300], Step [41/42], Loss: 3.5883\n",
      "Epoch [44/300], Step [42/42], Loss: 1.8544\n",
      "Val. loss :4.9275\n",
      "Epoch [45/300], Step [1/42], Loss: 2.5581\n",
      "Epoch [45/300], Step [2/42], Loss: 3.2444\n",
      "Epoch [45/300], Step [3/42], Loss: 5.1651\n",
      "Epoch [45/300], Step [4/42], Loss: 6.3698\n",
      "Epoch [45/300], Step [5/42], Loss: 3.9882\n",
      "Epoch [45/300], Step [6/42], Loss: 6.5743\n",
      "Epoch [45/300], Step [7/42], Loss: 4.2332\n",
      "Epoch [45/300], Step [8/42], Loss: 4.3608\n",
      "Epoch [45/300], Step [9/42], Loss: 4.4506\n",
      "Epoch [45/300], Step [10/42], Loss: 4.1229\n",
      "Epoch [45/300], Step [11/42], Loss: 3.2072\n",
      "Epoch [45/300], Step [12/42], Loss: 3.1638\n",
      "Epoch [45/300], Step [13/42], Loss: 3.5158\n",
      "Epoch [45/300], Step [14/42], Loss: 3.3287\n",
      "Epoch [45/300], Step [15/42], Loss: 3.6808\n",
      "Epoch [45/300], Step [16/42], Loss: 5.0311\n",
      "Epoch [45/300], Step [17/42], Loss: 5.0789\n",
      "Epoch [45/300], Step [18/42], Loss: 8.0436\n",
      "Epoch [45/300], Step [19/42], Loss: 5.7062\n",
      "Epoch [45/300], Step [20/42], Loss: 3.2282\n",
      "Epoch [45/300], Step [21/42], Loss: 6.6805\n",
      "Epoch [45/300], Step [22/42], Loss: 2.8636\n",
      "Epoch [45/300], Step [23/42], Loss: 2.8362\n",
      "Epoch [45/300], Step [24/42], Loss: 3.2263\n",
      "Epoch [45/300], Step [25/42], Loss: 4.5872\n",
      "Epoch [45/300], Step [26/42], Loss: 2.9119\n",
      "Epoch [45/300], Step [27/42], Loss: 4.1382\n",
      "Epoch [45/300], Step [28/42], Loss: 5.1732\n",
      "Epoch [45/300], Step [29/42], Loss: 3.1624\n",
      "Epoch [45/300], Step [30/42], Loss: 3.1911\n",
      "Epoch [45/300], Step [31/42], Loss: 3.2416\n",
      "Epoch [45/300], Step [32/42], Loss: 5.5981\n",
      "Epoch [45/300], Step [33/42], Loss: 8.4948\n",
      "Epoch [45/300], Step [34/42], Loss: 3.9697\n",
      "Epoch [45/300], Step [35/42], Loss: 6.6098\n",
      "Epoch [45/300], Step [36/42], Loss: 7.8940\n",
      "Epoch [45/300], Step [37/42], Loss: 3.2499\n",
      "Epoch [45/300], Step [38/42], Loss: 3.6375\n",
      "Epoch [45/300], Step [39/42], Loss: 5.9139\n",
      "Epoch [45/300], Step [40/42], Loss: 13.9453\n",
      "Epoch [45/300], Step [41/42], Loss: 3.4114\n",
      "Epoch [45/300], Step [42/42], Loss: 2.1685\n",
      "Val. loss :5.5460\n",
      "Epoch [46/300], Step [1/42], Loss: 7.5128\n",
      "Epoch [46/300], Step [2/42], Loss: 11.3284\n",
      "Epoch [46/300], Step [3/42], Loss: 3.6923\n",
      "Epoch [46/300], Step [4/42], Loss: 7.2419\n",
      "Epoch [46/300], Step [5/42], Loss: 5.1473\n",
      "Epoch [46/300], Step [6/42], Loss: 7.4661\n",
      "Epoch [46/300], Step [7/42], Loss: 9.0592\n",
      "Epoch [46/300], Step [8/42], Loss: 4.0762\n",
      "Epoch [46/300], Step [9/42], Loss: 9.2054\n",
      "Epoch [46/300], Step [10/42], Loss: 3.5042\n",
      "Epoch [46/300], Step [11/42], Loss: 5.5715\n",
      "Epoch [46/300], Step [12/42], Loss: 3.1356\n",
      "Epoch [46/300], Step [13/42], Loss: 4.7593\n",
      "Epoch [46/300], Step [14/42], Loss: 3.7168\n",
      "Epoch [46/300], Step [15/42], Loss: 5.0869\n",
      "Epoch [46/300], Step [16/42], Loss: 3.8670\n",
      "Epoch [46/300], Step [17/42], Loss: 3.4591\n",
      "Epoch [46/300], Step [18/42], Loss: 2.9425\n",
      "Epoch [46/300], Step [19/42], Loss: 4.9601\n",
      "Epoch [46/300], Step [20/42], Loss: 3.0542\n",
      "Epoch [46/300], Step [21/42], Loss: 4.4263\n",
      "Epoch [46/300], Step [22/42], Loss: 4.4845\n",
      "Epoch [46/300], Step [23/42], Loss: 5.8195\n",
      "Epoch [46/300], Step [24/42], Loss: 3.0557\n",
      "Epoch [46/300], Step [25/42], Loss: 6.0738\n",
      "Epoch [46/300], Step [26/42], Loss: 2.9903\n",
      "Epoch [46/300], Step [27/42], Loss: 2.9110\n",
      "Epoch [46/300], Step [28/42], Loss: 2.9297\n",
      "Epoch [46/300], Step [29/42], Loss: 3.8382\n",
      "Epoch [46/300], Step [30/42], Loss: 4.1175\n",
      "Epoch [46/300], Step [31/42], Loss: 3.2890\n",
      "Epoch [46/300], Step [32/42], Loss: 4.7693\n",
      "Epoch [46/300], Step [33/42], Loss: 2.4691\n",
      "Epoch [46/300], Step [34/42], Loss: 3.0029\n",
      "Epoch [46/300], Step [35/42], Loss: 3.5859\n",
      "Epoch [46/300], Step [36/42], Loss: 2.9479\n",
      "Epoch [46/300], Step [37/42], Loss: 2.7149\n",
      "Epoch [46/300], Step [38/42], Loss: 12.1929\n",
      "Epoch [46/300], Step [39/42], Loss: 3.0983\n",
      "Epoch [46/300], Step [40/42], Loss: 2.8770\n",
      "Epoch [46/300], Step [41/42], Loss: 4.2610\n",
      "Epoch [46/300], Step [42/42], Loss: 0.8403\n",
      "Val. loss :4.3211\n",
      "Epoch [47/300], Step [1/42], Loss: 4.0631\n",
      "Epoch [47/300], Step [2/42], Loss: 2.8064\n",
      "Epoch [47/300], Step [3/42], Loss: 3.3159\n",
      "Epoch [47/300], Step [4/42], Loss: 3.2486\n",
      "Epoch [47/300], Step [5/42], Loss: 3.9101\n",
      "Epoch [47/300], Step [6/42], Loss: 2.2045\n",
      "Epoch [47/300], Step [7/42], Loss: 3.5821\n",
      "Epoch [47/300], Step [8/42], Loss: 4.4535\n",
      "Epoch [47/300], Step [9/42], Loss: 7.4439\n",
      "Epoch [47/300], Step [10/42], Loss: 3.3888\n",
      "Epoch [47/300], Step [11/42], Loss: 2.3364\n",
      "Epoch [47/300], Step [12/42], Loss: 3.5988\n",
      "Epoch [47/300], Step [13/42], Loss: 4.1965\n",
      "Epoch [47/300], Step [14/42], Loss: 4.3629\n",
      "Epoch [47/300], Step [15/42], Loss: 3.1974\n",
      "Epoch [47/300], Step [16/42], Loss: 7.1517\n",
      "Epoch [47/300], Step [17/42], Loss: 3.9540\n",
      "Epoch [47/300], Step [18/42], Loss: 3.1011\n",
      "Epoch [47/300], Step [19/42], Loss: 3.4139\n",
      "Epoch [47/300], Step [20/42], Loss: 3.6946\n",
      "Epoch [47/300], Step [21/42], Loss: 2.7640\n",
      "Epoch [47/300], Step [22/42], Loss: 2.1832\n",
      "Epoch [47/300], Step [23/42], Loss: 1.6972\n",
      "Epoch [47/300], Step [24/42], Loss: 3.3949\n",
      "Epoch [47/300], Step [25/42], Loss: 4.6425\n",
      "Epoch [47/300], Step [26/42], Loss: 5.0365\n",
      "Epoch [47/300], Step [27/42], Loss: 4.5254\n",
      "Epoch [47/300], Step [28/42], Loss: 3.0524\n",
      "Epoch [47/300], Step [29/42], Loss: 3.1317\n",
      "Epoch [47/300], Step [30/42], Loss: 5.3617\n",
      "Epoch [47/300], Step [31/42], Loss: 2.8545\n",
      "Epoch [47/300], Step [32/42], Loss: 3.4408\n",
      "Epoch [47/300], Step [33/42], Loss: 3.7773\n",
      "Epoch [47/300], Step [34/42], Loss: 3.1812\n",
      "Epoch [47/300], Step [35/42], Loss: 3.3075\n",
      "Epoch [47/300], Step [36/42], Loss: 3.0131\n",
      "Epoch [47/300], Step [37/42], Loss: 4.6687\n",
      "Epoch [47/300], Step [38/42], Loss: 5.2217\n",
      "Epoch [47/300], Step [39/42], Loss: 3.9932\n",
      "Epoch [47/300], Step [40/42], Loss: 3.9014\n",
      "Epoch [47/300], Step [41/42], Loss: 4.1805\n",
      "Epoch [47/300], Step [42/42], Loss: 0.7332\n",
      "Val. loss :4.2097\n",
      "Epoch [48/300], Step [1/42], Loss: 2.1247\n",
      "Epoch [48/300], Step [2/42], Loss: 2.7151\n",
      "Epoch [48/300], Step [3/42], Loss: 2.3091\n",
      "Epoch [48/300], Step [4/42], Loss: 2.3714\n",
      "Epoch [48/300], Step [5/42], Loss: 4.2590\n",
      "Epoch [48/300], Step [6/42], Loss: 3.1416\n",
      "Epoch [48/300], Step [7/42], Loss: 3.3717\n",
      "Epoch [48/300], Step [8/42], Loss: 3.7055\n",
      "Epoch [48/300], Step [9/42], Loss: 4.1279\n",
      "Epoch [48/300], Step [10/42], Loss: 1.9844\n",
      "Epoch [48/300], Step [11/42], Loss: 3.0407\n",
      "Epoch [48/300], Step [12/42], Loss: 2.6768\n",
      "Epoch [48/300], Step [13/42], Loss: 4.8089\n",
      "Epoch [48/300], Step [14/42], Loss: 2.1902\n",
      "Epoch [48/300], Step [15/42], Loss: 2.7916\n",
      "Epoch [48/300], Step [16/42], Loss: 3.1914\n",
      "Epoch [48/300], Step [17/42], Loss: 2.0443\n",
      "Epoch [48/300], Step [18/42], Loss: 5.2170\n",
      "Epoch [48/300], Step [19/42], Loss: 3.4128\n",
      "Epoch [48/300], Step [20/42], Loss: 5.2050\n",
      "Epoch [48/300], Step [21/42], Loss: 4.7013\n",
      "Epoch [48/300], Step [22/42], Loss: 5.2093\n",
      "Epoch [48/300], Step [23/42], Loss: 4.2841\n",
      "Epoch [48/300], Step [24/42], Loss: 3.1819\n",
      "Epoch [48/300], Step [25/42], Loss: 2.4917\n",
      "Epoch [48/300], Step [26/42], Loss: 7.7137\n",
      "Epoch [48/300], Step [27/42], Loss: 3.0373\n",
      "Epoch [48/300], Step [28/42], Loss: 3.7764\n",
      "Epoch [48/300], Step [29/42], Loss: 2.8774\n",
      "Epoch [48/300], Step [30/42], Loss: 3.8067\n",
      "Epoch [48/300], Step [31/42], Loss: 1.9215\n",
      "Epoch [48/300], Step [32/42], Loss: 6.3807\n",
      "Epoch [48/300], Step [33/42], Loss: 3.7220\n",
      "Epoch [48/300], Step [34/42], Loss: 4.3461\n",
      "Epoch [48/300], Step [35/42], Loss: 2.7350\n",
      "Epoch [48/300], Step [36/42], Loss: 3.1282\n",
      "Epoch [48/300], Step [37/42], Loss: 2.4261\n",
      "Epoch [48/300], Step [38/42], Loss: 5.5031\n",
      "Epoch [48/300], Step [39/42], Loss: 5.0674\n",
      "Epoch [48/300], Step [40/42], Loss: 3.7212\n",
      "Epoch [48/300], Step [41/42], Loss: 4.4793\n",
      "Epoch [48/300], Step [42/42], Loss: 0.5834\n",
      "Val. loss :4.8745\n",
      "Epoch [49/300], Step [1/42], Loss: 5.4792\n",
      "Epoch [49/300], Step [2/42], Loss: 5.4923\n",
      "Epoch [49/300], Step [3/42], Loss: 3.4432\n",
      "Epoch [49/300], Step [4/42], Loss: 2.7196\n",
      "Epoch [49/300], Step [5/42], Loss: 3.2813\n",
      "Epoch [49/300], Step [6/42], Loss: 2.3420\n",
      "Epoch [49/300], Step [7/42], Loss: 5.8383\n",
      "Epoch [49/300], Step [8/42], Loss: 4.8936\n",
      "Epoch [49/300], Step [9/42], Loss: 3.1539\n",
      "Epoch [49/300], Step [10/42], Loss: 2.7418\n",
      "Epoch [49/300], Step [11/42], Loss: 6.5326\n",
      "Epoch [49/300], Step [12/42], Loss: 3.0096\n",
      "Epoch [49/300], Step [13/42], Loss: 2.4231\n",
      "Epoch [49/300], Step [14/42], Loss: 6.4590\n",
      "Epoch [49/300], Step [15/42], Loss: 2.1170\n",
      "Epoch [49/300], Step [16/42], Loss: 5.1822\n",
      "Epoch [49/300], Step [17/42], Loss: 2.8847\n",
      "Epoch [49/300], Step [18/42], Loss: 3.4891\n",
      "Epoch [49/300], Step [19/42], Loss: 3.5924\n",
      "Epoch [49/300], Step [20/42], Loss: 2.9003\n",
      "Epoch [49/300], Step [21/42], Loss: 3.2043\n",
      "Epoch [49/300], Step [22/42], Loss: 2.8426\n",
      "Epoch [49/300], Step [23/42], Loss: 2.9656\n",
      "Epoch [49/300], Step [24/42], Loss: 4.6267\n",
      "Epoch [49/300], Step [25/42], Loss: 4.6142\n",
      "Epoch [49/300], Step [26/42], Loss: 2.8909\n",
      "Epoch [49/300], Step [27/42], Loss: 3.2450\n",
      "Epoch [49/300], Step [28/42], Loss: 4.3259\n",
      "Epoch [49/300], Step [29/42], Loss: 4.4502\n",
      "Epoch [49/300], Step [30/42], Loss: 1.8430\n",
      "Epoch [49/300], Step [31/42], Loss: 2.0356\n",
      "Epoch [49/300], Step [32/42], Loss: 2.6222\n",
      "Epoch [49/300], Step [33/42], Loss: 2.3443\n",
      "Epoch [49/300], Step [34/42], Loss: 2.1675\n",
      "Epoch [49/300], Step [35/42], Loss: 4.7429\n",
      "Epoch [49/300], Step [36/42], Loss: 2.8144\n",
      "Epoch [49/300], Step [37/42], Loss: 3.2656\n",
      "Epoch [49/300], Step [38/42], Loss: 3.6882\n",
      "Epoch [49/300], Step [39/42], Loss: 3.1218\n",
      "Epoch [49/300], Step [40/42], Loss: 3.5394\n",
      "Epoch [49/300], Step [41/42], Loss: 2.4938\n",
      "Epoch [49/300], Step [42/42], Loss: 0.2595\n",
      "Val. loss :3.4663\n",
      "Epoch [50/300], Step [1/42], Loss: 6.1801\n",
      "Epoch [50/300], Step [2/42], Loss: 2.3956\n",
      "Epoch [50/300], Step [3/42], Loss: 2.6384\n",
      "Epoch [50/300], Step [4/42], Loss: 2.2996\n",
      "Epoch [50/300], Step [5/42], Loss: 5.4298\n",
      "Epoch [50/300], Step [6/42], Loss: 2.3718\n",
      "Epoch [50/300], Step [7/42], Loss: 2.1236\n",
      "Epoch [50/300], Step [8/42], Loss: 3.0843\n",
      "Epoch [50/300], Step [9/42], Loss: 4.1970\n",
      "Epoch [50/300], Step [10/42], Loss: 2.0758\n",
      "Epoch [50/300], Step [11/42], Loss: 3.5557\n",
      "Epoch [50/300], Step [12/42], Loss: 2.9438\n",
      "Epoch [50/300], Step [13/42], Loss: 3.7775\n",
      "Epoch [50/300], Step [14/42], Loss: 1.3606\n",
      "Epoch [50/300], Step [15/42], Loss: 3.9310\n",
      "Epoch [50/300], Step [16/42], Loss: 8.3278\n",
      "Epoch [50/300], Step [17/42], Loss: 2.5867\n",
      "Epoch [50/300], Step [18/42], Loss: 2.9979\n",
      "Epoch [50/300], Step [19/42], Loss: 4.2488\n",
      "Epoch [50/300], Step [20/42], Loss: 3.3790\n",
      "Epoch [50/300], Step [21/42], Loss: 3.2414\n",
      "Epoch [50/300], Step [22/42], Loss: 2.3122\n",
      "Epoch [50/300], Step [23/42], Loss: 2.4448\n",
      "Epoch [50/300], Step [24/42], Loss: 2.9606\n",
      "Epoch [50/300], Step [25/42], Loss: 1.7862\n",
      "Epoch [50/300], Step [26/42], Loss: 4.3876\n",
      "Epoch [50/300], Step [27/42], Loss: 2.8039\n",
      "Epoch [50/300], Step [28/42], Loss: 2.8700\n",
      "Epoch [50/300], Step [29/42], Loss: 2.0901\n",
      "Epoch [50/300], Step [30/42], Loss: 2.4955\n",
      "Epoch [50/300], Step [31/42], Loss: 2.1657\n",
      "Epoch [50/300], Step [32/42], Loss: 2.6904\n",
      "Epoch [50/300], Step [33/42], Loss: 1.8607\n",
      "Epoch [50/300], Step [34/42], Loss: 4.2746\n",
      "Epoch [50/300], Step [35/42], Loss: 2.6007\n",
      "Epoch [50/300], Step [36/42], Loss: 2.0785\n",
      "Epoch [50/300], Step [37/42], Loss: 2.7179\n",
      "Epoch [50/300], Step [38/42], Loss: 3.9630\n",
      "Epoch [50/300], Step [39/42], Loss: 4.6765\n",
      "Epoch [50/300], Step [40/42], Loss: 2.0931\n",
      "Epoch [50/300], Step [41/42], Loss: 1.9392\n",
      "Epoch [50/300], Step [42/42], Loss: 0.6447\n",
      "Val. loss :4.0332\n",
      "Epoch [51/300], Step [1/42], Loss: 4.0095\n",
      "Epoch [51/300], Step [2/42], Loss: 2.1956\n",
      "Epoch [51/300], Step [3/42], Loss: 3.2744\n",
      "Epoch [51/300], Step [4/42], Loss: 4.7366\n",
      "Epoch [51/300], Step [5/42], Loss: 3.8107\n",
      "Epoch [51/300], Step [6/42], Loss: 1.5993\n",
      "Epoch [51/300], Step [7/42], Loss: 2.6605\n",
      "Epoch [51/300], Step [8/42], Loss: 3.7895\n",
      "Epoch [51/300], Step [9/42], Loss: 2.6439\n",
      "Epoch [51/300], Step [10/42], Loss: 3.4708\n",
      "Epoch [51/300], Step [11/42], Loss: 3.1571\n",
      "Epoch [51/300], Step [12/42], Loss: 2.5678\n",
      "Epoch [51/300], Step [13/42], Loss: 4.0372\n",
      "Epoch [51/300], Step [14/42], Loss: 2.9106\n",
      "Epoch [51/300], Step [15/42], Loss: 4.5215\n",
      "Epoch [51/300], Step [16/42], Loss: 2.4269\n",
      "Epoch [51/300], Step [17/42], Loss: 1.5998\n",
      "Epoch [51/300], Step [18/42], Loss: 4.1654\n",
      "Epoch [51/300], Step [19/42], Loss: 3.7820\n",
      "Epoch [51/300], Step [20/42], Loss: 7.7269\n",
      "Epoch [51/300], Step [21/42], Loss: 2.3083\n",
      "Epoch [51/300], Step [22/42], Loss: 1.5196\n",
      "Epoch [51/300], Step [23/42], Loss: 2.6291\n",
      "Epoch [51/300], Step [24/42], Loss: 1.9113\n",
      "Epoch [51/300], Step [25/42], Loss: 4.4854\n",
      "Epoch [51/300], Step [26/42], Loss: 2.5551\n",
      "Epoch [51/300], Step [27/42], Loss: 3.2986\n",
      "Epoch [51/300], Step [28/42], Loss: 2.2128\n",
      "Epoch [51/300], Step [29/42], Loss: 2.1641\n",
      "Epoch [51/300], Step [30/42], Loss: 2.5220\n",
      "Epoch [51/300], Step [31/42], Loss: 3.5026\n",
      "Epoch [51/300], Step [32/42], Loss: 4.3508\n",
      "Epoch [51/300], Step [33/42], Loss: 3.0377\n",
      "Epoch [51/300], Step [34/42], Loss: 2.7284\n",
      "Epoch [51/300], Step [35/42], Loss: 2.3551\n",
      "Epoch [51/300], Step [36/42], Loss: 1.7144\n",
      "Epoch [51/300], Step [37/42], Loss: 2.7661\n",
      "Epoch [51/300], Step [38/42], Loss: 1.8343\n",
      "Epoch [51/300], Step [39/42], Loss: 4.5453\n",
      "Epoch [51/300], Step [40/42], Loss: 2.1940\n",
      "Epoch [51/300], Step [41/42], Loss: 4.2302\n",
      "Epoch [51/300], Step [42/42], Loss: 0.6029\n",
      "Val. loss :3.3769\n",
      "Epoch [52/300], Step [1/42], Loss: 1.3045\n",
      "Epoch [52/300], Step [2/42], Loss: 2.0092\n",
      "Epoch [52/300], Step [3/42], Loss: 1.8812\n",
      "Epoch [52/300], Step [4/42], Loss: 3.7017\n",
      "Epoch [52/300], Step [5/42], Loss: 2.3199\n",
      "Epoch [52/300], Step [6/42], Loss: 4.6071\n",
      "Epoch [52/300], Step [7/42], Loss: 1.6734\n",
      "Epoch [52/300], Step [8/42], Loss: 3.0769\n",
      "Epoch [52/300], Step [9/42], Loss: 1.9082\n",
      "Epoch [52/300], Step [10/42], Loss: 1.5840\n",
      "Epoch [52/300], Step [11/42], Loss: 4.0649\n",
      "Epoch [52/300], Step [12/42], Loss: 2.5743\n",
      "Epoch [52/300], Step [13/42], Loss: 1.7604\n",
      "Epoch [52/300], Step [14/42], Loss: 2.7997\n",
      "Epoch [52/300], Step [15/42], Loss: 2.1574\n",
      "Epoch [52/300], Step [16/42], Loss: 3.0903\n",
      "Epoch [52/300], Step [17/42], Loss: 2.3804\n",
      "Epoch [52/300], Step [18/42], Loss: 2.0661\n",
      "Epoch [52/300], Step [19/42], Loss: 2.8920\n",
      "Epoch [52/300], Step [20/42], Loss: 4.0049\n",
      "Epoch [52/300], Step [21/42], Loss: 7.8509\n",
      "Epoch [52/300], Step [22/42], Loss: 2.6274\n",
      "Epoch [52/300], Step [23/42], Loss: 4.2105\n",
      "Epoch [52/300], Step [24/42], Loss: 3.3834\n",
      "Epoch [52/300], Step [25/42], Loss: 3.5377\n",
      "Epoch [52/300], Step [26/42], Loss: 4.9590\n",
      "Epoch [52/300], Step [27/42], Loss: 1.8393\n",
      "Epoch [52/300], Step [28/42], Loss: 1.4119\n",
      "Epoch [52/300], Step [29/42], Loss: 2.9171\n",
      "Epoch [52/300], Step [30/42], Loss: 1.4705\n",
      "Epoch [52/300], Step [31/42], Loss: 5.6670\n",
      "Epoch [52/300], Step [32/42], Loss: 3.0339\n",
      "Epoch [52/300], Step [33/42], Loss: 4.4529\n",
      "Epoch [52/300], Step [34/42], Loss: 6.0562\n",
      "Epoch [52/300], Step [35/42], Loss: 1.5325\n",
      "Epoch [52/300], Step [36/42], Loss: 2.0279\n",
      "Epoch [52/300], Step [37/42], Loss: 2.1378\n",
      "Epoch [52/300], Step [38/42], Loss: 2.6144\n",
      "Epoch [52/300], Step [39/42], Loss: 1.8740\n",
      "Epoch [52/300], Step [40/42], Loss: 3.2750\n",
      "Epoch [52/300], Step [41/42], Loss: 3.0565\n",
      "Epoch [52/300], Step [42/42], Loss: 0.3308\n",
      "Val. loss :3.0859\n",
      "Epoch [53/300], Step [1/42], Loss: 2.1061\n",
      "Epoch [53/300], Step [2/42], Loss: 2.9001\n",
      "Epoch [53/300], Step [3/42], Loss: 4.1240\n",
      "Epoch [53/300], Step [4/42], Loss: 1.9354\n",
      "Epoch [53/300], Step [5/42], Loss: 3.4680\n",
      "Epoch [53/300], Step [6/42], Loss: 2.7576\n",
      "Epoch [53/300], Step [7/42], Loss: 2.9290\n",
      "Epoch [53/300], Step [8/42], Loss: 3.2209\n",
      "Epoch [53/300], Step [9/42], Loss: 2.0575\n",
      "Epoch [53/300], Step [10/42], Loss: 6.0529\n",
      "Epoch [53/300], Step [11/42], Loss: 1.8622\n",
      "Epoch [53/300], Step [12/42], Loss: 2.1338\n",
      "Epoch [53/300], Step [13/42], Loss: 1.6906\n",
      "Epoch [53/300], Step [14/42], Loss: 1.8704\n",
      "Epoch [53/300], Step [15/42], Loss: 1.3838\n",
      "Epoch [53/300], Step [16/42], Loss: 2.4212\n",
      "Epoch [53/300], Step [17/42], Loss: 1.3787\n",
      "Epoch [53/300], Step [18/42], Loss: 5.0882\n",
      "Epoch [53/300], Step [19/42], Loss: 2.3278\n",
      "Epoch [53/300], Step [20/42], Loss: 3.6676\n",
      "Epoch [53/300], Step [21/42], Loss: 3.4691\n",
      "Epoch [53/300], Step [22/42], Loss: 2.7042\n",
      "Epoch [53/300], Step [23/42], Loss: 2.9204\n",
      "Epoch [53/300], Step [24/42], Loss: 3.1784\n",
      "Epoch [53/300], Step [25/42], Loss: 3.5479\n",
      "Epoch [53/300], Step [26/42], Loss: 2.9110\n",
      "Epoch [53/300], Step [27/42], Loss: 1.2477\n",
      "Epoch [53/300], Step [28/42], Loss: 3.5595\n",
      "Epoch [53/300], Step [29/42], Loss: 1.9159\n",
      "Epoch [53/300], Step [30/42], Loss: 3.1496\n",
      "Epoch [53/300], Step [31/42], Loss: 1.4522\n",
      "Epoch [53/300], Step [32/42], Loss: 1.4779\n",
      "Epoch [53/300], Step [33/42], Loss: 3.5773\n",
      "Epoch [53/300], Step [34/42], Loss: 4.3841\n",
      "Epoch [53/300], Step [35/42], Loss: 2.5304\n",
      "Epoch [53/300], Step [36/42], Loss: 2.1247\n",
      "Epoch [53/300], Step [37/42], Loss: 2.4674\n",
      "Epoch [53/300], Step [38/42], Loss: 3.2687\n",
      "Epoch [53/300], Step [39/42], Loss: 4.0771\n",
      "Epoch [53/300], Step [40/42], Loss: 2.7500\n",
      "Epoch [53/300], Step [41/42], Loss: 1.4649\n",
      "Epoch [53/300], Step [42/42], Loss: 1.1363\n",
      "Val. loss :3.0127\n",
      "Epoch [54/300], Step [1/42], Loss: 2.8433\n",
      "Epoch [54/300], Step [2/42], Loss: 5.0048\n",
      "Epoch [54/300], Step [3/42], Loss: 2.6583\n",
      "Epoch [54/300], Step [4/42], Loss: 3.5125\n",
      "Epoch [54/300], Step [5/42], Loss: 1.9671\n",
      "Epoch [54/300], Step [6/42], Loss: 1.7514\n",
      "Epoch [54/300], Step [7/42], Loss: 1.5737\n",
      "Epoch [54/300], Step [8/42], Loss: 3.2171\n",
      "Epoch [54/300], Step [9/42], Loss: 3.7844\n",
      "Epoch [54/300], Step [10/42], Loss: 1.8961\n",
      "Epoch [54/300], Step [11/42], Loss: 1.4492\n",
      "Epoch [54/300], Step [12/42], Loss: 5.3581\n",
      "Epoch [54/300], Step [13/42], Loss: 1.5019\n",
      "Epoch [54/300], Step [14/42], Loss: 2.3276\n",
      "Epoch [54/300], Step [15/42], Loss: 2.7413\n",
      "Epoch [54/300], Step [16/42], Loss: 1.6175\n",
      "Epoch [54/300], Step [17/42], Loss: 3.3043\n",
      "Epoch [54/300], Step [18/42], Loss: 1.7057\n",
      "Epoch [54/300], Step [19/42], Loss: 3.0323\n",
      "Epoch [54/300], Step [20/42], Loss: 3.4040\n",
      "Epoch [54/300], Step [21/42], Loss: 2.4064\n",
      "Epoch [54/300], Step [22/42], Loss: 2.1663\n",
      "Epoch [54/300], Step [23/42], Loss: 1.7657\n",
      "Epoch [54/300], Step [24/42], Loss: 3.6447\n",
      "Epoch [54/300], Step [25/42], Loss: 1.5499\n",
      "Epoch [54/300], Step [26/42], Loss: 1.7678\n",
      "Epoch [54/300], Step [27/42], Loss: 4.0005\n",
      "Epoch [54/300], Step [28/42], Loss: 2.9836\n",
      "Epoch [54/300], Step [29/42], Loss: 3.8744\n",
      "Epoch [54/300], Step [30/42], Loss: 2.1955\n",
      "Epoch [54/300], Step [31/42], Loss: 2.5770\n",
      "Epoch [54/300], Step [32/42], Loss: 2.3218\n",
      "Epoch [54/300], Step [33/42], Loss: 3.6014\n",
      "Epoch [54/300], Step [34/42], Loss: 1.1023\n",
      "Epoch [54/300], Step [35/42], Loss: 1.2892\n",
      "Epoch [54/300], Step [36/42], Loss: 3.1606\n",
      "Epoch [54/300], Step [37/42], Loss: 2.5304\n",
      "Epoch [54/300], Step [38/42], Loss: 1.7976\n",
      "Epoch [54/300], Step [39/42], Loss: 2.0918\n",
      "Epoch [54/300], Step [40/42], Loss: 1.7280\n",
      "Epoch [54/300], Step [41/42], Loss: 3.8242\n",
      "Epoch [54/300], Step [42/42], Loss: 0.5358\n",
      "Val. loss :2.9997\n",
      "Epoch [55/300], Step [1/42], Loss: 4.4359\n",
      "Epoch [55/300], Step [2/42], Loss: 0.9493\n",
      "Epoch [55/300], Step [3/42], Loss: 2.0586\n",
      "Epoch [55/300], Step [4/42], Loss: 1.4715\n",
      "Epoch [55/300], Step [5/42], Loss: 1.1751\n",
      "Epoch [55/300], Step [6/42], Loss: 3.4465\n",
      "Epoch [55/300], Step [7/42], Loss: 2.1168\n",
      "Epoch [55/300], Step [8/42], Loss: 1.9811\n",
      "Epoch [55/300], Step [9/42], Loss: 1.2240\n",
      "Epoch [55/300], Step [10/42], Loss: 3.4932\n",
      "Epoch [55/300], Step [11/42], Loss: 2.0916\n",
      "Epoch [55/300], Step [12/42], Loss: 1.6430\n",
      "Epoch [55/300], Step [13/42], Loss: 1.3484\n",
      "Epoch [55/300], Step [14/42], Loss: 1.9360\n",
      "Epoch [55/300], Step [15/42], Loss: 3.3681\n",
      "Epoch [55/300], Step [16/42], Loss: 3.7734\n",
      "Epoch [55/300], Step [17/42], Loss: 2.6275\n",
      "Epoch [55/300], Step [18/42], Loss: 1.7329\n",
      "Epoch [55/300], Step [19/42], Loss: 1.7258\n",
      "Epoch [55/300], Step [20/42], Loss: 1.5089\n",
      "Epoch [55/300], Step [21/42], Loss: 3.1414\n",
      "Epoch [55/300], Step [22/42], Loss: 1.6026\n",
      "Epoch [55/300], Step [23/42], Loss: 2.6363\n",
      "Epoch [55/300], Step [24/42], Loss: 1.6199\n",
      "Epoch [55/300], Step [25/42], Loss: 6.4435\n",
      "Epoch [55/300], Step [26/42], Loss: 2.9059\n",
      "Epoch [55/300], Step [27/42], Loss: 2.5952\n",
      "Epoch [55/300], Step [28/42], Loss: 2.6970\n",
      "Epoch [55/300], Step [29/42], Loss: 3.6778\n",
      "Epoch [55/300], Step [30/42], Loss: 5.0659\n",
      "Epoch [55/300], Step [31/42], Loss: 2.2538\n",
      "Epoch [55/300], Step [32/42], Loss: 1.3970\n",
      "Epoch [55/300], Step [33/42], Loss: 3.4308\n",
      "Epoch [55/300], Step [34/42], Loss: 3.2495\n",
      "Epoch [55/300], Step [35/42], Loss: 3.0443\n",
      "Epoch [55/300], Step [36/42], Loss: 1.7946\n",
      "Epoch [55/300], Step [37/42], Loss: 3.1942\n",
      "Epoch [55/300], Step [38/42], Loss: 1.6000\n",
      "Epoch [55/300], Step [39/42], Loss: 1.9787\n",
      "Epoch [55/300], Step [40/42], Loss: 3.8465\n",
      "Epoch [55/300], Step [41/42], Loss: 1.6776\n",
      "Epoch [55/300], Step [42/42], Loss: 0.1561\n",
      "Val. loss :3.2477\n",
      "Epoch [56/300], Step [1/42], Loss: 2.4702\n",
      "Epoch [56/300], Step [2/42], Loss: 2.1744\n",
      "Epoch [56/300], Step [3/42], Loss: 1.4925\n",
      "Epoch [56/300], Step [4/42], Loss: 2.1062\n",
      "Epoch [56/300], Step [5/42], Loss: 2.4694\n",
      "Epoch [56/300], Step [6/42], Loss: 2.2268\n",
      "Epoch [56/300], Step [7/42], Loss: 2.6333\n",
      "Epoch [56/300], Step [8/42], Loss: 4.0112\n",
      "Epoch [56/300], Step [9/42], Loss: 1.6838\n",
      "Epoch [56/300], Step [10/42], Loss: 1.5535\n",
      "Epoch [56/300], Step [11/42], Loss: 1.3508\n",
      "Epoch [56/300], Step [12/42], Loss: 3.0888\n",
      "Epoch [56/300], Step [13/42], Loss: 2.8323\n",
      "Epoch [56/300], Step [14/42], Loss: 2.5994\n",
      "Epoch [56/300], Step [15/42], Loss: 3.3800\n",
      "Epoch [56/300], Step [16/42], Loss: 2.8329\n",
      "Epoch [56/300], Step [17/42], Loss: 1.9416\n",
      "Epoch [56/300], Step [18/42], Loss: 2.7622\n",
      "Epoch [56/300], Step [19/42], Loss: 1.5005\n",
      "Epoch [56/300], Step [20/42], Loss: 2.2047\n",
      "Epoch [56/300], Step [21/42], Loss: 2.2974\n",
      "Epoch [56/300], Step [22/42], Loss: 3.2298\n",
      "Epoch [56/300], Step [23/42], Loss: 2.2673\n",
      "Epoch [56/300], Step [24/42], Loss: 3.0532\n",
      "Epoch [56/300], Step [25/42], Loss: 1.4373\n",
      "Epoch [56/300], Step [26/42], Loss: 7.2550\n",
      "Epoch [56/300], Step [27/42], Loss: 3.8259\n",
      "Epoch [56/300], Step [28/42], Loss: 5.0710\n",
      "Epoch [56/300], Step [29/42], Loss: 2.5049\n",
      "Epoch [56/300], Step [30/42], Loss: 1.6860\n",
      "Epoch [56/300], Step [31/42], Loss: 3.7821\n",
      "Epoch [56/300], Step [32/42], Loss: 0.9174\n",
      "Epoch [56/300], Step [33/42], Loss: 3.6551\n",
      "Epoch [56/300], Step [34/42], Loss: 1.2115\n",
      "Epoch [56/300], Step [35/42], Loss: 1.6975\n",
      "Epoch [56/300], Step [36/42], Loss: 4.0184\n",
      "Epoch [56/300], Step [37/42], Loss: 5.3156\n",
      "Epoch [56/300], Step [38/42], Loss: 5.6039\n",
      "Epoch [56/300], Step [39/42], Loss: 2.4098\n",
      "Epoch [56/300], Step [40/42], Loss: 1.2839\n",
      "Epoch [56/300], Step [41/42], Loss: 1.1591\n",
      "Epoch [56/300], Step [42/42], Loss: 0.3653\n",
      "Val. loss :2.9335\n",
      "Epoch [57/300], Step [1/42], Loss: 2.1523\n",
      "Epoch [57/300], Step [2/42], Loss: 3.7970\n",
      "Epoch [57/300], Step [3/42], Loss: 2.3366\n",
      "Epoch [57/300], Step [4/42], Loss: 2.2332\n",
      "Epoch [57/300], Step [5/42], Loss: 2.5535\n",
      "Epoch [57/300], Step [6/42], Loss: 2.1209\n",
      "Epoch [57/300], Step [7/42], Loss: 1.7693\n",
      "Epoch [57/300], Step [8/42], Loss: 1.7487\n",
      "Epoch [57/300], Step [9/42], Loss: 2.4343\n",
      "Epoch [57/300], Step [10/42], Loss: 3.4759\n",
      "Epoch [57/300], Step [11/42], Loss: 1.3984\n",
      "Epoch [57/300], Step [12/42], Loss: 3.7660\n",
      "Epoch [57/300], Step [13/42], Loss: 1.2519\n",
      "Epoch [57/300], Step [14/42], Loss: 1.6642\n",
      "Epoch [57/300], Step [15/42], Loss: 3.7175\n",
      "Epoch [57/300], Step [16/42], Loss: 2.2843\n",
      "Epoch [57/300], Step [17/42], Loss: 2.0184\n",
      "Epoch [57/300], Step [18/42], Loss: 2.0143\n",
      "Epoch [57/300], Step [19/42], Loss: 1.2249\n",
      "Epoch [57/300], Step [20/42], Loss: 1.9506\n",
      "Epoch [57/300], Step [21/42], Loss: 2.1828\n",
      "Epoch [57/300], Step [22/42], Loss: 3.0461\n",
      "Epoch [57/300], Step [23/42], Loss: 2.3877\n",
      "Epoch [57/300], Step [24/42], Loss: 1.2210\n",
      "Epoch [57/300], Step [25/42], Loss: 2.3585\n",
      "Epoch [57/300], Step [26/42], Loss: 2.3345\n",
      "Epoch [57/300], Step [27/42], Loss: 4.0882\n",
      "Epoch [57/300], Step [28/42], Loss: 1.2908\n",
      "Epoch [57/300], Step [29/42], Loss: 1.8702\n",
      "Epoch [57/300], Step [30/42], Loss: 1.1984\n",
      "Epoch [57/300], Step [31/42], Loss: 4.6532\n",
      "Epoch [57/300], Step [32/42], Loss: 2.6336\n",
      "Epoch [57/300], Step [33/42], Loss: 1.9866\n",
      "Epoch [57/300], Step [34/42], Loss: 7.6573\n",
      "Epoch [57/300], Step [35/42], Loss: 2.1736\n",
      "Epoch [57/300], Step [36/42], Loss: 1.2277\n",
      "Epoch [57/300], Step [37/42], Loss: 3.0533\n",
      "Epoch [57/300], Step [38/42], Loss: 1.2373\n",
      "Epoch [57/300], Step [39/42], Loss: 1.8654\n",
      "Epoch [57/300], Step [40/42], Loss: 1.6998\n",
      "Epoch [57/300], Step [41/42], Loss: 1.2573\n",
      "Epoch [57/300], Step [42/42], Loss: 0.3626\n",
      "Val. loss :2.8534\n",
      "Epoch [58/300], Step [1/42], Loss: 2.2428\n",
      "Epoch [58/300], Step [2/42], Loss: 1.6370\n",
      "Epoch [58/300], Step [3/42], Loss: 3.8621\n",
      "Epoch [58/300], Step [4/42], Loss: 4.2760\n",
      "Epoch [58/300], Step [5/42], Loss: 2.2358\n",
      "Epoch [58/300], Step [6/42], Loss: 1.8633\n",
      "Epoch [58/300], Step [7/42], Loss: 1.2833\n",
      "Epoch [58/300], Step [8/42], Loss: 3.6056\n",
      "Epoch [58/300], Step [9/42], Loss: 1.2342\n",
      "Epoch [58/300], Step [10/42], Loss: 1.9290\n",
      "Epoch [58/300], Step [11/42], Loss: 1.8512\n",
      "Epoch [58/300], Step [12/42], Loss: 3.8576\n",
      "Epoch [58/300], Step [13/42], Loss: 1.4421\n",
      "Epoch [58/300], Step [14/42], Loss: 2.2087\n",
      "Epoch [58/300], Step [15/42], Loss: 1.8903\n",
      "Epoch [58/300], Step [16/42], Loss: 3.3052\n",
      "Epoch [58/300], Step [17/42], Loss: 1.6380\n",
      "Epoch [58/300], Step [18/42], Loss: 2.6446\n",
      "Epoch [58/300], Step [19/42], Loss: 1.7833\n",
      "Epoch [58/300], Step [20/42], Loss: 1.1829\n",
      "Epoch [58/300], Step [21/42], Loss: 1.8219\n",
      "Epoch [58/300], Step [22/42], Loss: 3.3774\n",
      "Epoch [58/300], Step [23/42], Loss: 2.8680\n",
      "Epoch [58/300], Step [24/42], Loss: 2.6745\n",
      "Epoch [58/300], Step [25/42], Loss: 1.7079\n",
      "Epoch [58/300], Step [26/42], Loss: 3.5491\n",
      "Epoch [58/300], Step [27/42], Loss: 1.5730\n",
      "Epoch [58/300], Step [28/42], Loss: 1.8758\n",
      "Epoch [58/300], Step [29/42], Loss: 1.8484\n",
      "Epoch [58/300], Step [30/42], Loss: 2.5431\n",
      "Epoch [58/300], Step [31/42], Loss: 1.6642\n",
      "Epoch [58/300], Step [32/42], Loss: 1.8785\n",
      "Epoch [58/300], Step [33/42], Loss: 2.3435\n",
      "Epoch [58/300], Step [34/42], Loss: 1.0468\n",
      "Epoch [58/300], Step [35/42], Loss: 3.5297\n",
      "Epoch [58/300], Step [36/42], Loss: 1.0637\n",
      "Epoch [58/300], Step [37/42], Loss: 1.6289\n",
      "Epoch [58/300], Step [38/42], Loss: 2.8737\n",
      "Epoch [58/300], Step [39/42], Loss: 6.0656\n",
      "Epoch [58/300], Step [40/42], Loss: 1.5116\n",
      "Epoch [58/300], Step [41/42], Loss: 2.4626\n",
      "Epoch [58/300], Step [42/42], Loss: 0.6588\n",
      "Val. loss :2.6109\n",
      "Epoch [59/300], Step [1/42], Loss: 1.1245\n",
      "Epoch [59/300], Step [2/42], Loss: 2.7413\n",
      "Epoch [59/300], Step [3/42], Loss: 4.5022\n",
      "Epoch [59/300], Step [4/42], Loss: 3.4117\n",
      "Epoch [59/300], Step [5/42], Loss: 1.9404\n",
      "Epoch [59/300], Step [6/42], Loss: 1.8483\n",
      "Epoch [59/300], Step [7/42], Loss: 3.6513\n",
      "Epoch [59/300], Step [8/42], Loss: 2.7638\n",
      "Epoch [59/300], Step [9/42], Loss: 3.2965\n",
      "Epoch [59/300], Step [10/42], Loss: 1.3538\n",
      "Epoch [59/300], Step [11/42], Loss: 3.3651\n",
      "Epoch [59/300], Step [12/42], Loss: 1.7730\n",
      "Epoch [59/300], Step [13/42], Loss: 9.2503\n",
      "Epoch [59/300], Step [14/42], Loss: 4.9045\n",
      "Epoch [59/300], Step [15/42], Loss: 1.3481\n",
      "Epoch [59/300], Step [16/42], Loss: 4.6273\n",
      "Epoch [59/300], Step [17/42], Loss: 1.7229\n",
      "Epoch [59/300], Step [18/42], Loss: 2.2153\n",
      "Epoch [59/300], Step [19/42], Loss: 2.0412\n",
      "Epoch [59/300], Step [20/42], Loss: 3.9961\n",
      "Epoch [59/300], Step [21/42], Loss: 2.1796\n",
      "Epoch [59/300], Step [22/42], Loss: 2.3271\n",
      "Epoch [59/300], Step [23/42], Loss: 1.3835\n",
      "Epoch [59/300], Step [24/42], Loss: 1.1777\n",
      "Epoch [59/300], Step [25/42], Loss: 2.6506\n",
      "Epoch [59/300], Step [26/42], Loss: 3.1139\n",
      "Epoch [59/300], Step [27/42], Loss: 3.2342\n",
      "Epoch [59/300], Step [28/42], Loss: 2.2808\n",
      "Epoch [59/300], Step [29/42], Loss: 0.8853\n",
      "Epoch [59/300], Step [30/42], Loss: 3.4971\n",
      "Epoch [59/300], Step [31/42], Loss: 1.4111\n",
      "Epoch [59/300], Step [32/42], Loss: 2.0541\n",
      "Epoch [59/300], Step [33/42], Loss: 2.9294\n",
      "Epoch [59/300], Step [34/42], Loss: 0.8415\n",
      "Epoch [59/300], Step [35/42], Loss: 3.1147\n",
      "Epoch [59/300], Step [36/42], Loss: 2.1183\n",
      "Epoch [59/300], Step [37/42], Loss: 2.7805\n",
      "Epoch [59/300], Step [38/42], Loss: 1.9203\n",
      "Epoch [59/300], Step [39/42], Loss: 2.8831\n",
      "Epoch [59/300], Step [40/42], Loss: 6.6006\n",
      "Epoch [59/300], Step [41/42], Loss: 1.2113\n",
      "Epoch [59/300], Step [42/42], Loss: 0.1175\n",
      "Val. loss :2.5746\n",
      "Epoch [60/300], Step [1/42], Loss: 1.3122\n",
      "Epoch [60/300], Step [2/42], Loss: 3.1115\n",
      "Epoch [60/300], Step [3/42], Loss: 2.2728\n",
      "Epoch [60/300], Step [4/42], Loss: 0.8582\n",
      "Epoch [60/300], Step [5/42], Loss: 1.3297\n",
      "Epoch [60/300], Step [6/42], Loss: 1.9237\n",
      "Epoch [60/300], Step [7/42], Loss: 2.0073\n",
      "Epoch [60/300], Step [8/42], Loss: 1.5769\n",
      "Epoch [60/300], Step [9/42], Loss: 1.4592\n",
      "Epoch [60/300], Step [10/42], Loss: 1.4868\n",
      "Epoch [60/300], Step [11/42], Loss: 1.0856\n",
      "Epoch [60/300], Step [12/42], Loss: 1.8609\n",
      "Epoch [60/300], Step [13/42], Loss: 2.5443\n",
      "Epoch [60/300], Step [14/42], Loss: 2.6867\n",
      "Epoch [60/300], Step [15/42], Loss: 1.7861\n",
      "Epoch [60/300], Step [16/42], Loss: 1.6100\n",
      "Epoch [60/300], Step [17/42], Loss: 1.2114\n",
      "Epoch [60/300], Step [18/42], Loss: 1.0077\n",
      "Epoch [60/300], Step [19/42], Loss: 1.2341\n",
      "Epoch [60/300], Step [20/42], Loss: 1.7300\n",
      "Epoch [60/300], Step [21/42], Loss: 1.8390\n",
      "Epoch [60/300], Step [22/42], Loss: 2.3698\n",
      "Epoch [60/300], Step [23/42], Loss: 2.1790\n",
      "Epoch [60/300], Step [24/42], Loss: 2.7999\n",
      "Epoch [60/300], Step [25/42], Loss: 1.8533\n",
      "Epoch [60/300], Step [26/42], Loss: 1.3401\n",
      "Epoch [60/300], Step [27/42], Loss: 1.7948\n",
      "Epoch [60/300], Step [28/42], Loss: 5.5106\n",
      "Epoch [60/300], Step [29/42], Loss: 1.8124\n",
      "Epoch [60/300], Step [30/42], Loss: 2.6473\n",
      "Epoch [60/300], Step [31/42], Loss: 1.3811\n",
      "Epoch [60/300], Step [32/42], Loss: 2.9711\n",
      "Epoch [60/300], Step [33/42], Loss: 1.2256\n",
      "Epoch [60/300], Step [34/42], Loss: 2.2697\n",
      "Epoch [60/300], Step [35/42], Loss: 2.6939\n",
      "Epoch [60/300], Step [36/42], Loss: 1.5360\n",
      "Epoch [60/300], Step [37/42], Loss: 1.1361\n",
      "Epoch [60/300], Step [38/42], Loss: 3.6628\n",
      "Epoch [60/300], Step [39/42], Loss: 5.4148\n",
      "Epoch [60/300], Step [40/42], Loss: 2.0785\n",
      "Epoch [60/300], Step [41/42], Loss: 2.6943\n",
      "Epoch [60/300], Step [42/42], Loss: 0.4754\n",
      "Val. loss :2.3482\n",
      "Epoch [61/300], Step [1/42], Loss: 1.3817\n",
      "Epoch [61/300], Step [2/42], Loss: 2.3600\n",
      "Epoch [61/300], Step [3/42], Loss: 1.7749\n",
      "Epoch [61/300], Step [4/42], Loss: 1.2016\n",
      "Epoch [61/300], Step [5/42], Loss: 2.7471\n",
      "Epoch [61/300], Step [6/42], Loss: 2.9583\n",
      "Epoch [61/300], Step [7/42], Loss: 2.8376\n",
      "Epoch [61/300], Step [8/42], Loss: 1.0444\n",
      "Epoch [61/300], Step [9/42], Loss: 0.9675\n",
      "Epoch [61/300], Step [10/42], Loss: 1.5975\n",
      "Epoch [61/300], Step [11/42], Loss: 1.6922\n",
      "Epoch [61/300], Step [12/42], Loss: 4.6418\n",
      "Epoch [61/300], Step [13/42], Loss: 1.2588\n",
      "Epoch [61/300], Step [14/42], Loss: 1.6265\n",
      "Epoch [61/300], Step [15/42], Loss: 2.7949\n",
      "Epoch [61/300], Step [16/42], Loss: 1.6831\n",
      "Epoch [61/300], Step [17/42], Loss: 1.7845\n",
      "Epoch [61/300], Step [18/42], Loss: 1.7176\n",
      "Epoch [61/300], Step [19/42], Loss: 1.8857\n",
      "Epoch [61/300], Step [20/42], Loss: 2.7541\n",
      "Epoch [61/300], Step [21/42], Loss: 1.7820\n",
      "Epoch [61/300], Step [22/42], Loss: 2.9678\n",
      "Epoch [61/300], Step [23/42], Loss: 1.9937\n",
      "Epoch [61/300], Step [24/42], Loss: 1.0226\n",
      "Epoch [61/300], Step [25/42], Loss: 2.0709\n",
      "Epoch [61/300], Step [26/42], Loss: 1.1937\n",
      "Epoch [61/300], Step [27/42], Loss: 0.6820\n",
      "Epoch [61/300], Step [28/42], Loss: 1.2839\n",
      "Epoch [61/300], Step [29/42], Loss: 2.1471\n",
      "Epoch [61/300], Step [30/42], Loss: 1.0556\n",
      "Epoch [61/300], Step [31/42], Loss: 2.2973\n",
      "Epoch [61/300], Step [32/42], Loss: 2.7389\n",
      "Epoch [61/300], Step [33/42], Loss: 1.2167\n",
      "Epoch [61/300], Step [34/42], Loss: 4.7984\n",
      "Epoch [61/300], Step [35/42], Loss: 1.7774\n",
      "Epoch [61/300], Step [36/42], Loss: 5.1451\n",
      "Epoch [61/300], Step [37/42], Loss: 2.1782\n",
      "Epoch [61/300], Step [38/42], Loss: 1.6354\n",
      "Epoch [61/300], Step [39/42], Loss: 1.3819\n",
      "Epoch [61/300], Step [40/42], Loss: 1.2872\n",
      "Epoch [61/300], Step [41/42], Loss: 2.1586\n",
      "Epoch [61/300], Step [42/42], Loss: 0.2473\n",
      "Val. loss :3.0709\n",
      "Epoch [62/300], Step [1/42], Loss: 2.1267\n",
      "Epoch [62/300], Step [2/42], Loss: 1.5920\n",
      "Epoch [62/300], Step [3/42], Loss: 4.0535\n",
      "Epoch [62/300], Step [4/42], Loss: 0.9532\n",
      "Epoch [62/300], Step [5/42], Loss: 2.2763\n",
      "Epoch [62/300], Step [6/42], Loss: 0.9797\n",
      "Epoch [62/300], Step [7/42], Loss: 4.1120\n",
      "Epoch [62/300], Step [8/42], Loss: 0.8507\n",
      "Epoch [62/300], Step [9/42], Loss: 1.4023\n",
      "Epoch [62/300], Step [10/42], Loss: 1.8092\n",
      "Epoch [62/300], Step [11/42], Loss: 1.3328\n",
      "Epoch [62/300], Step [12/42], Loss: 1.5815\n",
      "Epoch [62/300], Step [13/42], Loss: 1.8603\n",
      "Epoch [62/300], Step [14/42], Loss: 2.4246\n",
      "Epoch [62/300], Step [15/42], Loss: 4.0547\n",
      "Epoch [62/300], Step [16/42], Loss: 2.4327\n",
      "Epoch [62/300], Step [17/42], Loss: 1.9581\n",
      "Epoch [62/300], Step [18/42], Loss: 3.4587\n",
      "Epoch [62/300], Step [19/42], Loss: 4.1732\n",
      "Epoch [62/300], Step [20/42], Loss: 1.6874\n",
      "Epoch [62/300], Step [21/42], Loss: 0.9871\n",
      "Epoch [62/300], Step [22/42], Loss: 0.5935\n",
      "Epoch [62/300], Step [23/42], Loss: 3.8440\n",
      "Epoch [62/300], Step [24/42], Loss: 1.4075\n",
      "Epoch [62/300], Step [25/42], Loss: 2.6259\n",
      "Epoch [62/300], Step [26/42], Loss: 2.4679\n",
      "Epoch [62/300], Step [27/42], Loss: 2.6153\n",
      "Epoch [62/300], Step [28/42], Loss: 0.8736\n",
      "Epoch [62/300], Step [29/42], Loss: 0.6644\n",
      "Epoch [62/300], Step [30/42], Loss: 1.6450\n",
      "Epoch [62/300], Step [31/42], Loss: 1.5784\n",
      "Epoch [62/300], Step [32/42], Loss: 2.6719\n",
      "Epoch [62/300], Step [33/42], Loss: 2.5460\n",
      "Epoch [62/300], Step [34/42], Loss: 2.8306\n",
      "Epoch [62/300], Step [35/42], Loss: 1.9471\n",
      "Epoch [62/300], Step [36/42], Loss: 1.0222\n",
      "Epoch [62/300], Step [37/42], Loss: 1.1931\n",
      "Epoch [62/300], Step [38/42], Loss: 2.6919\n",
      "Epoch [62/300], Step [39/42], Loss: 1.0098\n",
      "Epoch [62/300], Step [40/42], Loss: 4.4926\n",
      "Epoch [62/300], Step [41/42], Loss: 1.3106\n",
      "Epoch [62/300], Step [42/42], Loss: 0.2217\n",
      "Val. loss :2.1859\n",
      "Epoch [63/300], Step [1/42], Loss: 1.7328\n",
      "Epoch [63/300], Step [2/42], Loss: 1.8320\n",
      "Epoch [63/300], Step [3/42], Loss: 1.4461\n",
      "Epoch [63/300], Step [4/42], Loss: 0.9821\n",
      "Epoch [63/300], Step [5/42], Loss: 1.3475\n",
      "Epoch [63/300], Step [6/42], Loss: 1.6544\n",
      "Epoch [63/300], Step [7/42], Loss: 2.0663\n",
      "Epoch [63/300], Step [8/42], Loss: 5.2200\n",
      "Epoch [63/300], Step [9/42], Loss: 1.5909\n",
      "Epoch [63/300], Step [10/42], Loss: 1.4726\n",
      "Epoch [63/300], Step [11/42], Loss: 3.4592\n",
      "Epoch [63/300], Step [12/42], Loss: 2.1001\n",
      "Epoch [63/300], Step [13/42], Loss: 1.2104\n",
      "Epoch [63/300], Step [14/42], Loss: 1.0164\n",
      "Epoch [63/300], Step [15/42], Loss: 1.8771\n",
      "Epoch [63/300], Step [16/42], Loss: 1.6466\n",
      "Epoch [63/300], Step [17/42], Loss: 2.4089\n",
      "Epoch [63/300], Step [18/42], Loss: 3.7541\n",
      "Epoch [63/300], Step [19/42], Loss: 2.1490\n",
      "Epoch [63/300], Step [20/42], Loss: 1.0463\n",
      "Epoch [63/300], Step [21/42], Loss: 7.4040\n",
      "Epoch [63/300], Step [22/42], Loss: 0.9032\n",
      "Epoch [63/300], Step [23/42], Loss: 1.3097\n",
      "Epoch [63/300], Step [24/42], Loss: 1.1452\n",
      "Epoch [63/300], Step [25/42], Loss: 4.3251\n",
      "Epoch [63/300], Step [26/42], Loss: 1.8744\n",
      "Epoch [63/300], Step [27/42], Loss: 1.5849\n",
      "Epoch [63/300], Step [28/42], Loss: 0.6244\n",
      "Epoch [63/300], Step [29/42], Loss: 2.0126\n",
      "Epoch [63/300], Step [30/42], Loss: 1.1665\n",
      "Epoch [63/300], Step [31/42], Loss: 2.0907\n",
      "Epoch [63/300], Step [32/42], Loss: 1.4517\n",
      "Epoch [63/300], Step [33/42], Loss: 3.5938\n",
      "Epoch [63/300], Step [34/42], Loss: 1.9856\n",
      "Epoch [63/300], Step [35/42], Loss: 1.8588\n",
      "Epoch [63/300], Step [36/42], Loss: 4.4787\n",
      "Epoch [63/300], Step [37/42], Loss: 3.4170\n",
      "Epoch [63/300], Step [38/42], Loss: 3.2130\n",
      "Epoch [63/300], Step [39/42], Loss: 4.0617\n",
      "Epoch [63/300], Step [40/42], Loss: 1.3822\n",
      "Epoch [63/300], Step [41/42], Loss: 2.1971\n",
      "Epoch [63/300], Step [42/42], Loss: 0.2150\n",
      "Val. loss :2.9208\n",
      "Epoch [64/300], Step [1/42], Loss: 3.1857\n",
      "Epoch [64/300], Step [2/42], Loss: 4.6741\n",
      "Epoch [64/300], Step [3/42], Loss: 2.7883\n",
      "Epoch [64/300], Step [4/42], Loss: 2.1900\n",
      "Epoch [64/300], Step [5/42], Loss: 1.2188\n",
      "Epoch [64/300], Step [6/42], Loss: 3.5790\n",
      "Epoch [64/300], Step [7/42], Loss: 3.2718\n",
      "Epoch [64/300], Step [8/42], Loss: 2.5183\n",
      "Epoch [64/300], Step [9/42], Loss: 4.5522\n",
      "Epoch [64/300], Step [10/42], Loss: 2.0971\n",
      "Epoch [64/300], Step [11/42], Loss: 3.0929\n",
      "Epoch [64/300], Step [12/42], Loss: 0.8007\n",
      "Epoch [64/300], Step [13/42], Loss: 2.9577\n",
      "Epoch [64/300], Step [14/42], Loss: 8.4349\n",
      "Epoch [64/300], Step [15/42], Loss: 0.8833\n",
      "Epoch [64/300], Step [16/42], Loss: 6.8751\n",
      "Epoch [64/300], Step [17/42], Loss: 2.1772\n",
      "Epoch [64/300], Step [18/42], Loss: 1.4147\n",
      "Epoch [64/300], Step [19/42], Loss: 1.7887\n",
      "Epoch [64/300], Step [20/42], Loss: 1.2636\n",
      "Epoch [64/300], Step [21/42], Loss: 1.5771\n",
      "Epoch [64/300], Step [22/42], Loss: 2.0942\n",
      "Epoch [64/300], Step [23/42], Loss: 2.1309\n",
      "Epoch [64/300], Step [24/42], Loss: 6.2690\n",
      "Epoch [64/300], Step [25/42], Loss: 0.8445\n",
      "Epoch [64/300], Step [26/42], Loss: 1.2699\n",
      "Epoch [64/300], Step [27/42], Loss: 1.2351\n",
      "Epoch [64/300], Step [28/42], Loss: 0.6244\n",
      "Epoch [64/300], Step [29/42], Loss: 0.6966\n",
      "Epoch [64/300], Step [30/42], Loss: 1.4031\n",
      "Epoch [64/300], Step [31/42], Loss: 1.9387\n",
      "Epoch [64/300], Step [32/42], Loss: 3.4444\n",
      "Epoch [64/300], Step [33/42], Loss: 2.1148\n",
      "Epoch [64/300], Step [34/42], Loss: 2.5020\n",
      "Epoch [64/300], Step [35/42], Loss: 4.5762\n",
      "Epoch [64/300], Step [36/42], Loss: 1.6743\n",
      "Epoch [64/300], Step [37/42], Loss: 1.6012\n",
      "Epoch [64/300], Step [38/42], Loss: 1.5796\n",
      "Epoch [64/300], Step [39/42], Loss: 1.1083\n",
      "Epoch [64/300], Step [40/42], Loss: 1.3263\n",
      "Epoch [64/300], Step [41/42], Loss: 1.1564\n",
      "Epoch [64/300], Step [42/42], Loss: 0.6010\n",
      "Val. loss :3.0209\n",
      "Epoch [65/300], Step [1/42], Loss: 3.4169\n",
      "Epoch [65/300], Step [2/42], Loss: 3.9697\n",
      "Epoch [65/300], Step [3/42], Loss: 1.5884\n",
      "Epoch [65/300], Step [4/42], Loss: 4.9088\n",
      "Epoch [65/300], Step [5/42], Loss: 1.1720\n",
      "Epoch [65/300], Step [6/42], Loss: 0.7479\n",
      "Epoch [65/300], Step [7/42], Loss: 0.9071\n",
      "Epoch [65/300], Step [8/42], Loss: 0.7383\n",
      "Epoch [65/300], Step [9/42], Loss: 1.9681\n",
      "Epoch [65/300], Step [10/42], Loss: 3.9095\n",
      "Epoch [65/300], Step [11/42], Loss: 1.0388\n",
      "Epoch [65/300], Step [12/42], Loss: 2.4897\n",
      "Epoch [65/300], Step [13/42], Loss: 1.3251\n",
      "Epoch [65/300], Step [14/42], Loss: 0.7394\n",
      "Epoch [65/300], Step [15/42], Loss: 2.7505\n",
      "Epoch [65/300], Step [16/42], Loss: 1.8385\n",
      "Epoch [65/300], Step [17/42], Loss: 1.8405\n",
      "Epoch [65/300], Step [18/42], Loss: 1.9965\n",
      "Epoch [65/300], Step [19/42], Loss: 1.5831\n",
      "Epoch [65/300], Step [20/42], Loss: 0.8456\n",
      "Epoch [65/300], Step [21/42], Loss: 2.3715\n",
      "Epoch [65/300], Step [22/42], Loss: 0.8590\n",
      "Epoch [65/300], Step [23/42], Loss: 1.5631\n",
      "Epoch [65/300], Step [24/42], Loss: 1.4626\n",
      "Epoch [65/300], Step [25/42], Loss: 0.6724\n",
      "Epoch [65/300], Step [26/42], Loss: 1.0406\n",
      "Epoch [65/300], Step [27/42], Loss: 1.4678\n",
      "Epoch [65/300], Step [28/42], Loss: 1.9896\n",
      "Epoch [65/300], Step [29/42], Loss: 2.8416\n",
      "Epoch [65/300], Step [30/42], Loss: 2.0299\n",
      "Epoch [65/300], Step [31/42], Loss: 1.7370\n",
      "Epoch [65/300], Step [32/42], Loss: 1.3201\n",
      "Epoch [65/300], Step [33/42], Loss: 0.8476\n",
      "Epoch [65/300], Step [34/42], Loss: 2.2231\n",
      "Epoch [65/300], Step [35/42], Loss: 1.0079\n",
      "Epoch [65/300], Step [36/42], Loss: 0.8840\n",
      "Epoch [65/300], Step [37/42], Loss: 0.7441\n",
      "Epoch [65/300], Step [38/42], Loss: 1.5591\n",
      "Epoch [65/300], Step [39/42], Loss: 1.5126\n",
      "Epoch [65/300], Step [40/42], Loss: 2.1830\n",
      "Epoch [65/300], Step [41/42], Loss: 2.1648\n",
      "Epoch [65/300], Step [42/42], Loss: 0.1233\n",
      "Val. loss :2.4033\n",
      "Epoch [66/300], Step [1/42], Loss: 1.5601\n",
      "Epoch [66/300], Step [2/42], Loss: 1.5541\n",
      "Epoch [66/300], Step [3/42], Loss: 2.0678\n",
      "Epoch [66/300], Step [4/42], Loss: 2.5439\n",
      "Epoch [66/300], Step [5/42], Loss: 0.8136\n",
      "Epoch [66/300], Step [6/42], Loss: 1.1663\n",
      "Epoch [66/300], Step [7/42], Loss: 1.7592\n",
      "Epoch [66/300], Step [8/42], Loss: 1.1886\n",
      "Epoch [66/300], Step [9/42], Loss: 1.1083\n",
      "Epoch [66/300], Step [10/42], Loss: 2.1434\n",
      "Epoch [66/300], Step [11/42], Loss: 1.3857\n",
      "Epoch [66/300], Step [12/42], Loss: 1.5767\n",
      "Epoch [66/300], Step [13/42], Loss: 2.4414\n",
      "Epoch [66/300], Step [14/42], Loss: 1.0692\n",
      "Epoch [66/300], Step [15/42], Loss: 0.6202\n",
      "Epoch [66/300], Step [16/42], Loss: 2.0376\n",
      "Epoch [66/300], Step [17/42], Loss: 0.7333\n",
      "Epoch [66/300], Step [18/42], Loss: 1.2121\n",
      "Epoch [66/300], Step [19/42], Loss: 3.4684\n",
      "Epoch [66/300], Step [20/42], Loss: 3.6403\n",
      "Epoch [66/300], Step [21/42], Loss: 2.4914\n",
      "Epoch [66/300], Step [22/42], Loss: 1.0442\n",
      "Epoch [66/300], Step [23/42], Loss: 1.3235\n",
      "Epoch [66/300], Step [24/42], Loss: 0.7235\n",
      "Epoch [66/300], Step [25/42], Loss: 0.8095\n",
      "Epoch [66/300], Step [26/42], Loss: 1.5329\n",
      "Epoch [66/300], Step [27/42], Loss: 2.8330\n",
      "Epoch [66/300], Step [28/42], Loss: 4.2463\n",
      "Epoch [66/300], Step [29/42], Loss: 0.9968\n",
      "Epoch [66/300], Step [30/42], Loss: 0.8192\n",
      "Epoch [66/300], Step [31/42], Loss: 2.1077\n",
      "Epoch [66/300], Step [32/42], Loss: 0.9639\n",
      "Epoch [66/300], Step [33/42], Loss: 1.4172\n",
      "Epoch [66/300], Step [34/42], Loss: 1.2472\n",
      "Epoch [66/300], Step [35/42], Loss: 0.8851\n",
      "Epoch [66/300], Step [36/42], Loss: 0.8211\n",
      "Epoch [66/300], Step [37/42], Loss: 1.6231\n",
      "Epoch [66/300], Step [38/42], Loss: 2.1997\n",
      "Epoch [66/300], Step [39/42], Loss: 2.3461\n",
      "Epoch [66/300], Step [40/42], Loss: 4.4744\n",
      "Epoch [66/300], Step [41/42], Loss: 0.5629\n",
      "Epoch [66/300], Step [42/42], Loss: 0.6389\n",
      "Val. loss :1.8644\n",
      "Epoch [67/300], Step [1/42], Loss: 1.6698\n",
      "Epoch [67/300], Step [2/42], Loss: 1.4950\n",
      "Epoch [67/300], Step [3/42], Loss: 1.3286\n",
      "Epoch [67/300], Step [4/42], Loss: 2.7583\n",
      "Epoch [67/300], Step [5/42], Loss: 1.0460\n",
      "Epoch [67/300], Step [6/42], Loss: 2.7738\n",
      "Epoch [67/300], Step [7/42], Loss: 1.0093\n",
      "Epoch [67/300], Step [8/42], Loss: 0.6596\n",
      "Epoch [67/300], Step [9/42], Loss: 1.0097\n",
      "Epoch [67/300], Step [10/42], Loss: 0.5933\n",
      "Epoch [67/300], Step [11/42], Loss: 2.8242\n",
      "Epoch [67/300], Step [12/42], Loss: 2.6309\n",
      "Epoch [67/300], Step [13/42], Loss: 1.4411\n",
      "Epoch [67/300], Step [14/42], Loss: 1.1141\n",
      "Epoch [67/300], Step [15/42], Loss: 1.4322\n",
      "Epoch [67/300], Step [16/42], Loss: 0.6299\n",
      "Epoch [67/300], Step [17/42], Loss: 2.7931\n",
      "Epoch [67/300], Step [18/42], Loss: 0.9355\n",
      "Epoch [67/300], Step [19/42], Loss: 3.0566\n",
      "Epoch [67/300], Step [20/42], Loss: 1.9978\n",
      "Epoch [67/300], Step [21/42], Loss: 1.7475\n",
      "Epoch [67/300], Step [22/42], Loss: 2.5432\n",
      "Epoch [67/300], Step [23/42], Loss: 1.5075\n",
      "Epoch [67/300], Step [24/42], Loss: 1.5591\n",
      "Epoch [67/300], Step [25/42], Loss: 1.5550\n",
      "Epoch [67/300], Step [26/42], Loss: 1.9480\n",
      "Epoch [67/300], Step [27/42], Loss: 1.4383\n",
      "Epoch [67/300], Step [28/42], Loss: 4.3762\n",
      "Epoch [67/300], Step [29/42], Loss: 1.3552\n",
      "Epoch [67/300], Step [30/42], Loss: 0.6368\n",
      "Epoch [67/300], Step [31/42], Loss: 2.0706\n",
      "Epoch [67/300], Step [32/42], Loss: 0.8084\n",
      "Epoch [67/300], Step [33/42], Loss: 1.4497\n",
      "Epoch [67/300], Step [34/42], Loss: 1.8971\n",
      "Epoch [67/300], Step [35/42], Loss: 0.8365\n",
      "Epoch [67/300], Step [36/42], Loss: 0.4790\n",
      "Epoch [67/300], Step [37/42], Loss: 2.0806\n",
      "Epoch [67/300], Step [38/42], Loss: 1.6245\n",
      "Epoch [67/300], Step [39/42], Loss: 0.8603\n",
      "Epoch [67/300], Step [40/42], Loss: 1.3128\n",
      "Epoch [67/300], Step [41/42], Loss: 0.9342\n",
      "Epoch [67/300], Step [42/42], Loss: 0.7247\n",
      "Val. loss :1.7955\n",
      "Epoch [68/300], Step [1/42], Loss: 1.6068\n",
      "Epoch [68/300], Step [2/42], Loss: 1.8922\n",
      "Epoch [68/300], Step [3/42], Loss: 1.9499\n",
      "Epoch [68/300], Step [4/42], Loss: 2.3450\n",
      "Epoch [68/300], Step [5/42], Loss: 1.4526\n",
      "Epoch [68/300], Step [6/42], Loss: 4.5044\n",
      "Epoch [68/300], Step [7/42], Loss: 1.9380\n",
      "Epoch [68/300], Step [8/42], Loss: 1.2966\n",
      "Epoch [68/300], Step [9/42], Loss: 1.3592\n",
      "Epoch [68/300], Step [10/42], Loss: 2.7405\n",
      "Epoch [68/300], Step [11/42], Loss: 1.0130\n",
      "Epoch [68/300], Step [12/42], Loss: 0.7640\n",
      "Epoch [68/300], Step [13/42], Loss: 0.6794\n",
      "Epoch [68/300], Step [14/42], Loss: 1.1142\n",
      "Epoch [68/300], Step [15/42], Loss: 0.5714\n",
      "Epoch [68/300], Step [16/42], Loss: 1.5083\n",
      "Epoch [68/300], Step [17/42], Loss: 1.1218\n",
      "Epoch [68/300], Step [18/42], Loss: 1.9590\n",
      "Epoch [68/300], Step [19/42], Loss: 1.7453\n",
      "Epoch [68/300], Step [20/42], Loss: 1.5568\n",
      "Epoch [68/300], Step [21/42], Loss: 2.4653\n",
      "Epoch [68/300], Step [22/42], Loss: 3.1311\n",
      "Epoch [68/300], Step [23/42], Loss: 1.1870\n",
      "Epoch [68/300], Step [24/42], Loss: 2.3392\n",
      "Epoch [68/300], Step [25/42], Loss: 1.3724\n",
      "Epoch [68/300], Step [26/42], Loss: 0.8998\n",
      "Epoch [68/300], Step [27/42], Loss: 1.0706\n",
      "Epoch [68/300], Step [28/42], Loss: 10.5554\n",
      "Epoch [68/300], Step [29/42], Loss: 1.4198\n",
      "Epoch [68/300], Step [30/42], Loss: 1.8414\n",
      "Epoch [68/300], Step [31/42], Loss: 2.7764\n",
      "Epoch [68/300], Step [32/42], Loss: 5.6403\n",
      "Epoch [68/300], Step [33/42], Loss: 4.3630\n",
      "Epoch [68/300], Step [34/42], Loss: 3.3975\n",
      "Epoch [68/300], Step [35/42], Loss: 4.4669\n",
      "Epoch [68/300], Step [36/42], Loss: 1.0405\n",
      "Epoch [68/300], Step [37/42], Loss: 8.2235\n",
      "Epoch [68/300], Step [38/42], Loss: 1.4911\n",
      "Epoch [68/300], Step [39/42], Loss: 1.3473\n",
      "Epoch [68/300], Step [40/42], Loss: 1.8800\n",
      "Epoch [68/300], Step [41/42], Loss: 2.9766\n",
      "Epoch [68/300], Step [42/42], Loss: 0.2441\n",
      "Val. loss :4.7794\n",
      "Epoch [69/300], Step [1/42], Loss: 3.7971\n",
      "Epoch [69/300], Step [2/42], Loss: 1.6125\n",
      "Epoch [69/300], Step [3/42], Loss: 3.4352\n",
      "Epoch [69/300], Step [4/42], Loss: 5.6162\n",
      "Epoch [69/300], Step [5/42], Loss: 0.6839\n",
      "Epoch [69/300], Step [6/42], Loss: 1.2123\n",
      "Epoch [69/300], Step [7/42], Loss: 2.3968\n",
      "Epoch [69/300], Step [8/42], Loss: 4.7202\n",
      "Epoch [69/300], Step [9/42], Loss: 1.8813\n",
      "Epoch [69/300], Step [10/42], Loss: 1.8468\n",
      "Epoch [69/300], Step [11/42], Loss: 1.8929\n",
      "Epoch [69/300], Step [12/42], Loss: 1.6117\n",
      "Epoch [69/300], Step [13/42], Loss: 2.0761\n",
      "Epoch [69/300], Step [14/42], Loss: 2.2464\n",
      "Epoch [69/300], Step [15/42], Loss: 2.5681\n",
      "Epoch [69/300], Step [16/42], Loss: 1.1257\n",
      "Epoch [69/300], Step [17/42], Loss: 5.2897\n",
      "Epoch [69/300], Step [18/42], Loss: 2.5085\n",
      "Epoch [69/300], Step [19/42], Loss: 1.2114\n",
      "Epoch [69/300], Step [20/42], Loss: 1.5260\n",
      "Epoch [69/300], Step [21/42], Loss: 1.2555\n",
      "Epoch [69/300], Step [22/42], Loss: 0.7059\n",
      "Epoch [69/300], Step [23/42], Loss: 1.0528\n",
      "Epoch [69/300], Step [24/42], Loss: 1.4046\n",
      "Epoch [69/300], Step [25/42], Loss: 3.3838\n",
      "Epoch [69/300], Step [26/42], Loss: 1.5002\n",
      "Epoch [69/300], Step [27/42], Loss: 2.7809\n",
      "Epoch [69/300], Step [28/42], Loss: 1.1485\n",
      "Epoch [69/300], Step [29/42], Loss: 1.7335\n",
      "Epoch [69/300], Step [30/42], Loss: 1.3809\n",
      "Epoch [69/300], Step [31/42], Loss: 1.3566\n",
      "Epoch [69/300], Step [32/42], Loss: 1.3362\n",
      "Epoch [69/300], Step [33/42], Loss: 0.6654\n",
      "Epoch [69/300], Step [34/42], Loss: 1.3125\n",
      "Epoch [69/300], Step [35/42], Loss: 2.1450\n",
      "Epoch [69/300], Step [36/42], Loss: 2.4772\n",
      "Epoch [69/300], Step [37/42], Loss: 2.9105\n",
      "Epoch [69/300], Step [38/42], Loss: 1.9112\n",
      "Epoch [69/300], Step [39/42], Loss: 1.7142\n",
      "Epoch [69/300], Step [40/42], Loss: 1.7469\n",
      "Epoch [69/300], Step [41/42], Loss: 1.1077\n",
      "Epoch [69/300], Step [42/42], Loss: 0.1728\n",
      "Val. loss :2.5284\n",
      "Epoch [70/300], Step [1/42], Loss: 2.0409\n",
      "Epoch [70/300], Step [2/42], Loss: 1.6335\n",
      "Epoch [70/300], Step [3/42], Loss: 1.2279\n",
      "Epoch [70/300], Step [4/42], Loss: 1.0246\n",
      "Epoch [70/300], Step [5/42], Loss: 2.1778\n",
      "Epoch [70/300], Step [6/42], Loss: 1.5425\n",
      "Epoch [70/300], Step [7/42], Loss: 1.2092\n",
      "Epoch [70/300], Step [8/42], Loss: 0.5115\n",
      "Epoch [70/300], Step [9/42], Loss: 1.2727\n",
      "Epoch [70/300], Step [10/42], Loss: 2.0459\n",
      "Epoch [70/300], Step [11/42], Loss: 1.0084\n",
      "Epoch [70/300], Step [12/42], Loss: 1.4566\n",
      "Epoch [70/300], Step [13/42], Loss: 1.8554\n",
      "Epoch [70/300], Step [14/42], Loss: 0.8542\n",
      "Epoch [70/300], Step [15/42], Loss: 1.1707\n",
      "Epoch [70/300], Step [16/42], Loss: 1.9692\n",
      "Epoch [70/300], Step [17/42], Loss: 1.3317\n",
      "Epoch [70/300], Step [18/42], Loss: 0.6150\n",
      "Epoch [70/300], Step [19/42], Loss: 0.5016\n",
      "Epoch [70/300], Step [20/42], Loss: 4.4503\n",
      "Epoch [70/300], Step [21/42], Loss: 2.1705\n",
      "Epoch [70/300], Step [22/42], Loss: 1.5773\n",
      "Epoch [70/300], Step [23/42], Loss: 1.2548\n",
      "Epoch [70/300], Step [24/42], Loss: 1.8568\n",
      "Epoch [70/300], Step [25/42], Loss: 3.4433\n",
      "Epoch [70/300], Step [26/42], Loss: 0.6389\n",
      "Epoch [70/300], Step [27/42], Loss: 1.4616\n",
      "Epoch [70/300], Step [28/42], Loss: 1.2682\n",
      "Epoch [70/300], Step [29/42], Loss: 1.3779\n",
      "Epoch [70/300], Step [30/42], Loss: 0.9982\n",
      "Epoch [70/300], Step [31/42], Loss: 2.7008\n",
      "Epoch [70/300], Step [32/42], Loss: 1.8255\n",
      "Epoch [70/300], Step [33/42], Loss: 1.3088\n",
      "Epoch [70/300], Step [34/42], Loss: 0.7578\n",
      "Epoch [70/300], Step [35/42], Loss: 2.4296\n",
      "Epoch [70/300], Step [36/42], Loss: 0.8557\n",
      "Epoch [70/300], Step [37/42], Loss: 2.3017\n",
      "Epoch [70/300], Step [38/42], Loss: 3.3690\n",
      "Epoch [70/300], Step [39/42], Loss: 4.5023\n",
      "Epoch [70/300], Step [40/42], Loss: 0.6889\n",
      "Epoch [70/300], Step [41/42], Loss: 1.6463\n",
      "Epoch [70/300], Step [42/42], Loss: 0.1916\n",
      "Val. loss :2.0640\n",
      "Epoch [71/300], Step [1/42], Loss: 1.0354\n",
      "Epoch [71/300], Step [2/42], Loss: 0.9388\n",
      "Epoch [71/300], Step [3/42], Loss: 2.8881\n",
      "Epoch [71/300], Step [4/42], Loss: 1.6084\n",
      "Epoch [71/300], Step [5/42], Loss: 1.4756\n",
      "Epoch [71/300], Step [6/42], Loss: 1.3541\n",
      "Epoch [71/300], Step [7/42], Loss: 1.4776\n",
      "Epoch [71/300], Step [8/42], Loss: 0.5416\n",
      "Epoch [71/300], Step [9/42], Loss: 1.6859\n",
      "Epoch [71/300], Step [10/42], Loss: 0.8526\n",
      "Epoch [71/300], Step [11/42], Loss: 2.7351\n",
      "Epoch [71/300], Step [12/42], Loss: 1.4398\n",
      "Epoch [71/300], Step [13/42], Loss: 0.6199\n",
      "Epoch [71/300], Step [14/42], Loss: 1.0231\n",
      "Epoch [71/300], Step [15/42], Loss: 2.8633\n",
      "Epoch [71/300], Step [16/42], Loss: 0.9773\n",
      "Epoch [71/300], Step [17/42], Loss: 1.0890\n",
      "Epoch [71/300], Step [18/42], Loss: 2.8660\n",
      "Epoch [71/300], Step [19/42], Loss: 0.9524\n",
      "Epoch [71/300], Step [20/42], Loss: 1.0150\n",
      "Epoch [71/300], Step [21/42], Loss: 1.7815\n",
      "Epoch [71/300], Step [22/42], Loss: 2.0399\n",
      "Epoch [71/300], Step [23/42], Loss: 1.2068\n",
      "Epoch [71/300], Step [24/42], Loss: 3.8966\n",
      "Epoch [71/300], Step [25/42], Loss: 2.6416\n",
      "Epoch [71/300], Step [26/42], Loss: 1.2626\n",
      "Epoch [71/300], Step [27/42], Loss: 0.9828\n",
      "Epoch [71/300], Step [28/42], Loss: 1.0433\n",
      "Epoch [71/300], Step [29/42], Loss: 0.6947\n",
      "Epoch [71/300], Step [30/42], Loss: 1.9175\n",
      "Epoch [71/300], Step [31/42], Loss: 1.7316\n",
      "Epoch [71/300], Step [32/42], Loss: 2.1997\n",
      "Epoch [71/300], Step [33/42], Loss: 0.5785\n",
      "Epoch [71/300], Step [34/42], Loss: 3.1557\n",
      "Epoch [71/300], Step [35/42], Loss: 0.7948\n",
      "Epoch [71/300], Step [36/42], Loss: 1.6773\n",
      "Epoch [71/300], Step [37/42], Loss: 2.7828\n",
      "Epoch [71/300], Step [38/42], Loss: 1.6578\n",
      "Epoch [71/300], Step [39/42], Loss: 1.4181\n",
      "Epoch [71/300], Step [40/42], Loss: 0.9714\n",
      "Epoch [71/300], Step [41/42], Loss: 1.2682\n",
      "Epoch [71/300], Step [42/42], Loss: 0.3017\n",
      "Val. loss :2.0550\n",
      "Epoch [72/300], Step [1/42], Loss: 1.3385\n",
      "Epoch [72/300], Step [2/42], Loss: 1.7744\n",
      "Epoch [72/300], Step [3/42], Loss: 1.1149\n",
      "Epoch [72/300], Step [4/42], Loss: 1.2592\n",
      "Epoch [72/300], Step [5/42], Loss: 1.1472\n",
      "Epoch [72/300], Step [6/42], Loss: 1.1122\n",
      "Epoch [72/300], Step [7/42], Loss: 1.4048\n",
      "Epoch [72/300], Step [8/42], Loss: 1.8701\n",
      "Epoch [72/300], Step [9/42], Loss: 0.8339\n",
      "Epoch [72/300], Step [10/42], Loss: 0.6529\n",
      "Epoch [72/300], Step [11/42], Loss: 1.1519\n",
      "Epoch [72/300], Step [12/42], Loss: 0.8116\n",
      "Epoch [72/300], Step [13/42], Loss: 0.9367\n",
      "Epoch [72/300], Step [14/42], Loss: 2.5457\n",
      "Epoch [72/300], Step [15/42], Loss: 2.0482\n",
      "Epoch [72/300], Step [16/42], Loss: 0.7047\n",
      "Epoch [72/300], Step [17/42], Loss: 0.6555\n",
      "Epoch [72/300], Step [18/42], Loss: 1.3836\n",
      "Epoch [72/300], Step [19/42], Loss: 2.1347\n",
      "Epoch [72/300], Step [20/42], Loss: 2.2392\n",
      "Epoch [72/300], Step [21/42], Loss: 3.6583\n",
      "Epoch [72/300], Step [22/42], Loss: 2.1754\n",
      "Epoch [72/300], Step [23/42], Loss: 4.9088\n",
      "Epoch [72/300], Step [24/42], Loss: 0.9569\n",
      "Epoch [72/300], Step [25/42], Loss: 1.7618\n",
      "Epoch [72/300], Step [26/42], Loss: 1.5535\n",
      "Epoch [72/300], Step [27/42], Loss: 0.9463\n",
      "Epoch [72/300], Step [28/42], Loss: 1.1515\n",
      "Epoch [72/300], Step [29/42], Loss: 0.6148\n",
      "Epoch [72/300], Step [30/42], Loss: 1.3851\n",
      "Epoch [72/300], Step [31/42], Loss: 1.2168\n",
      "Epoch [72/300], Step [32/42], Loss: 0.5836\n",
      "Epoch [72/300], Step [33/42], Loss: 2.5725\n",
      "Epoch [72/300], Step [34/42], Loss: 0.7311\n",
      "Epoch [72/300], Step [35/42], Loss: 2.0182\n",
      "Epoch [72/300], Step [36/42], Loss: 1.8057\n",
      "Epoch [72/300], Step [37/42], Loss: 1.0255\n",
      "Epoch [72/300], Step [38/42], Loss: 1.0397\n",
      "Epoch [72/300], Step [39/42], Loss: 1.4523\n",
      "Epoch [72/300], Step [40/42], Loss: 1.6471\n",
      "Epoch [72/300], Step [41/42], Loss: 1.7177\n",
      "Epoch [72/300], Step [42/42], Loss: 0.0841\n",
      "Val. loss :2.1572\n",
      "Epoch [73/300], Step [1/42], Loss: 1.9390\n",
      "Epoch [73/300], Step [2/42], Loss: 1.9663\n",
      "Epoch [73/300], Step [3/42], Loss: 1.2103\n",
      "Epoch [73/300], Step [4/42], Loss: 2.7608\n",
      "Epoch [73/300], Step [5/42], Loss: 2.5546\n",
      "Epoch [73/300], Step [6/42], Loss: 1.5451\n",
      "Epoch [73/300], Step [7/42], Loss: 1.3132\n",
      "Epoch [73/300], Step [8/42], Loss: 0.8343\n",
      "Epoch [73/300], Step [9/42], Loss: 0.6283\n",
      "Epoch [73/300], Step [10/42], Loss: 1.1377\n",
      "Epoch [73/300], Step [11/42], Loss: 2.4929\n",
      "Epoch [73/300], Step [12/42], Loss: 2.4430\n",
      "Epoch [73/300], Step [13/42], Loss: 0.7613\n",
      "Epoch [73/300], Step [14/42], Loss: 0.7347\n",
      "Epoch [73/300], Step [15/42], Loss: 1.2949\n",
      "Epoch [73/300], Step [16/42], Loss: 1.9114\n",
      "Epoch [73/300], Step [17/42], Loss: 1.2545\n",
      "Epoch [73/300], Step [18/42], Loss: 0.8781\n",
      "Epoch [73/300], Step [19/42], Loss: 2.6133\n",
      "Epoch [73/300], Step [20/42], Loss: 0.7868\n",
      "Epoch [73/300], Step [21/42], Loss: 1.8861\n",
      "Epoch [73/300], Step [22/42], Loss: 1.3415\n",
      "Epoch [73/300], Step [23/42], Loss: 0.4840\n",
      "Epoch [73/300], Step [24/42], Loss: 0.6801\n",
      "Epoch [73/300], Step [25/42], Loss: 1.3583\n",
      "Epoch [73/300], Step [26/42], Loss: 1.4196\n",
      "Epoch [73/300], Step [27/42], Loss: 0.6609\n",
      "Epoch [73/300], Step [28/42], Loss: 2.8669\n",
      "Epoch [73/300], Step [29/42], Loss: 0.4941\n",
      "Epoch [73/300], Step [30/42], Loss: 1.0995\n",
      "Epoch [73/300], Step [31/42], Loss: 1.5013\n",
      "Epoch [73/300], Step [32/42], Loss: 5.4795\n",
      "Epoch [73/300], Step [33/42], Loss: 2.1241\n",
      "Epoch [73/300], Step [34/42], Loss: 2.3536\n",
      "Epoch [73/300], Step [35/42], Loss: 1.0626\n",
      "Epoch [73/300], Step [36/42], Loss: 0.7346\n",
      "Epoch [73/300], Step [37/42], Loss: 5.4377\n",
      "Epoch [73/300], Step [38/42], Loss: 1.9962\n",
      "Epoch [73/300], Step [39/42], Loss: 1.8062\n",
      "Epoch [73/300], Step [40/42], Loss: 0.6913\n",
      "Epoch [73/300], Step [41/42], Loss: 8.2870\n",
      "Epoch [73/300], Step [42/42], Loss: 0.1389\n",
      "Val. loss :1.8390\n",
      "Epoch [74/300], Step [1/42], Loss: 0.7972\n",
      "Epoch [74/300], Step [2/42], Loss: 1.9040\n",
      "Epoch [74/300], Step [3/42], Loss: 0.8350\n",
      "Epoch [74/300], Step [4/42], Loss: 1.4580\n",
      "Epoch [74/300], Step [5/42], Loss: 5.6876\n",
      "Epoch [74/300], Step [6/42], Loss: 6.4742\n",
      "Epoch [74/300], Step [7/42], Loss: 1.1005\n",
      "Epoch [74/300], Step [8/42], Loss: 1.5546\n",
      "Epoch [74/300], Step [9/42], Loss: 3.5968\n",
      "Epoch [74/300], Step [10/42], Loss: 0.6176\n",
      "Epoch [74/300], Step [11/42], Loss: 3.5046\n",
      "Epoch [74/300], Step [12/42], Loss: 2.0500\n",
      "Epoch [74/300], Step [13/42], Loss: 3.3641\n",
      "Epoch [74/300], Step [14/42], Loss: 1.3954\n",
      "Epoch [74/300], Step [15/42], Loss: 1.4343\n",
      "Epoch [74/300], Step [16/42], Loss: 1.1550\n",
      "Epoch [74/300], Step [17/42], Loss: 2.3742\n",
      "Epoch [74/300], Step [18/42], Loss: 1.2542\n",
      "Epoch [74/300], Step [19/42], Loss: 1.1725\n",
      "Epoch [74/300], Step [20/42], Loss: 2.6249\n",
      "Epoch [74/300], Step [21/42], Loss: 4.8367\n",
      "Epoch [74/300], Step [22/42], Loss: 0.8053\n",
      "Epoch [74/300], Step [23/42], Loss: 0.5126\n",
      "Epoch [74/300], Step [24/42], Loss: 1.2905\n",
      "Epoch [74/300], Step [25/42], Loss: 2.1237\n",
      "Epoch [74/300], Step [26/42], Loss: 1.9305\n",
      "Epoch [74/300], Step [27/42], Loss: 2.0541\n",
      "Epoch [74/300], Step [28/42], Loss: 10.1221\n",
      "Epoch [74/300], Step [29/42], Loss: 1.2530\n",
      "Epoch [74/300], Step [30/42], Loss: 1.4134\n",
      "Epoch [74/300], Step [31/42], Loss: 2.6823\n",
      "Epoch [74/300], Step [32/42], Loss: 3.3053\n",
      "Epoch [74/300], Step [33/42], Loss: 1.4653\n",
      "Epoch [74/300], Step [34/42], Loss: 0.8296\n",
      "Epoch [74/300], Step [35/42], Loss: 0.8505\n",
      "Epoch [74/300], Step [36/42], Loss: 5.1397\n",
      "Epoch [74/300], Step [37/42], Loss: 0.6800\n",
      "Epoch [74/300], Step [38/42], Loss: 1.3888\n",
      "Epoch [74/300], Step [39/42], Loss: 0.7078\n",
      "Epoch [74/300], Step [40/42], Loss: 1.2167\n",
      "Epoch [74/300], Step [41/42], Loss: 3.1538\n",
      "Epoch [74/300], Step [42/42], Loss: 0.1612\n",
      "Val. loss :1.9555\n",
      "Epoch [75/300], Step [1/42], Loss: 1.3391\n",
      "Epoch [75/300], Step [2/42], Loss: 2.6124\n",
      "Epoch [75/300], Step [3/42], Loss: 1.2142\n",
      "Epoch [75/300], Step [4/42], Loss: 2.0027\n",
      "Epoch [75/300], Step [5/42], Loss: 1.6546\n",
      "Epoch [75/300], Step [6/42], Loss: 0.8613\n",
      "Epoch [75/300], Step [7/42], Loss: 2.2459\n",
      "Epoch [75/300], Step [8/42], Loss: 1.3651\n",
      "Epoch [75/300], Step [9/42], Loss: 0.8448\n",
      "Epoch [75/300], Step [10/42], Loss: 0.6999\n",
      "Epoch [75/300], Step [11/42], Loss: 0.9541\n",
      "Epoch [75/300], Step [12/42], Loss: 1.1245\n",
      "Epoch [75/300], Step [13/42], Loss: 1.4598\n",
      "Epoch [75/300], Step [14/42], Loss: 3.8961\n",
      "Epoch [75/300], Step [15/42], Loss: 1.5610\n",
      "Epoch [75/300], Step [16/42], Loss: 0.9611\n",
      "Epoch [75/300], Step [17/42], Loss: 2.3326\n",
      "Epoch [75/300], Step [18/42], Loss: 1.6397\n",
      "Epoch [75/300], Step [19/42], Loss: 0.6575\n",
      "Epoch [75/300], Step [20/42], Loss: 0.4692\n",
      "Epoch [75/300], Step [21/42], Loss: 1.3557\n",
      "Epoch [75/300], Step [22/42], Loss: 1.9121\n",
      "Epoch [75/300], Step [23/42], Loss: 1.2408\n",
      "Epoch [75/300], Step [24/42], Loss: 1.3968\n",
      "Epoch [75/300], Step [25/42], Loss: 1.5888\n",
      "Epoch [75/300], Step [26/42], Loss: 1.2991\n",
      "Epoch [75/300], Step [27/42], Loss: 0.5499\n",
      "Epoch [75/300], Step [28/42], Loss: 0.4433\n",
      "Epoch [75/300], Step [29/42], Loss: 3.6845\n",
      "Epoch [75/300], Step [30/42], Loss: 0.5381\n",
      "Epoch [75/300], Step [31/42], Loss: 2.7101\n",
      "Epoch [75/300], Step [32/42], Loss: 1.4057\n",
      "Epoch [75/300], Step [33/42], Loss: 1.2788\n",
      "Epoch [75/300], Step [34/42], Loss: 1.6197\n",
      "Epoch [75/300], Step [35/42], Loss: 0.6833\n",
      "Epoch [75/300], Step [36/42], Loss: 3.8337\n",
      "Epoch [75/300], Step [37/42], Loss: 1.4397\n",
      "Epoch [75/300], Step [38/42], Loss: 0.9014\n",
      "Epoch [75/300], Step [39/42], Loss: 0.8522\n",
      "Epoch [75/300], Step [40/42], Loss: 1.0586\n",
      "Epoch [75/300], Step [41/42], Loss: 0.6837\n",
      "Epoch [75/300], Step [42/42], Loss: 0.1643\n",
      "Val. loss :1.3943\n",
      "Epoch [76/300], Step [1/42], Loss: 0.5289\n",
      "Epoch [76/300], Step [2/42], Loss: 0.7388\n",
      "Epoch [76/300], Step [3/42], Loss: 0.4491\n",
      "Epoch [76/300], Step [4/42], Loss: 0.9463\n",
      "Epoch [76/300], Step [5/42], Loss: 0.6746\n",
      "Epoch [76/300], Step [6/42], Loss: 1.1775\n",
      "Epoch [76/300], Step [7/42], Loss: 2.3596\n",
      "Epoch [76/300], Step [8/42], Loss: 0.7651\n",
      "Epoch [76/300], Step [9/42], Loss: 0.7315\n",
      "Epoch [76/300], Step [10/42], Loss: 1.6002\n",
      "Epoch [76/300], Step [11/42], Loss: 1.1019\n",
      "Epoch [76/300], Step [12/42], Loss: 1.1311\n",
      "Epoch [76/300], Step [13/42], Loss: 1.1816\n",
      "Epoch [76/300], Step [14/42], Loss: 2.4850\n",
      "Epoch [76/300], Step [15/42], Loss: 1.1450\n",
      "Epoch [76/300], Step [16/42], Loss: 1.4445\n",
      "Epoch [76/300], Step [17/42], Loss: 1.8667\n",
      "Epoch [76/300], Step [18/42], Loss: 1.4186\n",
      "Epoch [76/300], Step [19/42], Loss: 4.1899\n",
      "Epoch [76/300], Step [20/42], Loss: 0.7428\n",
      "Epoch [76/300], Step [21/42], Loss: 0.9378\n",
      "Epoch [76/300], Step [22/42], Loss: 1.4311\n",
      "Epoch [76/300], Step [23/42], Loss: 0.4679\n",
      "Epoch [76/300], Step [24/42], Loss: 2.6023\n",
      "Epoch [76/300], Step [25/42], Loss: 1.1764\n",
      "Epoch [76/300], Step [26/42], Loss: 1.6224\n",
      "Epoch [76/300], Step [27/42], Loss: 1.4078\n",
      "Epoch [76/300], Step [28/42], Loss: 0.5662\n",
      "Epoch [76/300], Step [29/42], Loss: 1.2636\n",
      "Epoch [76/300], Step [30/42], Loss: 1.7026\n",
      "Epoch [76/300], Step [31/42], Loss: 1.0520\n",
      "Epoch [76/300], Step [32/42], Loss: 1.6299\n",
      "Epoch [76/300], Step [33/42], Loss: 0.9926\n",
      "Epoch [76/300], Step [34/42], Loss: 1.5116\n",
      "Epoch [76/300], Step [35/42], Loss: 0.6293\n",
      "Epoch [76/300], Step [36/42], Loss: 1.6359\n",
      "Epoch [76/300], Step [37/42], Loss: 1.1615\n",
      "Epoch [76/300], Step [38/42], Loss: 1.3557\n",
      "Epoch [76/300], Step [39/42], Loss: 1.2584\n",
      "Epoch [76/300], Step [40/42], Loss: 2.8854\n",
      "Epoch [76/300], Step [41/42], Loss: 0.6638\n",
      "Epoch [76/300], Step [42/42], Loss: 0.1735\n",
      "Val. loss :1.6108\n",
      "Epoch [77/300], Step [1/42], Loss: 0.5488\n",
      "Epoch [77/300], Step [2/42], Loss: 1.0578\n",
      "Epoch [77/300], Step [3/42], Loss: 0.8424\n",
      "Epoch [77/300], Step [4/42], Loss: 1.9274\n",
      "Epoch [77/300], Step [5/42], Loss: 1.3450\n",
      "Epoch [77/300], Step [6/42], Loss: 0.4350\n",
      "Epoch [77/300], Step [7/42], Loss: 1.4703\n",
      "Epoch [77/300], Step [8/42], Loss: 1.4282\n",
      "Epoch [77/300], Step [9/42], Loss: 1.2035\n",
      "Epoch [77/300], Step [10/42], Loss: 1.4274\n",
      "Epoch [77/300], Step [11/42], Loss: 0.7357\n",
      "Epoch [77/300], Step [12/42], Loss: 1.6722\n",
      "Epoch [77/300], Step [13/42], Loss: 2.3845\n",
      "Epoch [77/300], Step [14/42], Loss: 5.8712\n",
      "Epoch [77/300], Step [15/42], Loss: 1.0395\n",
      "Epoch [77/300], Step [16/42], Loss: 0.5359\n",
      "Epoch [77/300], Step [17/42], Loss: 1.0692\n",
      "Epoch [77/300], Step [18/42], Loss: 1.1608\n",
      "Epoch [77/300], Step [19/42], Loss: 0.6344\n",
      "Epoch [77/300], Step [20/42], Loss: 0.6669\n",
      "Epoch [77/300], Step [21/42], Loss: 0.7796\n",
      "Epoch [77/300], Step [22/42], Loss: 0.5024\n",
      "Epoch [77/300], Step [23/42], Loss: 1.5845\n",
      "Epoch [77/300], Step [24/42], Loss: 1.9165\n",
      "Epoch [77/300], Step [25/42], Loss: 0.8701\n",
      "Epoch [77/300], Step [26/42], Loss: 0.7837\n",
      "Epoch [77/300], Step [27/42], Loss: 1.5671\n",
      "Epoch [77/300], Step [28/42], Loss: 2.0343\n",
      "Epoch [77/300], Step [29/42], Loss: 0.3092\n",
      "Epoch [77/300], Step [30/42], Loss: 1.0818\n",
      "Epoch [77/300], Step [31/42], Loss: 0.7234\n",
      "Epoch [77/300], Step [32/42], Loss: 0.8561\n",
      "Epoch [77/300], Step [33/42], Loss: 0.6973\n",
      "Epoch [77/300], Step [34/42], Loss: 1.9678\n",
      "Epoch [77/300], Step [35/42], Loss: 2.8798\n",
      "Epoch [77/300], Step [36/42], Loss: 0.6337\n",
      "Epoch [77/300], Step [37/42], Loss: 1.1460\n",
      "Epoch [77/300], Step [38/42], Loss: 1.6634\n",
      "Epoch [77/300], Step [39/42], Loss: 1.4690\n",
      "Epoch [77/300], Step [40/42], Loss: 0.7458\n",
      "Epoch [77/300], Step [41/42], Loss: 1.5128\n",
      "Epoch [77/300], Step [42/42], Loss: 0.1631\n",
      "Val. loss :1.5967\n",
      "Epoch [78/300], Step [1/42], Loss: 1.1576\n",
      "Epoch [78/300], Step [2/42], Loss: 0.9388\n",
      "Epoch [78/300], Step [3/42], Loss: 0.9993\n",
      "Epoch [78/300], Step [4/42], Loss: 0.6155\n",
      "Epoch [78/300], Step [5/42], Loss: 0.9978\n",
      "Epoch [78/300], Step [6/42], Loss: 2.1194\n",
      "Epoch [78/300], Step [7/42], Loss: 3.5990\n",
      "Epoch [78/300], Step [8/42], Loss: 1.2531\n",
      "Epoch [78/300], Step [9/42], Loss: 1.4507\n",
      "Epoch [78/300], Step [10/42], Loss: 0.9665\n",
      "Epoch [78/300], Step [11/42], Loss: 1.0055\n",
      "Epoch [78/300], Step [12/42], Loss: 0.7638\n",
      "Epoch [78/300], Step [13/42], Loss: 0.7386\n",
      "Epoch [78/300], Step [14/42], Loss: 0.8821\n",
      "Epoch [78/300], Step [15/42], Loss: 0.8139\n",
      "Epoch [78/300], Step [16/42], Loss: 0.9287\n",
      "Epoch [78/300], Step [17/42], Loss: 1.0561\n",
      "Epoch [78/300], Step [18/42], Loss: 0.8386\n",
      "Epoch [78/300], Step [19/42], Loss: 1.3464\n",
      "Epoch [78/300], Step [20/42], Loss: 0.6394\n",
      "Epoch [78/300], Step [21/42], Loss: 0.7527\n",
      "Epoch [78/300], Step [22/42], Loss: 0.4303\n",
      "Epoch [78/300], Step [23/42], Loss: 0.9514\n",
      "Epoch [78/300], Step [24/42], Loss: 1.2900\n",
      "Epoch [78/300], Step [25/42], Loss: 1.8534\n",
      "Epoch [78/300], Step [26/42], Loss: 0.6995\n",
      "Epoch [78/300], Step [27/42], Loss: 1.5428\n",
      "Epoch [78/300], Step [28/42], Loss: 1.7849\n",
      "Epoch [78/300], Step [29/42], Loss: 1.0713\n",
      "Epoch [78/300], Step [30/42], Loss: 1.3462\n",
      "Epoch [78/300], Step [31/42], Loss: 0.4345\n",
      "Epoch [78/300], Step [32/42], Loss: 5.4239\n",
      "Epoch [78/300], Step [33/42], Loss: 7.9074\n",
      "Epoch [78/300], Step [34/42], Loss: 0.4692\n",
      "Epoch [78/300], Step [35/42], Loss: 0.8231\n",
      "Epoch [78/300], Step [36/42], Loss: 1.4249\n",
      "Epoch [78/300], Step [37/42], Loss: 2.0411\n",
      "Epoch [78/300], Step [38/42], Loss: 1.5497\n",
      "Epoch [78/300], Step [39/42], Loss: 0.4179\n",
      "Epoch [78/300], Step [40/42], Loss: 0.6496\n",
      "Epoch [78/300], Step [41/42], Loss: 1.1720\n",
      "Epoch [78/300], Step [42/42], Loss: 0.2219\n",
      "Val. loss :2.1159\n",
      "Epoch [79/300], Step [1/42], Loss: 2.2004\n",
      "Epoch [79/300], Step [2/42], Loss: 1.1984\n",
      "Epoch [79/300], Step [3/42], Loss: 3.2168\n",
      "Epoch [79/300], Step [4/42], Loss: 5.4955\n",
      "Epoch [79/300], Step [5/42], Loss: 0.9140\n",
      "Epoch [79/300], Step [6/42], Loss: 3.7086\n",
      "Epoch [79/300], Step [7/42], Loss: 2.2083\n",
      "Epoch [79/300], Step [8/42], Loss: 1.3250\n",
      "Epoch [79/300], Step [9/42], Loss: 1.0709\n",
      "Epoch [79/300], Step [10/42], Loss: 0.4948\n",
      "Epoch [79/300], Step [11/42], Loss: 0.6533\n",
      "Epoch [79/300], Step [12/42], Loss: 1.0727\n",
      "Epoch [79/300], Step [13/42], Loss: 1.1993\n",
      "Epoch [79/300], Step [14/42], Loss: 1.0478\n",
      "Epoch [79/300], Step [15/42], Loss: 0.8675\n",
      "Epoch [79/300], Step [16/42], Loss: 0.8506\n",
      "Epoch [79/300], Step [17/42], Loss: 1.2852\n",
      "Epoch [79/300], Step [18/42], Loss: 0.7206\n",
      "Epoch [79/300], Step [19/42], Loss: 0.8553\n",
      "Epoch [79/300], Step [20/42], Loss: 1.7161\n",
      "Epoch [79/300], Step [21/42], Loss: 1.2328\n",
      "Epoch [79/300], Step [22/42], Loss: 0.7054\n",
      "Epoch [79/300], Step [23/42], Loss: 1.4061\n",
      "Epoch [79/300], Step [24/42], Loss: 0.4297\n",
      "Epoch [79/300], Step [25/42], Loss: 0.6233\n",
      "Epoch [79/300], Step [26/42], Loss: 2.0487\n",
      "Epoch [79/300], Step [27/42], Loss: 0.9936\n",
      "Epoch [79/300], Step [28/42], Loss: 0.9732\n",
      "Epoch [79/300], Step [29/42], Loss: 2.0110\n",
      "Epoch [79/300], Step [30/42], Loss: 1.1566\n",
      "Epoch [79/300], Step [31/42], Loss: 0.4166\n",
      "Epoch [79/300], Step [32/42], Loss: 1.7507\n",
      "Epoch [79/300], Step [33/42], Loss: 1.9804\n",
      "Epoch [79/300], Step [34/42], Loss: 1.7041\n",
      "Epoch [79/300], Step [35/42], Loss: 0.6941\n",
      "Epoch [79/300], Step [36/42], Loss: 1.7473\n",
      "Epoch [79/300], Step [37/42], Loss: 1.5568\n",
      "Epoch [79/300], Step [38/42], Loss: 0.4911\n",
      "Epoch [79/300], Step [39/42], Loss: 1.5840\n",
      "Epoch [79/300], Step [40/42], Loss: 0.4795\n",
      "Epoch [79/300], Step [41/42], Loss: 0.6342\n",
      "Epoch [79/300], Step [42/42], Loss: 0.3035\n",
      "Val. loss :1.7581\n",
      "Epoch [80/300], Step [1/42], Loss: 0.3744\n",
      "Epoch [80/300], Step [2/42], Loss: 0.7704\n",
      "Epoch [80/300], Step [3/42], Loss: 1.3075\n",
      "Epoch [80/300], Step [4/42], Loss: 0.3719\n",
      "Epoch [80/300], Step [5/42], Loss: 0.5953\n",
      "Epoch [80/300], Step [6/42], Loss: 0.9056\n",
      "Epoch [80/300], Step [7/42], Loss: 0.9813\n",
      "Epoch [80/300], Step [8/42], Loss: 1.2755\n",
      "Epoch [80/300], Step [9/42], Loss: 1.1374\n",
      "Epoch [80/300], Step [10/42], Loss: 0.7601\n",
      "Epoch [80/300], Step [11/42], Loss: 0.6352\n",
      "Epoch [80/300], Step [12/42], Loss: 0.5745\n",
      "Epoch [80/300], Step [13/42], Loss: 1.6256\n",
      "Epoch [80/300], Step [14/42], Loss: 1.9847\n",
      "Epoch [80/300], Step [15/42], Loss: 0.4160\n",
      "Epoch [80/300], Step [16/42], Loss: 4.7791\n",
      "Epoch [80/300], Step [17/42], Loss: 0.4333\n",
      "Epoch [80/300], Step [18/42], Loss: 0.5565\n",
      "Epoch [80/300], Step [19/42], Loss: 0.6551\n",
      "Epoch [80/300], Step [20/42], Loss: 1.4942\n",
      "Epoch [80/300], Step [21/42], Loss: 1.5647\n",
      "Epoch [80/300], Step [22/42], Loss: 4.5202\n",
      "Epoch [80/300], Step [23/42], Loss: 0.8958\n",
      "Epoch [80/300], Step [24/42], Loss: 1.4582\n",
      "Epoch [80/300], Step [25/42], Loss: 0.8102\n",
      "Epoch [80/300], Step [26/42], Loss: 1.4763\n",
      "Epoch [80/300], Step [27/42], Loss: 1.1228\n",
      "Epoch [80/300], Step [28/42], Loss: 1.7590\n",
      "Epoch [80/300], Step [29/42], Loss: 4.1124\n",
      "Epoch [80/300], Step [30/42], Loss: 0.5667\n",
      "Epoch [80/300], Step [31/42], Loss: 0.6977\n",
      "Epoch [80/300], Step [32/42], Loss: 0.8936\n",
      "Epoch [80/300], Step [33/42], Loss: 1.0360\n",
      "Epoch [80/300], Step [34/42], Loss: 2.4541\n",
      "Epoch [80/300], Step [35/42], Loss: 1.3033\n",
      "Epoch [80/300], Step [36/42], Loss: 1.5182\n",
      "Epoch [80/300], Step [37/42], Loss: 0.4137\n",
      "Epoch [80/300], Step [38/42], Loss: 1.0060\n",
      "Epoch [80/300], Step [39/42], Loss: 2.3053\n",
      "Epoch [80/300], Step [40/42], Loss: 0.4257\n",
      "Epoch [80/300], Step [41/42], Loss: 0.5401\n",
      "Epoch [80/300], Step [42/42], Loss: 0.1264\n",
      "Val. loss :1.3856\n",
      "Epoch [81/300], Step [1/42], Loss: 0.5917\n",
      "Epoch [81/300], Step [2/42], Loss: 1.1151\n",
      "Epoch [81/300], Step [3/42], Loss: 1.4998\n",
      "Epoch [81/300], Step [4/42], Loss: 0.7333\n",
      "Epoch [81/300], Step [5/42], Loss: 1.6771\n",
      "Epoch [81/300], Step [6/42], Loss: 0.8411\n",
      "Epoch [81/300], Step [7/42], Loss: 1.0805\n",
      "Epoch [81/300], Step [8/42], Loss: 0.9798\n",
      "Epoch [81/300], Step [9/42], Loss: 0.7608\n",
      "Epoch [81/300], Step [10/42], Loss: 3.8073\n",
      "Epoch [81/300], Step [11/42], Loss: 1.4364\n",
      "Epoch [81/300], Step [12/42], Loss: 4.4125\n",
      "Epoch [81/300], Step [13/42], Loss: 2.2923\n",
      "Epoch [81/300], Step [14/42], Loss: 1.5872\n",
      "Epoch [81/300], Step [15/42], Loss: 0.6975\n",
      "Epoch [81/300], Step [16/42], Loss: 2.1025\n",
      "Epoch [81/300], Step [17/42], Loss: 0.6419\n",
      "Epoch [81/300], Step [18/42], Loss: 0.7819\n",
      "Epoch [81/300], Step [19/42], Loss: 8.3509\n",
      "Epoch [81/300], Step [20/42], Loss: 11.7539\n",
      "Epoch [81/300], Step [21/42], Loss: 7.6030\n",
      "Epoch [81/300], Step [22/42], Loss: 1.9008\n",
      "Epoch [81/300], Step [23/42], Loss: 0.9906\n",
      "Epoch [81/300], Step [24/42], Loss: 0.7711\n",
      "Epoch [81/300], Step [25/42], Loss: 1.9795\n",
      "Epoch [81/300], Step [26/42], Loss: 2.5043\n",
      "Epoch [81/300], Step [27/42], Loss: 1.1745\n",
      "Epoch [81/300], Step [28/42], Loss: 0.3861\n",
      "Epoch [81/300], Step [29/42], Loss: 1.9606\n",
      "Epoch [81/300], Step [30/42], Loss: 0.9848\n",
      "Epoch [81/300], Step [31/42], Loss: 2.5487\n",
      "Epoch [81/300], Step [32/42], Loss: 0.4662\n",
      "Epoch [81/300], Step [33/42], Loss: 0.8149\n",
      "Epoch [81/300], Step [34/42], Loss: 1.3490\n",
      "Epoch [81/300], Step [35/42], Loss: 0.9922\n",
      "Epoch [81/300], Step [36/42], Loss: 0.5030\n",
      "Epoch [81/300], Step [37/42], Loss: 2.5386\n",
      "Epoch [81/300], Step [38/42], Loss: 1.1340\n",
      "Epoch [81/300], Step [39/42], Loss: 1.9645\n",
      "Epoch [81/300], Step [40/42], Loss: 0.7477\n",
      "Epoch [81/300], Step [41/42], Loss: 0.7309\n",
      "Epoch [81/300], Step [42/42], Loss: 0.0986\n",
      "Val. loss :1.9786\n",
      "Epoch [82/300], Step [1/42], Loss: 1.2887\n",
      "Epoch [82/300], Step [2/42], Loss: 3.0892\n",
      "Epoch [82/300], Step [3/42], Loss: 3.1504\n",
      "Epoch [82/300], Step [4/42], Loss: 0.8021\n",
      "Epoch [82/300], Step [5/42], Loss: 1.2754\n",
      "Epoch [82/300], Step [6/42], Loss: 1.4803\n",
      "Epoch [82/300], Step [7/42], Loss: 2.4231\n",
      "Epoch [82/300], Step [8/42], Loss: 1.1170\n",
      "Epoch [82/300], Step [9/42], Loss: 0.4938\n",
      "Epoch [82/300], Step [10/42], Loss: 1.2487\n",
      "Epoch [82/300], Step [11/42], Loss: 0.4496\n",
      "Epoch [82/300], Step [12/42], Loss: 5.8877\n",
      "Epoch [82/300], Step [13/42], Loss: 1.4091\n",
      "Epoch [82/300], Step [14/42], Loss: 1.1380\n",
      "Epoch [82/300], Step [15/42], Loss: 2.7530\n",
      "Epoch [82/300], Step [16/42], Loss: 1.2573\n",
      "Epoch [82/300], Step [17/42], Loss: 0.6514\n",
      "Epoch [82/300], Step [18/42], Loss: 1.3281\n",
      "Epoch [82/300], Step [19/42], Loss: 0.5469\n",
      "Epoch [82/300], Step [20/42], Loss: 2.1878\n",
      "Epoch [82/300], Step [21/42], Loss: 0.9743\n",
      "Epoch [82/300], Step [22/42], Loss: 2.9396\n",
      "Epoch [82/300], Step [23/42], Loss: 0.8245\n",
      "Epoch [82/300], Step [24/42], Loss: 0.4872\n",
      "Epoch [82/300], Step [25/42], Loss: 0.8019\n",
      "Epoch [82/300], Step [26/42], Loss: 1.1717\n",
      "Epoch [82/300], Step [27/42], Loss: 0.5437\n",
      "Epoch [82/300], Step [28/42], Loss: 1.8286\n",
      "Epoch [82/300], Step [29/42], Loss: 0.6724\n",
      "Epoch [82/300], Step [30/42], Loss: 0.6436\n",
      "Epoch [82/300], Step [31/42], Loss: 3.5239\n",
      "Epoch [82/300], Step [32/42], Loss: 0.4974\n",
      "Epoch [82/300], Step [33/42], Loss: 1.6818\n",
      "Epoch [82/300], Step [34/42], Loss: 2.2354\n",
      "Epoch [82/300], Step [35/42], Loss: 0.9102\n",
      "Epoch [82/300], Step [36/42], Loss: 1.6548\n",
      "Epoch [82/300], Step [37/42], Loss: 0.9814\n",
      "Epoch [82/300], Step [38/42], Loss: 0.5648\n",
      "Epoch [82/300], Step [39/42], Loss: 0.6710\n",
      "Epoch [82/300], Step [40/42], Loss: 0.9339\n",
      "Epoch [82/300], Step [41/42], Loss: 0.5754\n",
      "Epoch [82/300], Step [42/42], Loss: 0.5117\n",
      "Val. loss :1.6359\n",
      "Epoch [83/300], Step [1/42], Loss: 0.4494\n",
      "Epoch [83/300], Step [2/42], Loss: 0.8569\n",
      "Epoch [83/300], Step [3/42], Loss: 1.0905\n",
      "Epoch [83/300], Step [4/42], Loss: 0.9160\n",
      "Epoch [83/300], Step [5/42], Loss: 2.0306\n",
      "Epoch [83/300], Step [6/42], Loss: 0.6455\n",
      "Epoch [83/300], Step [7/42], Loss: 0.8501\n",
      "Epoch [83/300], Step [8/42], Loss: 0.4697\n",
      "Epoch [83/300], Step [9/42], Loss: 0.7841\n",
      "Epoch [83/300], Step [10/42], Loss: 1.0693\n",
      "Epoch [83/300], Step [11/42], Loss: 1.2364\n",
      "Epoch [83/300], Step [12/42], Loss: 2.5844\n",
      "Epoch [83/300], Step [13/42], Loss: 0.8935\n",
      "Epoch [83/300], Step [14/42], Loss: 0.4097\n",
      "Epoch [83/300], Step [15/42], Loss: 0.3567\n",
      "Epoch [83/300], Step [16/42], Loss: 1.2806\n",
      "Epoch [83/300], Step [17/42], Loss: 0.5144\n",
      "Epoch [83/300], Step [18/42], Loss: 1.0977\n",
      "Epoch [83/300], Step [19/42], Loss: 1.1939\n",
      "Epoch [83/300], Step [20/42], Loss: 2.1644\n",
      "Epoch [83/300], Step [21/42], Loss: 1.6290\n",
      "Epoch [83/300], Step [22/42], Loss: 0.9225\n",
      "Epoch [83/300], Step [23/42], Loss: 1.0164\n",
      "Epoch [83/300], Step [24/42], Loss: 2.6464\n",
      "Epoch [83/300], Step [25/42], Loss: 0.6774\n",
      "Epoch [83/300], Step [26/42], Loss: 2.0924\n",
      "Epoch [83/300], Step [27/42], Loss: 1.4908\n",
      "Epoch [83/300], Step [28/42], Loss: 2.3208\n",
      "Epoch [83/300], Step [29/42], Loss: 0.5054\n",
      "Epoch [83/300], Step [30/42], Loss: 1.6934\n",
      "Epoch [83/300], Step [31/42], Loss: 1.1047\n",
      "Epoch [83/300], Step [32/42], Loss: 1.8529\n",
      "Epoch [83/300], Step [33/42], Loss: 0.6840\n",
      "Epoch [83/300], Step [34/42], Loss: 1.1130\n",
      "Epoch [83/300], Step [35/42], Loss: 0.7328\n",
      "Epoch [83/300], Step [36/42], Loss: 1.2933\n",
      "Epoch [83/300], Step [37/42], Loss: 1.7048\n",
      "Epoch [83/300], Step [38/42], Loss: 0.4666\n",
      "Epoch [83/300], Step [39/42], Loss: 0.9847\n",
      "Epoch [83/300], Step [40/42], Loss: 5.8809\n",
      "Epoch [83/300], Step [41/42], Loss: 3.0887\n",
      "Epoch [83/300], Step [42/42], Loss: 0.1631\n",
      "Val. loss :1.5742\n",
      "Epoch [84/300], Step [1/42], Loss: 0.4726\n",
      "Epoch [84/300], Step [2/42], Loss: 1.1527\n",
      "Epoch [84/300], Step [3/42], Loss: 0.9306\n",
      "Epoch [84/300], Step [4/42], Loss: 0.7563\n",
      "Epoch [84/300], Step [5/42], Loss: 3.2702\n",
      "Epoch [84/300], Step [6/42], Loss: 1.0651\n",
      "Epoch [84/300], Step [7/42], Loss: 2.2822\n",
      "Epoch [84/300], Step [8/42], Loss: 1.1737\n",
      "Epoch [84/300], Step [9/42], Loss: 2.0004\n",
      "Epoch [84/300], Step [10/42], Loss: 0.8845\n",
      "Epoch [84/300], Step [11/42], Loss: 1.8479\n",
      "Epoch [84/300], Step [12/42], Loss: 1.1577\n",
      "Epoch [84/300], Step [13/42], Loss: 1.6629\n",
      "Epoch [84/300], Step [14/42], Loss: 0.8372\n",
      "Epoch [84/300], Step [15/42], Loss: 0.6185\n",
      "Epoch [84/300], Step [16/42], Loss: 2.4994\n",
      "Epoch [84/300], Step [17/42], Loss: 1.3349\n",
      "Epoch [84/300], Step [18/42], Loss: 0.7501\n",
      "Epoch [84/300], Step [19/42], Loss: 2.7986\n",
      "Epoch [84/300], Step [20/42], Loss: 0.8595\n",
      "Epoch [84/300], Step [21/42], Loss: 0.6782\n",
      "Epoch [84/300], Step [22/42], Loss: 0.7108\n",
      "Epoch [84/300], Step [23/42], Loss: 0.9284\n",
      "Epoch [84/300], Step [24/42], Loss: 0.4068\n",
      "Epoch [84/300], Step [25/42], Loss: 1.7490\n",
      "Epoch [84/300], Step [26/42], Loss: 1.1297\n",
      "Epoch [84/300], Step [27/42], Loss: 0.7539\n",
      "Epoch [84/300], Step [28/42], Loss: 0.6802\n",
      "Epoch [84/300], Step [29/42], Loss: 0.4140\n",
      "Epoch [84/300], Step [30/42], Loss: 3.8773\n",
      "Epoch [84/300], Step [31/42], Loss: 1.1167\n",
      "Epoch [84/300], Step [32/42], Loss: 1.4148\n",
      "Epoch [84/300], Step [33/42], Loss: 1.2257\n",
      "Epoch [84/300], Step [34/42], Loss: 1.7586\n",
      "Epoch [84/300], Step [35/42], Loss: 0.9171\n",
      "Epoch [84/300], Step [36/42], Loss: 0.7349\n",
      "Epoch [84/300], Step [37/42], Loss: 1.3662\n",
      "Epoch [84/300], Step [38/42], Loss: 0.8656\n",
      "Epoch [84/300], Step [39/42], Loss: 1.6558\n",
      "Epoch [84/300], Step [40/42], Loss: 0.4307\n",
      "Epoch [84/300], Step [41/42], Loss: 0.7500\n",
      "Epoch [84/300], Step [42/42], Loss: 1.9182\n",
      "Val. loss :2.0014\n",
      "Epoch [85/300], Step [1/42], Loss: 3.7474\n",
      "Epoch [85/300], Step [2/42], Loss: 0.9938\n",
      "Epoch [85/300], Step [3/42], Loss: 0.5633\n",
      "Epoch [85/300], Step [4/42], Loss: 2.1713\n",
      "Epoch [85/300], Step [5/42], Loss: 2.1435\n",
      "Epoch [85/300], Step [6/42], Loss: 1.5105\n",
      "Epoch [85/300], Step [7/42], Loss: 3.3846\n",
      "Epoch [85/300], Step [8/42], Loss: 5.3341\n",
      "Epoch [85/300], Step [9/42], Loss: 0.9220\n",
      "Epoch [85/300], Step [10/42], Loss: 0.3171\n",
      "Epoch [85/300], Step [11/42], Loss: 1.2285\n",
      "Epoch [85/300], Step [12/42], Loss: 0.8339\n",
      "Epoch [85/300], Step [13/42], Loss: 1.2441\n",
      "Epoch [85/300], Step [14/42], Loss: 1.2206\n",
      "Epoch [85/300], Step [15/42], Loss: 0.5278\n",
      "Epoch [85/300], Step [16/42], Loss: 10.1134\n",
      "Epoch [85/300], Step [17/42], Loss: 1.8833\n",
      "Epoch [85/300], Step [18/42], Loss: 1.1374\n",
      "Epoch [85/300], Step [19/42], Loss: 6.3057\n",
      "Epoch [85/300], Step [20/42], Loss: 2.5300\n",
      "Epoch [85/300], Step [21/42], Loss: 2.2474\n",
      "Epoch [85/300], Step [22/42], Loss: 1.9305\n",
      "Epoch [85/300], Step [23/42], Loss: 1.8249\n",
      "Epoch [85/300], Step [24/42], Loss: 0.8927\n",
      "Epoch [85/300], Step [25/42], Loss: 1.6452\n",
      "Epoch [85/300], Step [26/42], Loss: 2.7487\n",
      "Epoch [85/300], Step [27/42], Loss: 0.3689\n",
      "Epoch [85/300], Step [28/42], Loss: 0.8121\n",
      "Epoch [85/300], Step [29/42], Loss: 1.5813\n",
      "Epoch [85/300], Step [30/42], Loss: 1.8019\n",
      "Epoch [85/300], Step [31/42], Loss: 1.1352\n",
      "Epoch [85/300], Step [32/42], Loss: 0.6989\n",
      "Epoch [85/300], Step [33/42], Loss: 0.7202\n",
      "Epoch [85/300], Step [34/42], Loss: 1.5637\n",
      "Epoch [85/300], Step [35/42], Loss: 0.7516\n",
      "Epoch [85/300], Step [36/42], Loss: 2.5798\n",
      "Epoch [85/300], Step [37/42], Loss: 0.4419\n",
      "Epoch [85/300], Step [38/42], Loss: 0.8724\n",
      "Epoch [85/300], Step [39/42], Loss: 2.7975\n",
      "Epoch [85/300], Step [40/42], Loss: 0.4971\n",
      "Epoch [85/300], Step [41/42], Loss: 1.1016\n",
      "Epoch [85/300], Step [42/42], Loss: 0.1214\n",
      "Val. loss :1.9922\n",
      "Epoch [86/300], Step [1/42], Loss: 1.5184\n",
      "Epoch [86/300], Step [2/42], Loss: 0.5446\n",
      "Epoch [86/300], Step [3/42], Loss: 0.5620\n",
      "Epoch [86/300], Step [4/42], Loss: 3.3234\n",
      "Epoch [86/300], Step [5/42], Loss: 0.7221\n",
      "Epoch [86/300], Step [6/42], Loss: 0.7905\n",
      "Epoch [86/300], Step [7/42], Loss: 1.1671\n",
      "Epoch [86/300], Step [8/42], Loss: 0.9896\n",
      "Epoch [86/300], Step [9/42], Loss: 1.0849\n",
      "Epoch [86/300], Step [10/42], Loss: 0.7969\n",
      "Epoch [86/300], Step [11/42], Loss: 0.8413\n",
      "Epoch [86/300], Step [12/42], Loss: 0.6717\n",
      "Epoch [86/300], Step [13/42], Loss: 0.6502\n",
      "Epoch [86/300], Step [14/42], Loss: 0.7570\n",
      "Epoch [86/300], Step [15/42], Loss: 1.9722\n",
      "Epoch [86/300], Step [16/42], Loss: 1.0230\n",
      "Epoch [86/300], Step [17/42], Loss: 0.6268\n",
      "Epoch [86/300], Step [18/42], Loss: 1.1644\n",
      "Epoch [86/300], Step [19/42], Loss: 1.3582\n",
      "Epoch [86/300], Step [20/42], Loss: 1.7714\n",
      "Epoch [86/300], Step [21/42], Loss: 0.8870\n",
      "Epoch [86/300], Step [22/42], Loss: 1.0353\n",
      "Epoch [86/300], Step [23/42], Loss: 0.8837\n",
      "Epoch [86/300], Step [24/42], Loss: 2.4995\n",
      "Epoch [86/300], Step [25/42], Loss: 1.1104\n",
      "Epoch [86/300], Step [26/42], Loss: 1.1650\n",
      "Epoch [86/300], Step [27/42], Loss: 0.7970\n",
      "Epoch [86/300], Step [28/42], Loss: 0.4015\n",
      "Epoch [86/300], Step [29/42], Loss: 0.4115\n",
      "Epoch [86/300], Step [30/42], Loss: 2.1463\n",
      "Epoch [86/300], Step [31/42], Loss: 0.6802\n",
      "Epoch [86/300], Step [32/42], Loss: 4.4354\n",
      "Epoch [86/300], Step [33/42], Loss: 0.3156\n",
      "Epoch [86/300], Step [34/42], Loss: 0.9612\n",
      "Epoch [86/300], Step [35/42], Loss: 0.7435\n",
      "Epoch [86/300], Step [36/42], Loss: 1.7149\n",
      "Epoch [86/300], Step [37/42], Loss: 0.7976\n",
      "Epoch [86/300], Step [38/42], Loss: 1.8469\n",
      "Epoch [86/300], Step [39/42], Loss: 4.3343\n",
      "Epoch [86/300], Step [40/42], Loss: 1.1765\n",
      "Epoch [86/300], Step [41/42], Loss: 0.4790\n",
      "Epoch [86/300], Step [42/42], Loss: 0.5076\n",
      "Val. loss :1.4007\n",
      "Epoch [87/300], Step [1/42], Loss: 0.7589\n",
      "Epoch [87/300], Step [2/42], Loss: 1.1700\n",
      "Epoch [87/300], Step [3/42], Loss: 1.3371\n",
      "Epoch [87/300], Step [4/42], Loss: 2.1209\n",
      "Epoch [87/300], Step [5/42], Loss: 0.4957\n",
      "Epoch [87/300], Step [6/42], Loss: 2.7311\n",
      "Epoch [87/300], Step [7/42], Loss: 0.9001\n",
      "Epoch [87/300], Step [8/42], Loss: 0.9252\n",
      "Epoch [87/300], Step [9/42], Loss: 0.5257\n",
      "Epoch [87/300], Step [10/42], Loss: 0.7811\n",
      "Epoch [87/300], Step [11/42], Loss: 0.8986\n",
      "Epoch [87/300], Step [12/42], Loss: 1.4497\n",
      "Epoch [87/300], Step [13/42], Loss: 0.9122\n",
      "Epoch [87/300], Step [14/42], Loss: 0.3297\n",
      "Epoch [87/300], Step [15/42], Loss: 1.1102\n",
      "Epoch [87/300], Step [16/42], Loss: 0.4918\n",
      "Epoch [87/300], Step [17/42], Loss: 0.6121\n",
      "Epoch [87/300], Step [18/42], Loss: 0.7630\n",
      "Epoch [87/300], Step [19/42], Loss: 1.2212\n",
      "Epoch [87/300], Step [20/42], Loss: 2.2226\n",
      "Epoch [87/300], Step [21/42], Loss: 0.6084\n",
      "Epoch [87/300], Step [22/42], Loss: 0.5208\n",
      "Epoch [87/300], Step [23/42], Loss: 1.2032\n",
      "Epoch [87/300], Step [24/42], Loss: 3.9126\n",
      "Epoch [87/300], Step [25/42], Loss: 0.4270\n",
      "Epoch [87/300], Step [26/42], Loss: 0.8812\n",
      "Epoch [87/300], Step [27/42], Loss: 0.8225\n",
      "Epoch [87/300], Step [28/42], Loss: 0.9542\n",
      "Epoch [87/300], Step [29/42], Loss: 0.8952\n",
      "Epoch [87/300], Step [30/42], Loss: 1.2678\n",
      "Epoch [87/300], Step [31/42], Loss: 1.4646\n",
      "Epoch [87/300], Step [32/42], Loss: 0.6975\n",
      "Epoch [87/300], Step [33/42], Loss: 1.2111\n",
      "Epoch [87/300], Step [34/42], Loss: 0.8509\n",
      "Epoch [87/300], Step [35/42], Loss: 1.2577\n",
      "Epoch [87/300], Step [36/42], Loss: 2.2382\n",
      "Epoch [87/300], Step [37/42], Loss: 0.3329\n",
      "Epoch [87/300], Step [38/42], Loss: 2.3300\n",
      "Epoch [87/300], Step [39/42], Loss: 1.1082\n",
      "Epoch [87/300], Step [40/42], Loss: 0.7920\n",
      "Epoch [87/300], Step [41/42], Loss: 1.1505\n",
      "Epoch [87/300], Step [42/42], Loss: 0.5656\n",
      "Val. loss :1.3626\n",
      "Epoch [88/300], Step [1/42], Loss: 1.8141\n",
      "Epoch [88/300], Step [2/42], Loss: 0.4779\n",
      "Epoch [88/300], Step [3/42], Loss: 1.5073\n",
      "Epoch [88/300], Step [4/42], Loss: 1.8514\n",
      "Epoch [88/300], Step [5/42], Loss: 0.8491\n",
      "Epoch [88/300], Step [6/42], Loss: 1.0549\n",
      "Epoch [88/300], Step [7/42], Loss: 0.5767\n",
      "Epoch [88/300], Step [8/42], Loss: 0.8756\n",
      "Epoch [88/300], Step [9/42], Loss: 0.3140\n",
      "Epoch [88/300], Step [10/42], Loss: 1.9906\n",
      "Epoch [88/300], Step [11/42], Loss: 0.7121\n",
      "Epoch [88/300], Step [12/42], Loss: 1.8825\n",
      "Epoch [88/300], Step [13/42], Loss: 0.3460\n",
      "Epoch [88/300], Step [14/42], Loss: 0.9050\n",
      "Epoch [88/300], Step [15/42], Loss: 0.8123\n",
      "Epoch [88/300], Step [16/42], Loss: 1.5225\n",
      "Epoch [88/300], Step [17/42], Loss: 0.7146\n",
      "Epoch [88/300], Step [18/42], Loss: 0.3059\n",
      "Epoch [88/300], Step [19/42], Loss: 1.5368\n",
      "Epoch [88/300], Step [20/42], Loss: 1.4521\n",
      "Epoch [88/300], Step [21/42], Loss: 0.3971\n",
      "Epoch [88/300], Step [22/42], Loss: 0.3345\n",
      "Epoch [88/300], Step [23/42], Loss: 0.6978\n",
      "Epoch [88/300], Step [24/42], Loss: 0.3165\n",
      "Epoch [88/300], Step [25/42], Loss: 1.7756\n",
      "Epoch [88/300], Step [26/42], Loss: 0.9112\n",
      "Epoch [88/300], Step [27/42], Loss: 0.5983\n",
      "Epoch [88/300], Step [28/42], Loss: 1.1976\n",
      "Epoch [88/300], Step [29/42], Loss: 1.2050\n",
      "Epoch [88/300], Step [30/42], Loss: 0.5321\n",
      "Epoch [88/300], Step [31/42], Loss: 0.9260\n",
      "Epoch [88/300], Step [32/42], Loss: 0.4524\n",
      "Epoch [88/300], Step [33/42], Loss: 0.5633\n",
      "Epoch [88/300], Step [34/42], Loss: 1.7259\n",
      "Epoch [88/300], Step [35/42], Loss: 0.9991\n",
      "Epoch [88/300], Step [36/42], Loss: 0.7719\n",
      "Epoch [88/300], Step [37/42], Loss: 0.3294\n",
      "Epoch [88/300], Step [38/42], Loss: 0.9029\n",
      "Epoch [88/300], Step [39/42], Loss: 6.2239\n",
      "Epoch [88/300], Step [40/42], Loss: 0.4214\n",
      "Epoch [88/300], Step [41/42], Loss: 2.1504\n",
      "Epoch [88/300], Step [42/42], Loss: 0.1029\n",
      "Val. loss :1.2006\n",
      "Epoch [89/300], Step [1/42], Loss: 0.7500\n",
      "Epoch [89/300], Step [2/42], Loss: 0.7195\n",
      "Epoch [89/300], Step [3/42], Loss: 0.4244\n",
      "Epoch [89/300], Step [4/42], Loss: 0.8529\n",
      "Epoch [89/300], Step [5/42], Loss: 1.5913\n",
      "Epoch [89/300], Step [6/42], Loss: 0.7142\n",
      "Epoch [89/300], Step [7/42], Loss: 1.4063\n",
      "Epoch [89/300], Step [8/42], Loss: 0.8911\n",
      "Epoch [89/300], Step [9/42], Loss: 0.7553\n",
      "Epoch [89/300], Step [10/42], Loss: 0.2848\n",
      "Epoch [89/300], Step [11/42], Loss: 0.8881\n",
      "Epoch [89/300], Step [12/42], Loss: 2.2194\n",
      "Epoch [89/300], Step [13/42], Loss: 0.9520\n",
      "Epoch [89/300], Step [14/42], Loss: 0.7168\n",
      "Epoch [89/300], Step [15/42], Loss: 1.1598\n",
      "Epoch [89/300], Step [16/42], Loss: 1.1931\n",
      "Epoch [89/300], Step [17/42], Loss: 0.4660\n",
      "Epoch [89/300], Step [18/42], Loss: 1.2600\n",
      "Epoch [89/300], Step [19/42], Loss: 0.7873\n",
      "Epoch [89/300], Step [20/42], Loss: 1.0450\n",
      "Epoch [89/300], Step [21/42], Loss: 1.2808\n",
      "Epoch [89/300], Step [22/42], Loss: 2.0299\n",
      "Epoch [89/300], Step [23/42], Loss: 1.0499\n",
      "Epoch [89/300], Step [24/42], Loss: 1.5868\n",
      "Epoch [89/300], Step [25/42], Loss: 0.7471\n",
      "Epoch [89/300], Step [26/42], Loss: 1.3613\n",
      "Epoch [89/300], Step [27/42], Loss: 0.5214\n",
      "Epoch [89/300], Step [28/42], Loss: 0.2609\n",
      "Epoch [89/300], Step [29/42], Loss: 0.3580\n",
      "Epoch [89/300], Step [30/42], Loss: 1.3236\n",
      "Epoch [89/300], Step [31/42], Loss: 0.3966\n",
      "Epoch [89/300], Step [32/42], Loss: 1.0349\n",
      "Epoch [89/300], Step [33/42], Loss: 1.5293\n",
      "Epoch [89/300], Step [34/42], Loss: 0.7241\n",
      "Epoch [89/300], Step [35/42], Loss: 0.9798\n",
      "Epoch [89/300], Step [36/42], Loss: 0.8723\n",
      "Epoch [89/300], Step [37/42], Loss: 3.6175\n",
      "Epoch [89/300], Step [38/42], Loss: 0.8926\n",
      "Epoch [89/300], Step [39/42], Loss: 0.3899\n",
      "Epoch [89/300], Step [40/42], Loss: 0.6260\n",
      "Epoch [89/300], Step [41/42], Loss: 0.9450\n",
      "Epoch [89/300], Step [42/42], Loss: 0.0633\n",
      "Val. loss :1.3909\n",
      "Epoch [90/300], Step [1/42], Loss: 0.4313\n",
      "Epoch [90/300], Step [2/42], Loss: 0.2877\n",
      "Epoch [90/300], Step [3/42], Loss: 0.7294\n",
      "Epoch [90/300], Step [4/42], Loss: 1.2805\n",
      "Epoch [90/300], Step [5/42], Loss: 0.9716\n",
      "Epoch [90/300], Step [6/42], Loss: 0.7231\n",
      "Epoch [90/300], Step [7/42], Loss: 0.4834\n",
      "Epoch [90/300], Step [8/42], Loss: 2.3426\n",
      "Epoch [90/300], Step [9/42], Loss: 0.5614\n",
      "Epoch [90/300], Step [10/42], Loss: 0.7805\n",
      "Epoch [90/300], Step [11/42], Loss: 0.2631\n",
      "Epoch [90/300], Step [12/42], Loss: 1.2005\n",
      "Epoch [90/300], Step [13/42], Loss: 0.3161\n",
      "Epoch [90/300], Step [14/42], Loss: 2.4776\n",
      "Epoch [90/300], Step [15/42], Loss: 1.7648\n",
      "Epoch [90/300], Step [16/42], Loss: 0.3813\n",
      "Epoch [90/300], Step [17/42], Loss: 0.5730\n",
      "Epoch [90/300], Step [18/42], Loss: 2.3087\n",
      "Epoch [90/300], Step [19/42], Loss: 0.7354\n",
      "Epoch [90/300], Step [20/42], Loss: 1.0243\n",
      "Epoch [90/300], Step [21/42], Loss: 0.4735\n",
      "Epoch [90/300], Step [22/42], Loss: 1.7183\n",
      "Epoch [90/300], Step [23/42], Loss: 1.1294\n",
      "Epoch [90/300], Step [24/42], Loss: 1.2110\n",
      "Epoch [90/300], Step [25/42], Loss: 0.3411\n",
      "Epoch [90/300], Step [26/42], Loss: 0.5925\n",
      "Epoch [90/300], Step [27/42], Loss: 0.9192\n",
      "Epoch [90/300], Step [28/42], Loss: 0.8815\n",
      "Epoch [90/300], Step [29/42], Loss: 1.3795\n",
      "Epoch [90/300], Step [30/42], Loss: 1.3110\n",
      "Epoch [90/300], Step [31/42], Loss: 2.1426\n",
      "Epoch [90/300], Step [32/42], Loss: 0.7261\n",
      "Epoch [90/300], Step [33/42], Loss: 1.0786\n",
      "Epoch [90/300], Step [34/42], Loss: 1.2777\n",
      "Epoch [90/300], Step [35/42], Loss: 0.4421\n",
      "Epoch [90/300], Step [36/42], Loss: 1.5940\n",
      "Epoch [90/300], Step [37/42], Loss: 0.8484\n",
      "Epoch [90/300], Step [38/42], Loss: 0.8295\n",
      "Epoch [90/300], Step [39/42], Loss: 3.3477\n",
      "Epoch [90/300], Step [40/42], Loss: 5.5794\n",
      "Epoch [90/300], Step [41/42], Loss: 0.9013\n",
      "Epoch [90/300], Step [42/42], Loss: 0.1451\n",
      "Val. loss :1.3254\n",
      "Epoch [91/300], Step [1/42], Loss: 2.3689\n",
      "Epoch [91/300], Step [2/42], Loss: 0.3505\n",
      "Epoch [91/300], Step [3/42], Loss: 0.6624\n",
      "Epoch [91/300], Step [4/42], Loss: 1.9674\n",
      "Epoch [91/300], Step [5/42], Loss: 1.9793\n",
      "Epoch [91/300], Step [6/42], Loss: 1.5647\n",
      "Epoch [91/300], Step [7/42], Loss: 0.6828\n",
      "Epoch [91/300], Step [8/42], Loss: 0.6968\n",
      "Epoch [91/300], Step [9/42], Loss: 0.6148\n",
      "Epoch [91/300], Step [10/42], Loss: 1.3915\n",
      "Epoch [91/300], Step [11/42], Loss: 3.1107\n",
      "Epoch [91/300], Step [12/42], Loss: 1.1276\n",
      "Epoch [91/300], Step [13/42], Loss: 0.9841\n",
      "Epoch [91/300], Step [14/42], Loss: 0.4044\n",
      "Epoch [91/300], Step [15/42], Loss: 0.6221\n",
      "Epoch [91/300], Step [16/42], Loss: 0.8247\n",
      "Epoch [91/300], Step [17/42], Loss: 1.9417\n",
      "Epoch [91/300], Step [18/42], Loss: 1.4926\n",
      "Epoch [91/300], Step [19/42], Loss: 2.3695\n",
      "Epoch [91/300], Step [20/42], Loss: 4.5838\n",
      "Epoch [91/300], Step [21/42], Loss: 1.8932\n",
      "Epoch [91/300], Step [22/42], Loss: 0.6021\n",
      "Epoch [91/300], Step [23/42], Loss: 1.0141\n",
      "Epoch [91/300], Step [24/42], Loss: 1.5792\n",
      "Epoch [91/300], Step [25/42], Loss: 2.8339\n",
      "Epoch [91/300], Step [26/42], Loss: 0.8672\n",
      "Epoch [91/300], Step [27/42], Loss: 1.4415\n",
      "Epoch [91/300], Step [28/42], Loss: 0.6733\n",
      "Epoch [91/300], Step [29/42], Loss: 3.6683\n",
      "Epoch [91/300], Step [30/42], Loss: 1.4729\n",
      "Epoch [91/300], Step [31/42], Loss: 0.9445\n",
      "Epoch [91/300], Step [32/42], Loss: 0.3214\n",
      "Epoch [91/300], Step [33/42], Loss: 0.3638\n",
      "Epoch [91/300], Step [34/42], Loss: 3.0337\n",
      "Epoch [91/300], Step [35/42], Loss: 7.0956\n",
      "Epoch [91/300], Step [36/42], Loss: 1.8665\n",
      "Epoch [91/300], Step [37/42], Loss: 2.9695\n",
      "Epoch [91/300], Step [38/42], Loss: 0.8487\n",
      "Epoch [91/300], Step [39/42], Loss: 1.2892\n",
      "Epoch [91/300], Step [40/42], Loss: 5.3998\n",
      "Epoch [91/300], Step [41/42], Loss: 1.1150\n",
      "Epoch [91/300], Step [42/42], Loss: 6.4735\n",
      "Val. loss :1.8108\n",
      "Epoch [92/300], Step [1/42], Loss: 0.6241\n",
      "Epoch [92/300], Step [2/42], Loss: 0.6963\n",
      "Epoch [92/300], Step [3/42], Loss: 0.7831\n",
      "Epoch [92/300], Step [4/42], Loss: 0.9244\n",
      "Epoch [92/300], Step [5/42], Loss: 3.2141\n",
      "Epoch [92/300], Step [6/42], Loss: 1.8690\n",
      "Epoch [92/300], Step [7/42], Loss: 1.3104\n",
      "Epoch [92/300], Step [8/42], Loss: 1.1800\n",
      "Epoch [92/300], Step [9/42], Loss: 2.2447\n",
      "Epoch [92/300], Step [10/42], Loss: 1.6819\n",
      "Epoch [92/300], Step [11/42], Loss: 3.0707\n",
      "Epoch [92/300], Step [12/42], Loss: 0.9798\n",
      "Epoch [92/300], Step [13/42], Loss: 1.6575\n",
      "Epoch [92/300], Step [14/42], Loss: 0.9990\n",
      "Epoch [92/300], Step [15/42], Loss: 1.3676\n",
      "Epoch [92/300], Step [16/42], Loss: 0.7833\n",
      "Epoch [92/300], Step [17/42], Loss: 2.2334\n",
      "Epoch [92/300], Step [18/42], Loss: 0.9297\n",
      "Epoch [92/300], Step [19/42], Loss: 1.5133\n",
      "Epoch [92/300], Step [20/42], Loss: 1.7405\n",
      "Epoch [92/300], Step [21/42], Loss: 0.6192\n",
      "Epoch [92/300], Step [22/42], Loss: 0.4963\n",
      "Epoch [92/300], Step [23/42], Loss: 1.3545\n",
      "Epoch [92/300], Step [24/42], Loss: 5.0331\n",
      "Epoch [92/300], Step [25/42], Loss: 1.8813\n",
      "Epoch [92/300], Step [26/42], Loss: 1.0676\n",
      "Epoch [92/300], Step [27/42], Loss: 1.1363\n",
      "Epoch [92/300], Step [28/42], Loss: 1.7812\n",
      "Epoch [92/300], Step [29/42], Loss: 0.5005\n",
      "Epoch [92/300], Step [30/42], Loss: 1.6979\n",
      "Epoch [92/300], Step [31/42], Loss: 0.6418\n",
      "Epoch [92/300], Step [32/42], Loss: 0.3010\n",
      "Epoch [92/300], Step [33/42], Loss: 0.8232\n",
      "Epoch [92/300], Step [34/42], Loss: 1.1711\n",
      "Epoch [92/300], Step [35/42], Loss: 0.6375\n",
      "Epoch [92/300], Step [36/42], Loss: 3.4599\n",
      "Epoch [92/300], Step [37/42], Loss: 1.2075\n",
      "Epoch [92/300], Step [38/42], Loss: 0.3728\n",
      "Epoch [92/300], Step [39/42], Loss: 3.0939\n",
      "Epoch [92/300], Step [40/42], Loss: 0.4441\n",
      "Epoch [92/300], Step [41/42], Loss: 2.1456\n",
      "Epoch [92/300], Step [42/42], Loss: 0.1273\n",
      "Val. loss :1.4866\n",
      "Epoch [93/300], Step [1/42], Loss: 0.8596\n",
      "Epoch [93/300], Step [2/42], Loss: 0.2818\n",
      "Epoch [93/300], Step [3/42], Loss: 1.2596\n",
      "Epoch [93/300], Step [4/42], Loss: 2.3457\n",
      "Epoch [93/300], Step [5/42], Loss: 0.5791\n",
      "Epoch [93/300], Step [6/42], Loss: 0.5504\n",
      "Epoch [93/300], Step [7/42], Loss: 2.3855\n",
      "Epoch [93/300], Step [8/42], Loss: 1.6589\n",
      "Epoch [93/300], Step [9/42], Loss: 0.2540\n",
      "Epoch [93/300], Step [10/42], Loss: 2.3236\n",
      "Epoch [93/300], Step [11/42], Loss: 1.8436\n",
      "Epoch [93/300], Step [12/42], Loss: 0.5973\n",
      "Epoch [93/300], Step [13/42], Loss: 1.3058\n",
      "Epoch [93/300], Step [14/42], Loss: 0.3387\n",
      "Epoch [93/300], Step [15/42], Loss: 1.5649\n",
      "Epoch [93/300], Step [16/42], Loss: 2.5791\n",
      "Epoch [93/300], Step [17/42], Loss: 1.6632\n",
      "Epoch [93/300], Step [18/42], Loss: 0.8177\n",
      "Epoch [93/300], Step [19/42], Loss: 0.7998\n",
      "Epoch [93/300], Step [20/42], Loss: 1.0268\n",
      "Epoch [93/300], Step [21/42], Loss: 0.5454\n",
      "Epoch [93/300], Step [22/42], Loss: 0.5561\n",
      "Epoch [93/300], Step [23/42], Loss: 1.1032\n",
      "Epoch [93/300], Step [24/42], Loss: 0.8244\n",
      "Epoch [93/300], Step [25/42], Loss: 0.4836\n",
      "Epoch [93/300], Step [26/42], Loss: 1.8623\n",
      "Epoch [93/300], Step [27/42], Loss: 0.5379\n",
      "Epoch [93/300], Step [28/42], Loss: 1.4884\n",
      "Epoch [93/300], Step [29/42], Loss: 0.5282\n",
      "Epoch [93/300], Step [30/42], Loss: 0.4463\n",
      "Epoch [93/300], Step [31/42], Loss: 0.2823\n",
      "Epoch [93/300], Step [32/42], Loss: 6.5107\n",
      "Epoch [93/300], Step [33/42], Loss: 0.6697\n",
      "Epoch [93/300], Step [34/42], Loss: 1.6114\n",
      "Epoch [93/300], Step [35/42], Loss: 0.4048\n",
      "Epoch [93/300], Step [36/42], Loss: 0.4620\n",
      "Epoch [93/300], Step [37/42], Loss: 2.5999\n",
      "Epoch [93/300], Step [38/42], Loss: 1.7363\n",
      "Epoch [93/300], Step [39/42], Loss: 1.1426\n",
      "Epoch [93/300], Step [40/42], Loss: 0.3971\n",
      "Epoch [93/300], Step [41/42], Loss: 2.0023\n",
      "Epoch [93/300], Step [42/42], Loss: 2.8035\n",
      "Val. loss :2.3448\n",
      "Epoch [94/300], Step [1/42], Loss: 0.4521\n",
      "Epoch [94/300], Step [2/42], Loss: 1.7735\n",
      "Epoch [94/300], Step [3/42], Loss: 1.8533\n",
      "Epoch [94/300], Step [4/42], Loss: 1.0333\n",
      "Epoch [94/300], Step [5/42], Loss: 0.9618\n",
      "Epoch [94/300], Step [6/42], Loss: 0.4696\n",
      "Epoch [94/300], Step [7/42], Loss: 2.8889\n",
      "Epoch [94/300], Step [8/42], Loss: 6.9511\n",
      "Epoch [94/300], Step [9/42], Loss: 0.7021\n",
      "Epoch [94/300], Step [10/42], Loss: 0.9867\n",
      "Epoch [94/300], Step [11/42], Loss: 2.4690\n",
      "Epoch [94/300], Step [12/42], Loss: 1.0042\n",
      "Epoch [94/300], Step [13/42], Loss: 0.4240\n",
      "Epoch [94/300], Step [14/42], Loss: 2.4829\n",
      "Epoch [94/300], Step [15/42], Loss: 1.7426\n",
      "Epoch [94/300], Step [16/42], Loss: 1.5911\n",
      "Epoch [94/300], Step [17/42], Loss: 0.9098\n",
      "Epoch [94/300], Step [18/42], Loss: 0.6564\n",
      "Epoch [94/300], Step [19/42], Loss: 0.4877\n",
      "Epoch [94/300], Step [20/42], Loss: 0.3311\n",
      "Epoch [94/300], Step [21/42], Loss: 1.4001\n",
      "Epoch [94/300], Step [22/42], Loss: 0.8194\n",
      "Epoch [94/300], Step [23/42], Loss: 2.8687\n",
      "Epoch [94/300], Step [24/42], Loss: 0.8028\n",
      "Epoch [94/300], Step [25/42], Loss: 0.4516\n",
      "Epoch [94/300], Step [26/42], Loss: 0.5520\n",
      "Epoch [94/300], Step [27/42], Loss: 0.6012\n",
      "Epoch [94/300], Step [28/42], Loss: 0.7084\n",
      "Epoch [94/300], Step [29/42], Loss: 0.9992\n",
      "Epoch [94/300], Step [30/42], Loss: 0.9447\n",
      "Epoch [94/300], Step [31/42], Loss: 0.5391\n",
      "Epoch [94/300], Step [32/42], Loss: 0.6724\n",
      "Epoch [94/300], Step [33/42], Loss: 2.4096\n",
      "Epoch [94/300], Step [34/42], Loss: 1.0114\n",
      "Epoch [94/300], Step [35/42], Loss: 0.7133\n",
      "Epoch [94/300], Step [36/42], Loss: 1.2524\n",
      "Epoch [94/300], Step [37/42], Loss: 0.8148\n",
      "Epoch [94/300], Step [38/42], Loss: 0.3098\n",
      "Epoch [94/300], Step [39/42], Loss: 3.3713\n",
      "Epoch [94/300], Step [40/42], Loss: 0.6288\n",
      "Epoch [94/300], Step [41/42], Loss: 1.5417\n",
      "Epoch [94/300], Step [42/42], Loss: 0.0479\n",
      "Val. loss :1.4039\n",
      "Epoch [95/300], Step [1/42], Loss: 0.4385\n",
      "Epoch [95/300], Step [2/42], Loss: 1.6246\n",
      "Epoch [95/300], Step [3/42], Loss: 0.8633\n",
      "Epoch [95/300], Step [4/42], Loss: 0.8703\n",
      "Epoch [95/300], Step [5/42], Loss: 0.3407\n",
      "Epoch [95/300], Step [6/42], Loss: 2.6439\n",
      "Epoch [95/300], Step [7/42], Loss: 1.3264\n",
      "Epoch [95/300], Step [8/42], Loss: 1.4155\n",
      "Epoch [95/300], Step [9/42], Loss: 0.5085\n",
      "Epoch [95/300], Step [10/42], Loss: 0.5754\n",
      "Epoch [95/300], Step [11/42], Loss: 1.7058\n",
      "Epoch [95/300], Step [12/42], Loss: 0.9406\n",
      "Epoch [95/300], Step [13/42], Loss: 0.4791\n",
      "Epoch [95/300], Step [14/42], Loss: 0.3482\n",
      "Epoch [95/300], Step [15/42], Loss: 0.4823\n",
      "Epoch [95/300], Step [16/42], Loss: 0.6368\n",
      "Epoch [95/300], Step [17/42], Loss: 0.2930\n",
      "Epoch [95/300], Step [18/42], Loss: 6.0661\n",
      "Epoch [95/300], Step [19/42], Loss: 0.5862\n",
      "Epoch [95/300], Step [20/42], Loss: 1.0628\n",
      "Epoch [95/300], Step [21/42], Loss: 0.4242\n",
      "Epoch [95/300], Step [22/42], Loss: 0.8603\n",
      "Epoch [95/300], Step [23/42], Loss: 0.4328\n",
      "Epoch [95/300], Step [24/42], Loss: 1.0068\n",
      "Epoch [95/300], Step [25/42], Loss: 0.8422\n",
      "Epoch [95/300], Step [26/42], Loss: 0.6268\n",
      "Epoch [95/300], Step [27/42], Loss: 0.3981\n",
      "Epoch [95/300], Step [28/42], Loss: 0.7462\n",
      "Epoch [95/300], Step [29/42], Loss: 1.0207\n",
      "Epoch [95/300], Step [30/42], Loss: 0.3111\n",
      "Epoch [95/300], Step [31/42], Loss: 0.3273\n",
      "Epoch [95/300], Step [32/42], Loss: 2.0350\n",
      "Epoch [95/300], Step [33/42], Loss: 0.9250\n",
      "Epoch [95/300], Step [34/42], Loss: 1.5182\n",
      "Epoch [95/300], Step [35/42], Loss: 0.5970\n",
      "Epoch [95/300], Step [36/42], Loss: 0.8957\n",
      "Epoch [95/300], Step [37/42], Loss: 1.1256\n",
      "Epoch [95/300], Step [38/42], Loss: 1.2181\n",
      "Epoch [95/300], Step [39/42], Loss: 2.9479\n",
      "Epoch [95/300], Step [40/42], Loss: 1.4950\n",
      "Epoch [95/300], Step [41/42], Loss: 0.7600\n",
      "Epoch [95/300], Step [42/42], Loss: 0.0486\n",
      "Val. loss :1.1610\n",
      "Epoch [96/300], Step [1/42], Loss: 0.4258\n",
      "Epoch [96/300], Step [2/42], Loss: 0.6191\n",
      "Epoch [96/300], Step [3/42], Loss: 2.0503\n",
      "Epoch [96/300], Step [4/42], Loss: 1.5465\n",
      "Epoch [96/300], Step [5/42], Loss: 0.5215\n",
      "Epoch [96/300], Step [6/42], Loss: 0.4735\n",
      "Epoch [96/300], Step [7/42], Loss: 0.5959\n",
      "Epoch [96/300], Step [8/42], Loss: 0.6393\n",
      "Epoch [96/300], Step [9/42], Loss: 1.7210\n",
      "Epoch [96/300], Step [10/42], Loss: 1.8055\n",
      "Epoch [96/300], Step [11/42], Loss: 0.7688\n",
      "Epoch [96/300], Step [12/42], Loss: 0.5092\n",
      "Epoch [96/300], Step [13/42], Loss: 0.4396\n",
      "Epoch [96/300], Step [14/42], Loss: 0.7165\n",
      "Epoch [96/300], Step [15/42], Loss: 0.3078\n",
      "Epoch [96/300], Step [16/42], Loss: 0.4853\n",
      "Epoch [96/300], Step [17/42], Loss: 0.5015\n",
      "Epoch [96/300], Step [18/42], Loss: 0.5629\n",
      "Epoch [96/300], Step [19/42], Loss: 0.6870\n",
      "Epoch [96/300], Step [20/42], Loss: 1.0128\n",
      "Epoch [96/300], Step [21/42], Loss: 0.9966\n",
      "Epoch [96/300], Step [22/42], Loss: 0.7408\n",
      "Epoch [96/300], Step [23/42], Loss: 0.3895\n",
      "Epoch [96/300], Step [24/42], Loss: 0.3447\n",
      "Epoch [96/300], Step [25/42], Loss: 0.3305\n",
      "Epoch [96/300], Step [26/42], Loss: 0.5551\n",
      "Epoch [96/300], Step [27/42], Loss: 1.0017\n",
      "Epoch [96/300], Step [28/42], Loss: 0.3211\n",
      "Epoch [96/300], Step [29/42], Loss: 0.6318\n",
      "Epoch [96/300], Step [30/42], Loss: 0.6037\n",
      "Epoch [96/300], Step [31/42], Loss: 1.5286\n",
      "Epoch [96/300], Step [32/42], Loss: 0.4452\n",
      "Epoch [96/300], Step [33/42], Loss: 1.3664\n",
      "Epoch [96/300], Step [34/42], Loss: 0.4412\n",
      "Epoch [96/300], Step [35/42], Loss: 0.9733\n",
      "Epoch [96/300], Step [36/42], Loss: 0.4807\n",
      "Epoch [96/300], Step [37/42], Loss: 0.3640\n",
      "Epoch [96/300], Step [38/42], Loss: 0.2352\n",
      "Epoch [96/300], Step [39/42], Loss: 0.8149\n",
      "Epoch [96/300], Step [40/42], Loss: 0.6089\n",
      "Epoch [96/300], Step [41/42], Loss: 4.4601\n",
      "Epoch [96/300], Step [42/42], Loss: 0.2634\n",
      "Val. loss :0.9843\n",
      "Epoch [97/300], Step [1/42], Loss: 1.2038\n",
      "Epoch [97/300], Step [2/42], Loss: 0.3158\n",
      "Epoch [97/300], Step [3/42], Loss: 0.8289\n",
      "Epoch [97/300], Step [4/42], Loss: 0.9700\n",
      "Epoch [97/300], Step [5/42], Loss: 1.3048\n",
      "Epoch [97/300], Step [6/42], Loss: 0.3047\n",
      "Epoch [97/300], Step [7/42], Loss: 0.4200\n",
      "Epoch [97/300], Step [8/42], Loss: 0.7809\n",
      "Epoch [97/300], Step [9/42], Loss: 0.8577\n",
      "Epoch [97/300], Step [10/42], Loss: 0.5229\n",
      "Epoch [97/300], Step [11/42], Loss: 0.5991\n",
      "Epoch [97/300], Step [12/42], Loss: 0.5306\n",
      "Epoch [97/300], Step [13/42], Loss: 0.6331\n",
      "Epoch [97/300], Step [14/42], Loss: 0.3690\n",
      "Epoch [97/300], Step [15/42], Loss: 1.5829\n",
      "Epoch [97/300], Step [16/42], Loss: 2.6295\n",
      "Epoch [97/300], Step [17/42], Loss: 0.6087\n",
      "Epoch [97/300], Step [18/42], Loss: 0.5168\n",
      "Epoch [97/300], Step [19/42], Loss: 0.4577\n",
      "Epoch [97/300], Step [20/42], Loss: 0.8055\n",
      "Epoch [97/300], Step [21/42], Loss: 1.0004\n",
      "Epoch [97/300], Step [22/42], Loss: 0.2446\n",
      "Epoch [97/300], Step [23/42], Loss: 0.7101\n",
      "Epoch [97/300], Step [24/42], Loss: 0.5260\n",
      "Epoch [97/300], Step [25/42], Loss: 0.4435\n",
      "Epoch [97/300], Step [26/42], Loss: 0.6932\n",
      "Epoch [97/300], Step [27/42], Loss: 1.0213\n",
      "Epoch [97/300], Step [28/42], Loss: 0.5240\n",
      "Epoch [97/300], Step [29/42], Loss: 0.6501\n",
      "Epoch [97/300], Step [30/42], Loss: 1.1967\n",
      "Epoch [97/300], Step [31/42], Loss: 0.5377\n",
      "Epoch [97/300], Step [32/42], Loss: 1.3955\n",
      "Epoch [97/300], Step [33/42], Loss: 1.1146\n",
      "Epoch [97/300], Step [34/42], Loss: 0.8710\n",
      "Epoch [97/300], Step [35/42], Loss: 0.3244\n",
      "Epoch [97/300], Step [36/42], Loss: 0.6677\n",
      "Epoch [97/300], Step [37/42], Loss: 0.7612\n",
      "Epoch [97/300], Step [38/42], Loss: 0.6227\n",
      "Epoch [97/300], Step [39/42], Loss: 0.8660\n",
      "Epoch [97/300], Step [40/42], Loss: 1.2451\n",
      "Epoch [97/300], Step [41/42], Loss: 0.6484\n",
      "Epoch [97/300], Step [42/42], Loss: 0.0598\n",
      "Val. loss :1.1266\n",
      "Epoch [98/300], Step [1/42], Loss: 0.2677\n",
      "Epoch [98/300], Step [2/42], Loss: 0.4706\n",
      "Epoch [98/300], Step [3/42], Loss: 0.4679\n",
      "Epoch [98/300], Step [4/42], Loss: 0.6789\n",
      "Epoch [98/300], Step [5/42], Loss: 1.5100\n",
      "Epoch [98/300], Step [6/42], Loss: 0.9386\n",
      "Epoch [98/300], Step [7/42], Loss: 0.7340\n",
      "Epoch [98/300], Step [8/42], Loss: 0.4862\n",
      "Epoch [98/300], Step [9/42], Loss: 0.3444\n",
      "Epoch [98/300], Step [10/42], Loss: 0.2530\n",
      "Epoch [98/300], Step [11/42], Loss: 0.4588\n",
      "Epoch [98/300], Step [12/42], Loss: 1.1310\n",
      "Epoch [98/300], Step [13/42], Loss: 1.1499\n",
      "Epoch [98/300], Step [14/42], Loss: 3.2063\n",
      "Epoch [98/300], Step [15/42], Loss: 1.2990\n",
      "Epoch [98/300], Step [16/42], Loss: 0.4136\n",
      "Epoch [98/300], Step [17/42], Loss: 1.8712\n",
      "Epoch [98/300], Step [18/42], Loss: 0.4283\n",
      "Epoch [98/300], Step [19/42], Loss: 0.3494\n",
      "Epoch [98/300], Step [20/42], Loss: 0.6446\n",
      "Epoch [98/300], Step [21/42], Loss: 0.9941\n",
      "Epoch [98/300], Step [22/42], Loss: 0.6545\n",
      "Epoch [98/300], Step [23/42], Loss: 2.2789\n",
      "Epoch [98/300], Step [24/42], Loss: 0.7556\n",
      "Epoch [98/300], Step [25/42], Loss: 0.4294\n",
      "Epoch [98/300], Step [26/42], Loss: 0.5292\n",
      "Epoch [98/300], Step [27/42], Loss: 0.3155\n",
      "Epoch [98/300], Step [28/42], Loss: 0.5952\n",
      "Epoch [98/300], Step [29/42], Loss: 1.7996\n",
      "Epoch [98/300], Step [30/42], Loss: 0.9263\n",
      "Epoch [98/300], Step [31/42], Loss: 0.7376\n",
      "Epoch [98/300], Step [32/42], Loss: 0.5829\n",
      "Epoch [98/300], Step [33/42], Loss: 1.4349\n",
      "Epoch [98/300], Step [34/42], Loss: 0.1969\n",
      "Epoch [98/300], Step [35/42], Loss: 0.6015\n",
      "Epoch [98/300], Step [36/42], Loss: 1.6554\n",
      "Epoch [98/300], Step [37/42], Loss: 0.2608\n",
      "Epoch [98/300], Step [38/42], Loss: 0.4392\n",
      "Epoch [98/300], Step [39/42], Loss: 0.4183\n",
      "Epoch [98/300], Step [40/42], Loss: 0.5778\n",
      "Epoch [98/300], Step [41/42], Loss: 0.4048\n",
      "Epoch [98/300], Step [42/42], Loss: 0.1898\n",
      "Val. loss :1.4735\n",
      "Epoch [99/300], Step [1/42], Loss: 0.2892\n",
      "Epoch [99/300], Step [2/42], Loss: 0.5956\n",
      "Epoch [99/300], Step [3/42], Loss: 0.6807\n",
      "Epoch [99/300], Step [4/42], Loss: 1.2354\n",
      "Epoch [99/300], Step [5/42], Loss: 2.0170\n",
      "Epoch [99/300], Step [6/42], Loss: 0.4049\n",
      "Epoch [99/300], Step [7/42], Loss: 0.4551\n",
      "Epoch [99/300], Step [8/42], Loss: 2.1392\n",
      "Epoch [99/300], Step [9/42], Loss: 2.7169\n",
      "Epoch [99/300], Step [10/42], Loss: 0.8418\n",
      "Epoch [99/300], Step [11/42], Loss: 1.2545\n",
      "Epoch [99/300], Step [12/42], Loss: 0.5212\n",
      "Epoch [99/300], Step [13/42], Loss: 0.2611\n",
      "Epoch [99/300], Step [14/42], Loss: 1.2992\n",
      "Epoch [99/300], Step [15/42], Loss: 0.3636\n",
      "Epoch [99/300], Step [16/42], Loss: 0.6430\n",
      "Epoch [99/300], Step [17/42], Loss: 0.7611\n",
      "Epoch [99/300], Step [18/42], Loss: 0.4761\n",
      "Epoch [99/300], Step [19/42], Loss: 0.5341\n",
      "Epoch [99/300], Step [20/42], Loss: 1.4969\n",
      "Epoch [99/300], Step [21/42], Loss: 0.4573\n",
      "Epoch [99/300], Step [22/42], Loss: 0.2747\n",
      "Epoch [99/300], Step [23/42], Loss: 0.4421\n",
      "Epoch [99/300], Step [24/42], Loss: 1.6623\n",
      "Epoch [99/300], Step [25/42], Loss: 3.1019\n",
      "Epoch [99/300], Step [26/42], Loss: 0.5414\n",
      "Epoch [99/300], Step [27/42], Loss: 0.9143\n",
      "Epoch [99/300], Step [28/42], Loss: 1.1140\n",
      "Epoch [99/300], Step [29/42], Loss: 0.9942\n",
      "Epoch [99/300], Step [30/42], Loss: 3.4311\n",
      "Epoch [99/300], Step [31/42], Loss: 0.7244\n",
      "Epoch [99/300], Step [32/42], Loss: 0.4859\n",
      "Epoch [99/300], Step [33/42], Loss: 1.3918\n",
      "Epoch [99/300], Step [34/42], Loss: 0.4351\n",
      "Epoch [99/300], Step [35/42], Loss: 1.7098\n",
      "Epoch [99/300], Step [36/42], Loss: 0.8503\n",
      "Epoch [99/300], Step [37/42], Loss: 1.1459\n",
      "Epoch [99/300], Step [38/42], Loss: 0.6729\n",
      "Epoch [99/300], Step [39/42], Loss: 0.8988\n",
      "Epoch [99/300], Step [40/42], Loss: 0.2430\n",
      "Epoch [99/300], Step [41/42], Loss: 0.6509\n",
      "Epoch [99/300], Step [42/42], Loss: 0.3744\n",
      "Val. loss :1.3156\n",
      "Epoch [100/300], Step [1/42], Loss: 0.7002\n",
      "Epoch [100/300], Step [2/42], Loss: 0.5592\n",
      "Epoch [100/300], Step [3/42], Loss: 0.5231\n",
      "Epoch [100/300], Step [4/42], Loss: 0.3275\n",
      "Epoch [100/300], Step [5/42], Loss: 0.5978\n",
      "Epoch [100/300], Step [6/42], Loss: 0.7472\n",
      "Epoch [100/300], Step [7/42], Loss: 1.4653\n",
      "Epoch [100/300], Step [8/42], Loss: 0.7561\n",
      "Epoch [100/300], Step [9/42], Loss: 0.6039\n",
      "Epoch [100/300], Step [10/42], Loss: 0.8515\n",
      "Epoch [100/300], Step [11/42], Loss: 0.3932\n",
      "Epoch [100/300], Step [12/42], Loss: 1.1388\n",
      "Epoch [100/300], Step [13/42], Loss: 0.3604\n",
      "Epoch [100/300], Step [14/42], Loss: 2.4452\n",
      "Epoch [100/300], Step [15/42], Loss: 0.5609\n",
      "Epoch [100/300], Step [16/42], Loss: 0.3711\n",
      "Epoch [100/300], Step [17/42], Loss: 1.0742\n",
      "Epoch [100/300], Step [18/42], Loss: 0.2470\n",
      "Epoch [100/300], Step [19/42], Loss: 0.5413\n",
      "Epoch [100/300], Step [20/42], Loss: 1.2840\n",
      "Epoch [100/300], Step [21/42], Loss: 0.4558\n",
      "Epoch [100/300], Step [22/42], Loss: 0.5336\n",
      "Epoch [100/300], Step [23/42], Loss: 1.4414\n",
      "Epoch [100/300], Step [24/42], Loss: 1.0285\n",
      "Epoch [100/300], Step [25/42], Loss: 0.8618\n",
      "Epoch [100/300], Step [26/42], Loss: 1.3594\n",
      "Epoch [100/300], Step [27/42], Loss: 0.4500\n",
      "Epoch [100/300], Step [28/42], Loss: 1.9645\n",
      "Epoch [100/300], Step [29/42], Loss: 3.1143\n",
      "Epoch [100/300], Step [30/42], Loss: 0.7400\n",
      "Epoch [100/300], Step [31/42], Loss: 0.9262\n",
      "Epoch [100/300], Step [32/42], Loss: 1.1191\n",
      "Epoch [100/300], Step [33/42], Loss: 1.1592\n",
      "Epoch [100/300], Step [34/42], Loss: 0.3663\n",
      "Epoch [100/300], Step [35/42], Loss: 0.4921\n",
      "Epoch [100/300], Step [36/42], Loss: 0.4121\n",
      "Epoch [100/300], Step [37/42], Loss: 0.5274\n",
      "Epoch [100/300], Step [38/42], Loss: 2.1515\n",
      "Epoch [100/300], Step [39/42], Loss: 0.2783\n",
      "Epoch [100/300], Step [40/42], Loss: 6.5144\n",
      "Epoch [100/300], Step [41/42], Loss: 0.6180\n",
      "Epoch [100/300], Step [42/42], Loss: 0.0782\n",
      "Val. loss :1.0064\n",
      "Epoch [101/300], Step [1/42], Loss: 0.7987\n",
      "Epoch [101/300], Step [2/42], Loss: 1.1333\n",
      "Epoch [101/300], Step [3/42], Loss: 1.5415\n",
      "Epoch [101/300], Step [4/42], Loss: 0.3247\n",
      "Epoch [101/300], Step [5/42], Loss: 0.4126\n",
      "Epoch [101/300], Step [6/42], Loss: 0.5208\n",
      "Epoch [101/300], Step [7/42], Loss: 3.3222\n",
      "Epoch [101/300], Step [8/42], Loss: 0.7998\n",
      "Epoch [101/300], Step [9/42], Loss: 0.7119\n",
      "Epoch [101/300], Step [10/42], Loss: 2.7801\n",
      "Epoch [101/300], Step [11/42], Loss: 0.7749\n",
      "Epoch [101/300], Step [12/42], Loss: 0.1995\n",
      "Epoch [101/300], Step [13/42], Loss: 2.8410\n",
      "Epoch [101/300], Step [14/42], Loss: 0.4812\n",
      "Epoch [101/300], Step [15/42], Loss: 2.1259\n",
      "Epoch [101/300], Step [16/42], Loss: 0.2788\n",
      "Epoch [101/300], Step [17/42], Loss: 0.6681\n",
      "Epoch [101/300], Step [18/42], Loss: 0.4471\n",
      "Epoch [101/300], Step [19/42], Loss: 0.6548\n",
      "Epoch [101/300], Step [20/42], Loss: 1.2588\n",
      "Epoch [101/300], Step [21/42], Loss: 0.5674\n",
      "Epoch [101/300], Step [22/42], Loss: 1.6812\n",
      "Epoch [101/300], Step [23/42], Loss: 1.3722\n",
      "Epoch [101/300], Step [24/42], Loss: 0.4731\n",
      "Epoch [101/300], Step [25/42], Loss: 0.5589\n",
      "Epoch [101/300], Step [26/42], Loss: 0.5088\n",
      "Epoch [101/300], Step [27/42], Loss: 0.8944\n",
      "Epoch [101/300], Step [28/42], Loss: 4.5597\n",
      "Epoch [101/300], Step [29/42], Loss: 0.7808\n",
      "Epoch [101/300], Step [30/42], Loss: 0.1904\n",
      "Epoch [101/300], Step [31/42], Loss: 1.7179\n",
      "Epoch [101/300], Step [32/42], Loss: 0.4325\n",
      "Epoch [101/300], Step [33/42], Loss: 1.0623\n",
      "Epoch [101/300], Step [34/42], Loss: 0.5338\n",
      "Epoch [101/300], Step [35/42], Loss: 0.4998\n",
      "Epoch [101/300], Step [36/42], Loss: 1.8847\n",
      "Epoch [101/300], Step [37/42], Loss: 1.2124\n",
      "Epoch [101/300], Step [38/42], Loss: 0.2581\n",
      "Epoch [101/300], Step [39/42], Loss: 1.6166\n",
      "Epoch [101/300], Step [40/42], Loss: 0.5612\n",
      "Epoch [101/300], Step [41/42], Loss: 0.6348\n",
      "Epoch [101/300], Step [42/42], Loss: 0.0439\n",
      "Val. loss :1.1061\n",
      "Epoch [102/300], Step [1/42], Loss: 0.5253\n",
      "Epoch [102/300], Step [2/42], Loss: 1.0071\n",
      "Epoch [102/300], Step [3/42], Loss: 1.3608\n",
      "Epoch [102/300], Step [4/42], Loss: 0.8825\n",
      "Epoch [102/300], Step [5/42], Loss: 0.7411\n",
      "Epoch [102/300], Step [6/42], Loss: 0.4151\n",
      "Epoch [102/300], Step [7/42], Loss: 1.0917\n",
      "Epoch [102/300], Step [8/42], Loss: 0.5312\n",
      "Epoch [102/300], Step [9/42], Loss: 1.3477\n",
      "Epoch [102/300], Step [10/42], Loss: 0.2669\n",
      "Epoch [102/300], Step [11/42], Loss: 0.4068\n",
      "Epoch [102/300], Step [12/42], Loss: 1.4361\n",
      "Epoch [102/300], Step [13/42], Loss: 0.6434\n",
      "Epoch [102/300], Step [14/42], Loss: 3.7207\n",
      "Epoch [102/300], Step [15/42], Loss: 0.8225\n",
      "Epoch [102/300], Step [16/42], Loss: 0.3314\n",
      "Epoch [102/300], Step [17/42], Loss: 1.8226\n",
      "Epoch [102/300], Step [18/42], Loss: 0.7037\n",
      "Epoch [102/300], Step [19/42], Loss: 0.2723\n",
      "Epoch [102/300], Step [20/42], Loss: 1.7154\n",
      "Epoch [102/300], Step [21/42], Loss: 0.3591\n",
      "Epoch [102/300], Step [22/42], Loss: 1.1039\n",
      "Epoch [102/300], Step [23/42], Loss: 0.9689\n",
      "Epoch [102/300], Step [24/42], Loss: 1.3332\n",
      "Epoch [102/300], Step [25/42], Loss: 0.3692\n",
      "Epoch [102/300], Step [26/42], Loss: 1.9542\n",
      "Epoch [102/300], Step [27/42], Loss: 0.8594\n",
      "Epoch [102/300], Step [28/42], Loss: 0.4915\n",
      "Epoch [102/300], Step [29/42], Loss: 2.2848\n",
      "Epoch [102/300], Step [30/42], Loss: 1.4462\n",
      "Epoch [102/300], Step [31/42], Loss: 1.0669\n",
      "Epoch [102/300], Step [32/42], Loss: 0.7465\n",
      "Epoch [102/300], Step [33/42], Loss: 1.1374\n",
      "Epoch [102/300], Step [34/42], Loss: 0.6357\n",
      "Epoch [102/300], Step [35/42], Loss: 1.4017\n",
      "Epoch [102/300], Step [36/42], Loss: 0.8396\n",
      "Epoch [102/300], Step [37/42], Loss: 1.1084\n",
      "Epoch [102/300], Step [38/42], Loss: 0.4613\n",
      "Epoch [102/300], Step [39/42], Loss: 1.2409\n",
      "Epoch [102/300], Step [40/42], Loss: 0.9339\n",
      "Epoch [102/300], Step [41/42], Loss: 2.3816\n",
      "Epoch [102/300], Step [42/42], Loss: 0.0841\n",
      "Val. loss :2.3391\n",
      "Epoch [103/300], Step [1/42], Loss: 1.8473\n",
      "Epoch [103/300], Step [2/42], Loss: 1.6759\n",
      "Epoch [103/300], Step [3/42], Loss: 0.3308\n",
      "Epoch [103/300], Step [4/42], Loss: 0.5470\n",
      "Epoch [103/300], Step [5/42], Loss: 0.3266\n",
      "Epoch [103/300], Step [6/42], Loss: 0.6484\n",
      "Epoch [103/300], Step [7/42], Loss: 2.2632\n",
      "Epoch [103/300], Step [8/42], Loss: 2.1713\n",
      "Epoch [103/300], Step [9/42], Loss: 0.7711\n",
      "Epoch [103/300], Step [10/42], Loss: 0.6083\n",
      "Epoch [103/300], Step [11/42], Loss: 0.6113\n",
      "Epoch [103/300], Step [12/42], Loss: 0.2129\n",
      "Epoch [103/300], Step [13/42], Loss: 1.5893\n",
      "Epoch [103/300], Step [14/42], Loss: 6.1116\n",
      "Epoch [103/300], Step [15/42], Loss: 0.5159\n",
      "Epoch [103/300], Step [16/42], Loss: 0.5955\n",
      "Epoch [103/300], Step [17/42], Loss: 1.3335\n",
      "Epoch [103/300], Step [18/42], Loss: 0.4529\n",
      "Epoch [103/300], Step [19/42], Loss: 0.8809\n",
      "Epoch [103/300], Step [20/42], Loss: 2.5628\n",
      "Epoch [103/300], Step [21/42], Loss: 0.2521\n",
      "Epoch [103/300], Step [22/42], Loss: 1.7131\n",
      "Epoch [103/300], Step [23/42], Loss: 0.5786\n",
      "Epoch [103/300], Step [24/42], Loss: 0.1549\n",
      "Epoch [103/300], Step [25/42], Loss: 0.5504\n",
      "Epoch [103/300], Step [26/42], Loss: 1.1437\n",
      "Epoch [103/300], Step [27/42], Loss: 3.3335\n",
      "Epoch [103/300], Step [28/42], Loss: 2.6025\n",
      "Epoch [103/300], Step [29/42], Loss: 0.5015\n",
      "Epoch [103/300], Step [30/42], Loss: 0.8626\n",
      "Epoch [103/300], Step [31/42], Loss: 0.6806\n",
      "Epoch [103/300], Step [32/42], Loss: 1.2046\n",
      "Epoch [103/300], Step [33/42], Loss: 0.5858\n",
      "Epoch [103/300], Step [34/42], Loss: 0.5209\n",
      "Epoch [103/300], Step [35/42], Loss: 0.6424\n",
      "Epoch [103/300], Step [36/42], Loss: 0.3328\n",
      "Epoch [103/300], Step [37/42], Loss: 2.4498\n",
      "Epoch [103/300], Step [38/42], Loss: 0.5128\n",
      "Epoch [103/300], Step [39/42], Loss: 0.9038\n",
      "Epoch [103/300], Step [40/42], Loss: 0.7978\n",
      "Epoch [103/300], Step [41/42], Loss: 0.5867\n",
      "Epoch [103/300], Step [42/42], Loss: 1.2757\n",
      "Val. loss :1.2981\n",
      "Epoch [104/300], Step [1/42], Loss: 0.4964\n",
      "Epoch [104/300], Step [2/42], Loss: 0.4685\n",
      "Epoch [104/300], Step [3/42], Loss: 1.0053\n",
      "Epoch [104/300], Step [4/42], Loss: 0.7143\n",
      "Epoch [104/300], Step [5/42], Loss: 0.9286\n",
      "Epoch [104/300], Step [6/42], Loss: 1.1054\n",
      "Epoch [104/300], Step [7/42], Loss: 0.6271\n",
      "Epoch [104/300], Step [8/42], Loss: 0.7966\n",
      "Epoch [104/300], Step [9/42], Loss: 0.8427\n",
      "Epoch [104/300], Step [10/42], Loss: 0.2268\n",
      "Epoch [104/300], Step [11/42], Loss: 0.9981\n",
      "Epoch [104/300], Step [12/42], Loss: 0.4329\n",
      "Epoch [104/300], Step [13/42], Loss: 0.8552\n",
      "Epoch [104/300], Step [14/42], Loss: 0.4950\n",
      "Epoch [104/300], Step [15/42], Loss: 0.4975\n",
      "Epoch [104/300], Step [16/42], Loss: 0.4037\n",
      "Epoch [104/300], Step [17/42], Loss: 0.9233\n",
      "Epoch [104/300], Step [18/42], Loss: 2.7689\n",
      "Epoch [104/300], Step [19/42], Loss: 0.8775\n",
      "Epoch [104/300], Step [20/42], Loss: 0.3245\n",
      "Epoch [104/300], Step [21/42], Loss: 0.6393\n",
      "Epoch [104/300], Step [22/42], Loss: 0.5312\n",
      "Epoch [104/300], Step [23/42], Loss: 0.3491\n",
      "Epoch [104/300], Step [24/42], Loss: 1.4748\n",
      "Epoch [104/300], Step [25/42], Loss: 0.3341\n",
      "Epoch [104/300], Step [26/42], Loss: 0.5777\n",
      "Epoch [104/300], Step [27/42], Loss: 0.7127\n",
      "Epoch [104/300], Step [28/42], Loss: 0.7323\n",
      "Epoch [104/300], Step [29/42], Loss: 0.7408\n",
      "Epoch [104/300], Step [30/42], Loss: 1.7061\n",
      "Epoch [104/300], Step [31/42], Loss: 2.8763\n",
      "Epoch [104/300], Step [32/42], Loss: 0.3677\n",
      "Epoch [104/300], Step [33/42], Loss: 6.3226\n",
      "Epoch [104/300], Step [34/42], Loss: 1.7197\n",
      "Epoch [104/300], Step [35/42], Loss: 0.6772\n",
      "Epoch [104/300], Step [36/42], Loss: 0.9774\n",
      "Epoch [104/300], Step [37/42], Loss: 0.6595\n",
      "Epoch [104/300], Step [38/42], Loss: 3.9189\n",
      "Epoch [104/300], Step [39/42], Loss: 1.8041\n",
      "Epoch [104/300], Step [40/42], Loss: 2.7829\n",
      "Epoch [104/300], Step [41/42], Loss: 2.3686\n",
      "Epoch [104/300], Step [42/42], Loss: 0.1393\n",
      "Val. loss :4.1211\n",
      "Epoch [105/300], Step [1/42], Loss: 10.7262\n",
      "Epoch [105/300], Step [2/42], Loss: 0.3496\n",
      "Epoch [105/300], Step [3/42], Loss: 1.1813\n",
      "Epoch [105/300], Step [4/42], Loss: 2.0578\n",
      "Epoch [105/300], Step [5/42], Loss: 0.7203\n",
      "Epoch [105/300], Step [6/42], Loss: 1.6083\n",
      "Epoch [105/300], Step [7/42], Loss: 2.2345\n",
      "Epoch [105/300], Step [8/42], Loss: 1.3866\n",
      "Epoch [105/300], Step [9/42], Loss: 1.1235\n",
      "Epoch [105/300], Step [10/42], Loss: 0.8629\n",
      "Epoch [105/300], Step [11/42], Loss: 2.3799\n",
      "Epoch [105/300], Step [12/42], Loss: 1.8841\n",
      "Epoch [105/300], Step [13/42], Loss: 3.3815\n",
      "Epoch [105/300], Step [14/42], Loss: 2.5308\n",
      "Epoch [105/300], Step [15/42], Loss: 2.6054\n",
      "Epoch [105/300], Step [16/42], Loss: 2.9554\n",
      "Epoch [105/300], Step [17/42], Loss: 0.8925\n",
      "Epoch [105/300], Step [18/42], Loss: 1.0131\n",
      "Epoch [105/300], Step [19/42], Loss: 1.5873\n",
      "Epoch [105/300], Step [20/42], Loss: 1.9508\n",
      "Epoch [105/300], Step [21/42], Loss: 0.5497\n",
      "Epoch [105/300], Step [22/42], Loss: 0.7281\n",
      "Epoch [105/300], Step [23/42], Loss: 0.9691\n",
      "Epoch [105/300], Step [24/42], Loss: 1.2833\n",
      "Epoch [105/300], Step [25/42], Loss: 2.6354\n",
      "Epoch [105/300], Step [26/42], Loss: 0.8398\n",
      "Epoch [105/300], Step [27/42], Loss: 0.2438\n",
      "Epoch [105/300], Step [28/42], Loss: 0.7077\n",
      "Epoch [105/300], Step [29/42], Loss: 2.2194\n",
      "Epoch [105/300], Step [30/42], Loss: 0.7516\n",
      "Epoch [105/300], Step [31/42], Loss: 1.3421\n",
      "Epoch [105/300], Step [32/42], Loss: 1.2808\n",
      "Epoch [105/300], Step [33/42], Loss: 1.6943\n",
      "Epoch [105/300], Step [34/42], Loss: 0.8576\n",
      "Epoch [105/300], Step [35/42], Loss: 1.4530\n",
      "Epoch [105/300], Step [36/42], Loss: 2.9875\n",
      "Epoch [105/300], Step [37/42], Loss: 1.8830\n",
      "Epoch [105/300], Step [38/42], Loss: 0.4001\n",
      "Epoch [105/300], Step [39/42], Loss: 1.0468\n",
      "Epoch [105/300], Step [40/42], Loss: 0.4902\n",
      "Epoch [105/300], Step [41/42], Loss: 3.2805\n",
      "Epoch [105/300], Step [42/42], Loss: 0.6060\n",
      "Val. loss :1.8145\n",
      "Epoch [106/300], Step [1/42], Loss: 3.5849\n",
      "Epoch [106/300], Step [2/42], Loss: 1.6104\n",
      "Epoch [106/300], Step [3/42], Loss: 1.6315\n",
      "Epoch [106/300], Step [4/42], Loss: 2.4034\n",
      "Epoch [106/300], Step [5/42], Loss: 3.4389\n",
      "Epoch [106/300], Step [6/42], Loss: 0.8815\n",
      "Epoch [106/300], Step [7/42], Loss: 0.6727\n",
      "Epoch [106/300], Step [8/42], Loss: 1.1226\n",
      "Epoch [106/300], Step [9/42], Loss: 0.6394\n",
      "Epoch [106/300], Step [10/42], Loss: 4.7525\n",
      "Epoch [106/300], Step [11/42], Loss: 1.4881\n",
      "Epoch [106/300], Step [12/42], Loss: 0.7578\n",
      "Epoch [106/300], Step [13/42], Loss: 1.3980\n",
      "Epoch [106/300], Step [14/42], Loss: 2.0502\n",
      "Epoch [106/300], Step [15/42], Loss: 0.7415\n",
      "Epoch [106/300], Step [16/42], Loss: 1.4000\n",
      "Epoch [106/300], Step [17/42], Loss: 0.9056\n",
      "Epoch [106/300], Step [18/42], Loss: 0.6281\n",
      "Epoch [106/300], Step [19/42], Loss: 0.5097\n",
      "Epoch [106/300], Step [20/42], Loss: 1.7800\n",
      "Epoch [106/300], Step [21/42], Loss: 2.0794\n",
      "Epoch [106/300], Step [22/42], Loss: 1.2012\n",
      "Epoch [106/300], Step [23/42], Loss: 0.7082\n",
      "Epoch [106/300], Step [24/42], Loss: 0.9395\n",
      "Epoch [106/300], Step [25/42], Loss: 0.5902\n",
      "Epoch [106/300], Step [26/42], Loss: 1.7665\n",
      "Epoch [106/300], Step [27/42], Loss: 1.0848\n",
      "Epoch [106/300], Step [28/42], Loss: 0.4869\n",
      "Epoch [106/300], Step [29/42], Loss: 1.5258\n",
      "Epoch [106/300], Step [30/42], Loss: 1.6751\n",
      "Epoch [106/300], Step [31/42], Loss: 0.3319\n",
      "Epoch [106/300], Step [32/42], Loss: 0.3370\n",
      "Epoch [106/300], Step [33/42], Loss: 0.4407\n",
      "Epoch [106/300], Step [34/42], Loss: 1.5315\n",
      "Epoch [106/300], Step [35/42], Loss: 0.4904\n",
      "Epoch [106/300], Step [36/42], Loss: 5.0968\n",
      "Epoch [106/300], Step [37/42], Loss: 0.7112\n",
      "Epoch [106/300], Step [38/42], Loss: 1.1341\n",
      "Epoch [106/300], Step [39/42], Loss: 0.7485\n",
      "Epoch [106/300], Step [40/42], Loss: 1.4828\n",
      "Epoch [106/300], Step [41/42], Loss: 2.8021\n",
      "Epoch [106/300], Step [42/42], Loss: 0.6732\n",
      "Val. loss :1.4196\n",
      "Epoch [107/300], Step [1/42], Loss: 0.5701\n",
      "Epoch [107/300], Step [2/42], Loss: 0.5428\n",
      "Epoch [107/300], Step [3/42], Loss: 0.8219\n",
      "Epoch [107/300], Step [4/42], Loss: 5.0580\n",
      "Epoch [107/300], Step [5/42], Loss: 3.0768\n",
      "Epoch [107/300], Step [6/42], Loss: 2.3480\n",
      "Epoch [107/300], Step [7/42], Loss: 3.2184\n",
      "Epoch [107/300], Step [8/42], Loss: 1.1525\n",
      "Epoch [107/300], Step [9/42], Loss: 0.6323\n",
      "Epoch [107/300], Step [10/42], Loss: 0.2961\n",
      "Epoch [107/300], Step [11/42], Loss: 3.3757\n",
      "Epoch [107/300], Step [12/42], Loss: 0.4463\n",
      "Epoch [107/300], Step [13/42], Loss: 1.4625\n",
      "Epoch [107/300], Step [14/42], Loss: 0.7992\n",
      "Epoch [107/300], Step [15/42], Loss: 1.8689\n",
      "Epoch [107/300], Step [16/42], Loss: 1.0474\n",
      "Epoch [107/300], Step [17/42], Loss: 0.5623\n",
      "Epoch [107/300], Step [18/42], Loss: 1.1770\n",
      "Epoch [107/300], Step [19/42], Loss: 0.3910\n",
      "Epoch [107/300], Step [20/42], Loss: 0.4011\n",
      "Epoch [107/300], Step [21/42], Loss: 0.4769\n",
      "Epoch [107/300], Step [22/42], Loss: 1.4201\n",
      "Epoch [107/300], Step [23/42], Loss: 0.8303\n",
      "Epoch [107/300], Step [24/42], Loss: 1.2594\n",
      "Epoch [107/300], Step [25/42], Loss: 2.5579\n",
      "Epoch [107/300], Step [26/42], Loss: 0.3607\n",
      "Epoch [107/300], Step [27/42], Loss: 0.7464\n",
      "Epoch [107/300], Step [28/42], Loss: 0.6851\n",
      "Epoch [107/300], Step [29/42], Loss: 0.5088\n",
      "Epoch [107/300], Step [30/42], Loss: 0.5741\n",
      "Epoch [107/300], Step [31/42], Loss: 0.4803\n",
      "Epoch [107/300], Step [32/42], Loss: 2.9216\n",
      "Epoch [107/300], Step [33/42], Loss: 0.5622\n",
      "Epoch [107/300], Step [34/42], Loss: 1.2560\n",
      "Epoch [107/300], Step [35/42], Loss: 1.6505\n",
      "Epoch [107/300], Step [36/42], Loss: 1.7955\n",
      "Epoch [107/300], Step [37/42], Loss: 0.5117\n",
      "Epoch [107/300], Step [38/42], Loss: 1.6372\n",
      "Epoch [107/300], Step [39/42], Loss: 0.5071\n",
      "Epoch [107/300], Step [40/42], Loss: 1.3078\n",
      "Epoch [107/300], Step [41/42], Loss: 1.6687\n",
      "Epoch [107/300], Step [42/42], Loss: 1.4814\n",
      "Val. loss :2.3250\n",
      "Epoch [108/300], Step [1/42], Loss: 1.0588\n",
      "Epoch [108/300], Step [2/42], Loss: 0.5465\n",
      "Epoch [108/300], Step [3/42], Loss: 1.8567\n",
      "Epoch [108/300], Step [4/42], Loss: 1.7714\n",
      "Epoch [108/300], Step [5/42], Loss: 0.7314\n",
      "Epoch [108/300], Step [6/42], Loss: 0.6305\n",
      "Epoch [108/300], Step [7/42], Loss: 0.6042\n",
      "Epoch [108/300], Step [8/42], Loss: 0.5833\n",
      "Epoch [108/300], Step [9/42], Loss: 0.3707\n",
      "Epoch [108/300], Step [10/42], Loss: 0.3156\n",
      "Epoch [108/300], Step [11/42], Loss: 0.5938\n",
      "Epoch [108/300], Step [12/42], Loss: 2.1044\n",
      "Epoch [108/300], Step [13/42], Loss: 0.6625\n",
      "Epoch [108/300], Step [14/42], Loss: 0.2427\n",
      "Epoch [108/300], Step [15/42], Loss: 1.2496\n",
      "Epoch [108/300], Step [16/42], Loss: 1.1791\n",
      "Epoch [108/300], Step [17/42], Loss: 1.4182\n",
      "Epoch [108/300], Step [18/42], Loss: 0.4973\n",
      "Epoch [108/300], Step [19/42], Loss: 0.5507\n",
      "Epoch [108/300], Step [20/42], Loss: 0.5967\n",
      "Epoch [108/300], Step [21/42], Loss: 1.8236\n",
      "Epoch [108/300], Step [22/42], Loss: 1.5560\n",
      "Epoch [108/300], Step [23/42], Loss: 0.2627\n",
      "Epoch [108/300], Step [24/42], Loss: 0.6192\n",
      "Epoch [108/300], Step [25/42], Loss: 1.5790\n",
      "Epoch [108/300], Step [26/42], Loss: 0.8142\n",
      "Epoch [108/300], Step [27/42], Loss: 1.3575\n",
      "Epoch [108/300], Step [28/42], Loss: 2.5820\n",
      "Epoch [108/300], Step [29/42], Loss: 0.2856\n",
      "Epoch [108/300], Step [30/42], Loss: 2.2619\n",
      "Epoch [108/300], Step [31/42], Loss: 2.0803\n",
      "Epoch [108/300], Step [32/42], Loss: 1.0056\n",
      "Epoch [108/300], Step [33/42], Loss: 0.5012\n",
      "Epoch [108/300], Step [34/42], Loss: 1.7270\n",
      "Epoch [108/300], Step [35/42], Loss: 0.8040\n",
      "Epoch [108/300], Step [36/42], Loss: 0.7122\n",
      "Epoch [108/300], Step [37/42], Loss: 1.9577\n",
      "Epoch [108/300], Step [38/42], Loss: 5.6857\n",
      "Epoch [108/300], Step [39/42], Loss: 0.4350\n",
      "Epoch [108/300], Step [40/42], Loss: 0.8953\n",
      "Epoch [108/300], Step [41/42], Loss: 0.3522\n",
      "Epoch [108/300], Step [42/42], Loss: 0.0613\n",
      "Val. loss :1.2178\n",
      "Epoch [109/300], Step [1/42], Loss: 0.6856\n",
      "Epoch [109/300], Step [2/42], Loss: 1.0530\n",
      "Epoch [109/300], Step [3/42], Loss: 0.5575\n",
      "Epoch [109/300], Step [4/42], Loss: 0.5855\n",
      "Epoch [109/300], Step [5/42], Loss: 0.8688\n",
      "Epoch [109/300], Step [6/42], Loss: 0.3240\n",
      "Epoch [109/300], Step [7/42], Loss: 0.8307\n",
      "Epoch [109/300], Step [8/42], Loss: 0.9141\n",
      "Epoch [109/300], Step [9/42], Loss: 0.5531\n",
      "Epoch [109/300], Step [10/42], Loss: 0.4753\n",
      "Epoch [109/300], Step [11/42], Loss: 1.3116\n",
      "Epoch [109/300], Step [12/42], Loss: 0.5202\n",
      "Epoch [109/300], Step [13/42], Loss: 0.2428\n",
      "Epoch [109/300], Step [14/42], Loss: 0.9941\n",
      "Epoch [109/300], Step [15/42], Loss: 0.2079\n",
      "Epoch [109/300], Step [16/42], Loss: 0.3733\n",
      "Epoch [109/300], Step [17/42], Loss: 0.6561\n",
      "Epoch [109/300], Step [18/42], Loss: 0.7698\n",
      "Epoch [109/300], Step [19/42], Loss: 0.4690\n",
      "Epoch [109/300], Step [20/42], Loss: 0.2304\n",
      "Epoch [109/300], Step [21/42], Loss: 1.5688\n",
      "Epoch [109/300], Step [22/42], Loss: 1.0567\n",
      "Epoch [109/300], Step [23/42], Loss: 1.8172\n",
      "Epoch [109/300], Step [24/42], Loss: 0.3519\n",
      "Epoch [109/300], Step [25/42], Loss: 0.4073\n",
      "Epoch [109/300], Step [26/42], Loss: 0.9044\n",
      "Epoch [109/300], Step [27/42], Loss: 0.6141\n",
      "Epoch [109/300], Step [28/42], Loss: 0.5242\n",
      "Epoch [109/300], Step [29/42], Loss: 0.8939\n",
      "Epoch [109/300], Step [30/42], Loss: 1.2140\n",
      "Epoch [109/300], Step [31/42], Loss: 0.9779\n",
      "Epoch [109/300], Step [32/42], Loss: 0.5272\n",
      "Epoch [109/300], Step [33/42], Loss: 6.0083\n",
      "Epoch [109/300], Step [34/42], Loss: 0.5487\n",
      "Epoch [109/300], Step [35/42], Loss: 0.8631\n",
      "Epoch [109/300], Step [36/42], Loss: 0.5376\n",
      "Epoch [109/300], Step [37/42], Loss: 1.8203\n",
      "Epoch [109/300], Step [38/42], Loss: 0.5107\n",
      "Epoch [109/300], Step [39/42], Loss: 0.9295\n",
      "Epoch [109/300], Step [40/42], Loss: 0.4479\n",
      "Epoch [109/300], Step [41/42], Loss: 0.6045\n",
      "Epoch [109/300], Step [42/42], Loss: 0.0634\n",
      "Val. loss :1.0763\n",
      "Epoch [110/300], Step [1/42], Loss: 0.5916\n",
      "Epoch [110/300], Step [2/42], Loss: 1.2519\n",
      "Epoch [110/300], Step [3/42], Loss: 0.6514\n",
      "Epoch [110/300], Step [4/42], Loss: 0.4916\n",
      "Epoch [110/300], Step [5/42], Loss: 3.2535\n",
      "Epoch [110/300], Step [6/42], Loss: 0.4801\n",
      "Epoch [110/300], Step [7/42], Loss: 1.3266\n",
      "Epoch [110/300], Step [8/42], Loss: 2.2560\n",
      "Epoch [110/300], Step [9/42], Loss: 2.7790\n",
      "Epoch [110/300], Step [10/42], Loss: 0.2748\n",
      "Epoch [110/300], Step [11/42], Loss: 0.6917\n",
      "Epoch [110/300], Step [12/42], Loss: 3.1138\n",
      "Epoch [110/300], Step [13/42], Loss: 1.3442\n",
      "Epoch [110/300], Step [14/42], Loss: 0.3942\n",
      "Epoch [110/300], Step [15/42], Loss: 4.6216\n",
      "Epoch [110/300], Step [16/42], Loss: 1.5622\n",
      "Epoch [110/300], Step [17/42], Loss: 3.0291\n",
      "Epoch [110/300], Step [18/42], Loss: 0.9167\n",
      "Epoch [110/300], Step [19/42], Loss: 0.2673\n",
      "Epoch [110/300], Step [20/42], Loss: 2.6858\n",
      "Epoch [110/300], Step [21/42], Loss: 0.6786\n",
      "Epoch [110/300], Step [22/42], Loss: 0.7005\n",
      "Epoch [110/300], Step [23/42], Loss: 1.3587\n",
      "Epoch [110/300], Step [24/42], Loss: 0.8849\n",
      "Epoch [110/300], Step [25/42], Loss: 3.1449\n",
      "Epoch [110/300], Step [26/42], Loss: 1.5804\n",
      "Epoch [110/300], Step [27/42], Loss: 0.4134\n",
      "Epoch [110/300], Step [28/42], Loss: 1.8312\n",
      "Epoch [110/300], Step [29/42], Loss: 2.8487\n",
      "Epoch [110/300], Step [30/42], Loss: 5.1079\n",
      "Epoch [110/300], Step [31/42], Loss: 0.2783\n",
      "Epoch [110/300], Step [32/42], Loss: 1.1932\n",
      "Epoch [110/300], Step [33/42], Loss: 0.9490\n",
      "Epoch [110/300], Step [34/42], Loss: 1.5037\n",
      "Epoch [110/300], Step [35/42], Loss: 4.2889\n",
      "Epoch [110/300], Step [36/42], Loss: 2.1301\n",
      "Epoch [110/300], Step [37/42], Loss: 0.7740\n",
      "Epoch [110/300], Step [38/42], Loss: 3.6502\n",
      "Epoch [110/300], Step [39/42], Loss: 6.3583\n",
      "Epoch [110/300], Step [40/42], Loss: 0.7559\n",
      "Epoch [110/300], Step [41/42], Loss: 1.9609\n",
      "Epoch [110/300], Step [42/42], Loss: 0.3810\n",
      "Val. loss :4.7101\n",
      "Epoch [111/300], Step [1/42], Loss: 5.9737\n",
      "Epoch [111/300], Step [2/42], Loss: 8.7430\n",
      "Epoch [111/300], Step [3/42], Loss: 2.6096\n",
      "Epoch [111/300], Step [4/42], Loss: 1.9050\n",
      "Epoch [111/300], Step [5/42], Loss: 3.2997\n",
      "Epoch [111/300], Step [6/42], Loss: 0.6714\n",
      "Epoch [111/300], Step [7/42], Loss: 1.4172\n",
      "Epoch [111/300], Step [8/42], Loss: 0.4068\n",
      "Epoch [111/300], Step [9/42], Loss: 0.4181\n",
      "Epoch [111/300], Step [10/42], Loss: 3.6769\n",
      "Epoch [111/300], Step [11/42], Loss: 0.4037\n",
      "Epoch [111/300], Step [12/42], Loss: 0.4715\n",
      "Epoch [111/300], Step [13/42], Loss: 1.8384\n",
      "Epoch [111/300], Step [14/42], Loss: 4.3961\n",
      "Epoch [111/300], Step [15/42], Loss: 0.2986\n",
      "Epoch [111/300], Step [16/42], Loss: 0.9035\n",
      "Epoch [111/300], Step [17/42], Loss: 1.3359\n",
      "Epoch [111/300], Step [18/42], Loss: 0.9227\n",
      "Epoch [111/300], Step [19/42], Loss: 0.6541\n",
      "Epoch [111/300], Step [20/42], Loss: 0.4023\n",
      "Epoch [111/300], Step [21/42], Loss: 1.1926\n",
      "Epoch [111/300], Step [22/42], Loss: 2.4673\n",
      "Epoch [111/300], Step [23/42], Loss: 0.8992\n",
      "Epoch [111/300], Step [24/42], Loss: 0.9381\n",
      "Epoch [111/300], Step [25/42], Loss: 3.1634\n",
      "Epoch [111/300], Step [26/42], Loss: 1.0093\n",
      "Epoch [111/300], Step [27/42], Loss: 8.4219\n",
      "Epoch [111/300], Step [28/42], Loss: 0.9539\n",
      "Epoch [111/300], Step [29/42], Loss: 0.5721\n",
      "Epoch [111/300], Step [30/42], Loss: 0.5616\n",
      "Epoch [111/300], Step [31/42], Loss: 2.0672\n",
      "Epoch [111/300], Step [32/42], Loss: 2.1118\n",
      "Epoch [111/300], Step [33/42], Loss: 0.9864\n",
      "Epoch [111/300], Step [34/42], Loss: 0.6227\n",
      "Epoch [111/300], Step [35/42], Loss: 0.6257\n",
      "Epoch [111/300], Step [36/42], Loss: 1.8516\n",
      "Epoch [111/300], Step [37/42], Loss: 0.6935\n",
      "Epoch [111/300], Step [38/42], Loss: 0.5262\n",
      "Epoch [111/300], Step [39/42], Loss: 1.4148\n",
      "Epoch [111/300], Step [40/42], Loss: 0.5921\n",
      "Epoch [111/300], Step [41/42], Loss: 0.6235\n",
      "Epoch [111/300], Step [42/42], Loss: 1.6616\n",
      "Val. loss :1.6025\n",
      "Epoch [112/300], Step [1/42], Loss: 0.6046\n",
      "Epoch [112/300], Step [2/42], Loss: 2.3277\n",
      "Epoch [112/300], Step [3/42], Loss: 0.5121\n",
      "Epoch [112/300], Step [4/42], Loss: 0.5712\n",
      "Epoch [112/300], Step [5/42], Loss: 0.7725\n",
      "Epoch [112/300], Step [6/42], Loss: 0.8476\n",
      "Epoch [112/300], Step [7/42], Loss: 0.3833\n",
      "Epoch [112/300], Step [8/42], Loss: 0.4998\n",
      "Epoch [112/300], Step [9/42], Loss: 0.3742\n",
      "Epoch [112/300], Step [10/42], Loss: 0.7447\n",
      "Epoch [112/300], Step [11/42], Loss: 0.6989\n",
      "Epoch [112/300], Step [12/42], Loss: 1.0085\n",
      "Epoch [112/300], Step [13/42], Loss: 0.9063\n",
      "Epoch [112/300], Step [14/42], Loss: 0.9690\n",
      "Epoch [112/300], Step [15/42], Loss: 0.3953\n",
      "Epoch [112/300], Step [16/42], Loss: 1.5996\n",
      "Epoch [112/300], Step [17/42], Loss: 2.9888\n",
      "Epoch [112/300], Step [18/42], Loss: 0.3581\n",
      "Epoch [112/300], Step [19/42], Loss: 1.1034\n",
      "Epoch [112/300], Step [20/42], Loss: 0.9930\n",
      "Epoch [112/300], Step [21/42], Loss: 0.7813\n",
      "Epoch [112/300], Step [22/42], Loss: 0.5587\n",
      "Epoch [112/300], Step [23/42], Loss: 0.7994\n",
      "Epoch [112/300], Step [24/42], Loss: 0.4899\n",
      "Epoch [112/300], Step [25/42], Loss: 0.6097\n",
      "Epoch [112/300], Step [26/42], Loss: 1.4066\n",
      "Epoch [112/300], Step [27/42], Loss: 1.6812\n",
      "Epoch [112/300], Step [28/42], Loss: 0.6660\n",
      "Epoch [112/300], Step [29/42], Loss: 1.0692\n",
      "Epoch [112/300], Step [30/42], Loss: 0.9723\n",
      "Epoch [112/300], Step [31/42], Loss: 0.6091\n",
      "Epoch [112/300], Step [32/42], Loss: 0.5854\n",
      "Epoch [112/300], Step [33/42], Loss: 0.4937\n",
      "Epoch [112/300], Step [34/42], Loss: 0.3824\n",
      "Epoch [112/300], Step [35/42], Loss: 0.5738\n",
      "Epoch [112/300], Step [36/42], Loss: 4.2688\n",
      "Epoch [112/300], Step [37/42], Loss: 1.6646\n",
      "Epoch [112/300], Step [38/42], Loss: 2.1468\n",
      "Epoch [112/300], Step [39/42], Loss: 0.3368\n",
      "Epoch [112/300], Step [40/42], Loss: 1.9624\n",
      "Epoch [112/300], Step [41/42], Loss: 0.4811\n",
      "Epoch [112/300], Step [42/42], Loss: 0.0762\n",
      "Val. loss :1.2523\n",
      "Epoch [113/300], Step [1/42], Loss: 1.6565\n",
      "Epoch [113/300], Step [2/42], Loss: 0.6456\n",
      "Epoch [113/300], Step [3/42], Loss: 0.5333\n",
      "Epoch [113/300], Step [4/42], Loss: 1.4199\n",
      "Epoch [113/300], Step [5/42], Loss: 0.2490\n",
      "Epoch [113/300], Step [6/42], Loss: 0.7532\n",
      "Epoch [113/300], Step [7/42], Loss: 1.4044\n",
      "Epoch [113/300], Step [8/42], Loss: 0.9066\n",
      "Epoch [113/300], Step [9/42], Loss: 0.3607\n",
      "Epoch [113/300], Step [10/42], Loss: 0.7712\n",
      "Epoch [113/300], Step [11/42], Loss: 0.3151\n",
      "Epoch [113/300], Step [12/42], Loss: 6.3907\n",
      "Epoch [113/300], Step [13/42], Loss: 0.5676\n",
      "Epoch [113/300], Step [14/42], Loss: 0.4182\n",
      "Epoch [113/300], Step [15/42], Loss: 0.3198\n",
      "Epoch [113/300], Step [16/42], Loss: 0.5573\n",
      "Epoch [113/300], Step [17/42], Loss: 2.8839\n",
      "Epoch [113/300], Step [18/42], Loss: 0.8563\n",
      "Epoch [113/300], Step [19/42], Loss: 1.9693\n",
      "Epoch [113/300], Step [20/42], Loss: 1.1806\n",
      "Epoch [113/300], Step [21/42], Loss: 0.6213\n",
      "Epoch [113/300], Step [22/42], Loss: 0.6238\n",
      "Epoch [113/300], Step [23/42], Loss: 0.2968\n",
      "Epoch [113/300], Step [24/42], Loss: 4.5420\n",
      "Epoch [113/300], Step [25/42], Loss: 2.2666\n",
      "Epoch [113/300], Step [26/42], Loss: 0.4451\n",
      "Epoch [113/300], Step [27/42], Loss: 1.6856\n",
      "Epoch [113/300], Step [28/42], Loss: 0.6312\n",
      "Epoch [113/300], Step [29/42], Loss: 0.5379\n",
      "Epoch [113/300], Step [30/42], Loss: 0.6062\n",
      "Epoch [113/300], Step [31/42], Loss: 0.8215\n",
      "Epoch [113/300], Step [32/42], Loss: 1.1434\n",
      "Epoch [113/300], Step [33/42], Loss: 0.7398\n",
      "Epoch [113/300], Step [34/42], Loss: 0.6630\n",
      "Epoch [113/300], Step [35/42], Loss: 1.2085\n",
      "Epoch [113/300], Step [36/42], Loss: 1.7740\n",
      "Epoch [113/300], Step [37/42], Loss: 0.3538\n",
      "Epoch [113/300], Step [38/42], Loss: 1.3279\n",
      "Epoch [113/300], Step [39/42], Loss: 0.2908\n",
      "Epoch [113/300], Step [40/42], Loss: 0.6435\n",
      "Epoch [113/300], Step [41/42], Loss: 0.7057\n",
      "Epoch [113/300], Step [42/42], Loss: 0.1036\n",
      "Val. loss :1.8173\n",
      "Epoch [114/300], Step [1/42], Loss: 1.0557\n",
      "Epoch [114/300], Step [2/42], Loss: 1.0511\n",
      "Epoch [114/300], Step [3/42], Loss: 0.6173\n",
      "Epoch [114/300], Step [4/42], Loss: 0.6561\n",
      "Epoch [114/300], Step [5/42], Loss: 0.8357\n",
      "Epoch [114/300], Step [6/42], Loss: 0.4519\n",
      "Epoch [114/300], Step [7/42], Loss: 2.9532\n",
      "Epoch [114/300], Step [8/42], Loss: 5.2430\n",
      "Epoch [114/300], Step [9/42], Loss: 1.0999\n",
      "Epoch [114/300], Step [10/42], Loss: 0.2741\n",
      "Epoch [114/300], Step [11/42], Loss: 2.3513\n",
      "Epoch [114/300], Step [12/42], Loss: 0.6211\n",
      "Epoch [114/300], Step [13/42], Loss: 0.9190\n",
      "Epoch [114/300], Step [14/42], Loss: 0.1956\n",
      "Epoch [114/300], Step [15/42], Loss: 0.3315\n",
      "Epoch [114/300], Step [16/42], Loss: 0.9535\n",
      "Epoch [114/300], Step [17/42], Loss: 0.6339\n",
      "Epoch [114/300], Step [18/42], Loss: 0.3725\n",
      "Epoch [114/300], Step [19/42], Loss: 0.2617\n",
      "Epoch [114/300], Step [20/42], Loss: 1.5805\n",
      "Epoch [114/300], Step [21/42], Loss: 0.6018\n",
      "Epoch [114/300], Step [22/42], Loss: 2.4354\n",
      "Epoch [114/300], Step [23/42], Loss: 1.8767\n",
      "Epoch [114/300], Step [24/42], Loss: 1.3888\n",
      "Epoch [114/300], Step [25/42], Loss: 0.1774\n",
      "Epoch [114/300], Step [26/42], Loss: 0.6544\n",
      "Epoch [114/300], Step [27/42], Loss: 0.6271\n",
      "Epoch [114/300], Step [28/42], Loss: 1.0523\n",
      "Epoch [114/300], Step [29/42], Loss: 0.5733\n",
      "Epoch [114/300], Step [30/42], Loss: 1.0945\n",
      "Epoch [114/300], Step [31/42], Loss: 1.7110\n",
      "Epoch [114/300], Step [32/42], Loss: 1.0492\n",
      "Epoch [114/300], Step [33/42], Loss: 0.5565\n",
      "Epoch [114/300], Step [34/42], Loss: 0.9560\n",
      "Epoch [114/300], Step [35/42], Loss: 0.2598\n",
      "Epoch [114/300], Step [36/42], Loss: 0.5446\n",
      "Epoch [114/300], Step [37/42], Loss: 1.4317\n",
      "Epoch [114/300], Step [38/42], Loss: 0.7416\n",
      "Epoch [114/300], Step [39/42], Loss: 0.7599\n",
      "Epoch [114/300], Step [40/42], Loss: 0.6952\n",
      "Epoch [114/300], Step [41/42], Loss: 0.9934\n",
      "Epoch [114/300], Step [42/42], Loss: 0.0708\n",
      "Val. loss :1.1461\n",
      "Epoch [115/300], Step [1/42], Loss: 0.5461\n",
      "Epoch [115/300], Step [2/42], Loss: 0.5935\n",
      "Epoch [115/300], Step [3/42], Loss: 0.6437\n",
      "Epoch [115/300], Step [4/42], Loss: 2.2379\n",
      "Epoch [115/300], Step [5/42], Loss: 0.5481\n",
      "Epoch [115/300], Step [6/42], Loss: 0.3951\n",
      "Epoch [115/300], Step [7/42], Loss: 0.5308\n",
      "Epoch [115/300], Step [8/42], Loss: 0.3441\n",
      "Epoch [115/300], Step [9/42], Loss: 0.7152\n",
      "Epoch [115/300], Step [10/42], Loss: 0.8499\n",
      "Epoch [115/300], Step [11/42], Loss: 2.2085\n",
      "Epoch [115/300], Step [12/42], Loss: 0.5849\n",
      "Epoch [115/300], Step [13/42], Loss: 2.1531\n",
      "Epoch [115/300], Step [14/42], Loss: 0.3487\n",
      "Epoch [115/300], Step [15/42], Loss: 0.8087\n",
      "Epoch [115/300], Step [16/42], Loss: 0.2123\n",
      "Epoch [115/300], Step [17/42], Loss: 0.9833\n",
      "Epoch [115/300], Step [18/42], Loss: 2.5627\n",
      "Epoch [115/300], Step [19/42], Loss: 1.1919\n",
      "Epoch [115/300], Step [20/42], Loss: 0.9231\n",
      "Epoch [115/300], Step [21/42], Loss: 0.9130\n",
      "Epoch [115/300], Step [22/42], Loss: 1.5576\n",
      "Epoch [115/300], Step [23/42], Loss: 0.5194\n",
      "Epoch [115/300], Step [24/42], Loss: 0.3633\n",
      "Epoch [115/300], Step [25/42], Loss: 1.0185\n",
      "Epoch [115/300], Step [26/42], Loss: 0.8445\n",
      "Epoch [115/300], Step [27/42], Loss: 2.0786\n",
      "Epoch [115/300], Step [28/42], Loss: 0.4342\n",
      "Epoch [115/300], Step [29/42], Loss: 1.4113\n",
      "Epoch [115/300], Step [30/42], Loss: 5.6641\n",
      "Epoch [115/300], Step [31/42], Loss: 0.4267\n",
      "Epoch [115/300], Step [32/42], Loss: 0.8539\n",
      "Epoch [115/300], Step [33/42], Loss: 2.3262\n",
      "Epoch [115/300], Step [34/42], Loss: 0.4557\n",
      "Epoch [115/300], Step [35/42], Loss: 1.7361\n",
      "Epoch [115/300], Step [36/42], Loss: 1.5921\n",
      "Epoch [115/300], Step [37/42], Loss: 0.4410\n",
      "Epoch [115/300], Step [38/42], Loss: 0.6456\n",
      "Epoch [115/300], Step [39/42], Loss: 0.5114\n",
      "Epoch [115/300], Step [40/42], Loss: 0.5284\n",
      "Epoch [115/300], Step [41/42], Loss: 0.6366\n",
      "Epoch [115/300], Step [42/42], Loss: 1.1546\n",
      "Val. loss :1.8585\n",
      "Epoch [116/300], Step [1/42], Loss: 3.7576\n",
      "Epoch [116/300], Step [2/42], Loss: 0.3922\n",
      "Epoch [116/300], Step [3/42], Loss: 0.6487\n",
      "Epoch [116/300], Step [4/42], Loss: 4.9085\n",
      "Epoch [116/300], Step [5/42], Loss: 0.7070\n",
      "Epoch [116/300], Step [6/42], Loss: 0.5161\n",
      "Epoch [116/300], Step [7/42], Loss: 0.8408\n",
      "Epoch [116/300], Step [8/42], Loss: 0.9402\n",
      "Epoch [116/300], Step [9/42], Loss: 1.4242\n",
      "Epoch [116/300], Step [10/42], Loss: 0.6882\n",
      "Epoch [116/300], Step [11/42], Loss: 0.7954\n",
      "Epoch [116/300], Step [12/42], Loss: 0.8471\n",
      "Epoch [116/300], Step [13/42], Loss: 0.5609\n",
      "Epoch [116/300], Step [14/42], Loss: 0.7000\n",
      "Epoch [116/300], Step [15/42], Loss: 0.5725\n",
      "Epoch [116/300], Step [16/42], Loss: 0.8304\n",
      "Epoch [116/300], Step [17/42], Loss: 0.6012\n",
      "Epoch [116/300], Step [18/42], Loss: 1.7474\n",
      "Epoch [116/300], Step [19/42], Loss: 0.4774\n",
      "Epoch [116/300], Step [20/42], Loss: 0.7057\n",
      "Epoch [116/300], Step [21/42], Loss: 1.3895\n",
      "Epoch [116/300], Step [22/42], Loss: 1.2161\n",
      "Epoch [116/300], Step [23/42], Loss: 0.2566\n",
      "Epoch [116/300], Step [24/42], Loss: 1.5982\n",
      "Epoch [116/300], Step [25/42], Loss: 0.3590\n",
      "Epoch [116/300], Step [26/42], Loss: 0.6612\n",
      "Epoch [116/300], Step [27/42], Loss: 0.6365\n",
      "Epoch [116/300], Step [28/42], Loss: 0.5870\n",
      "Epoch [116/300], Step [29/42], Loss: 0.7326\n",
      "Epoch [116/300], Step [30/42], Loss: 0.8333\n",
      "Epoch [116/300], Step [31/42], Loss: 0.3674\n",
      "Epoch [116/300], Step [32/42], Loss: 1.5300\n",
      "Epoch [116/300], Step [33/42], Loss: 0.4462\n",
      "Epoch [116/300], Step [34/42], Loss: 0.5503\n",
      "Epoch [116/300], Step [35/42], Loss: 1.3570\n",
      "Epoch [116/300], Step [36/42], Loss: 0.6460\n",
      "Epoch [116/300], Step [37/42], Loss: 0.8789\n",
      "Epoch [116/300], Step [38/42], Loss: 1.2425\n",
      "Epoch [116/300], Step [39/42], Loss: 0.3105\n",
      "Epoch [116/300], Step [40/42], Loss: 0.4796\n",
      "Epoch [116/300], Step [41/42], Loss: 2.8865\n",
      "Epoch [116/300], Step [42/42], Loss: 0.0460\n",
      "Val. loss :1.8949\n",
      "Epoch [117/300], Step [1/42], Loss: 0.4272\n",
      "Epoch [117/300], Step [2/42], Loss: 1.1744\n",
      "Epoch [117/300], Step [3/42], Loss: 0.4539\n",
      "Epoch [117/300], Step [4/42], Loss: 1.3648\n",
      "Epoch [117/300], Step [5/42], Loss: 0.9100\n",
      "Epoch [117/300], Step [6/42], Loss: 1.5903\n",
      "Epoch [117/300], Step [7/42], Loss: 0.4612\n",
      "Epoch [117/300], Step [8/42], Loss: 0.3296\n",
      "Epoch [117/300], Step [9/42], Loss: 1.3476\n",
      "Epoch [117/300], Step [10/42], Loss: 0.6172\n",
      "Epoch [117/300], Step [11/42], Loss: 0.5030\n",
      "Epoch [117/300], Step [12/42], Loss: 0.8481\n",
      "Epoch [117/300], Step [13/42], Loss: 0.8180\n",
      "Epoch [117/300], Step [14/42], Loss: 1.9504\n",
      "Epoch [117/300], Step [15/42], Loss: 1.0410\n",
      "Epoch [117/300], Step [16/42], Loss: 1.0925\n",
      "Epoch [117/300], Step [17/42], Loss: 0.2018\n",
      "Epoch [117/300], Step [18/42], Loss: 0.3310\n",
      "Epoch [117/300], Step [19/42], Loss: 1.0076\n",
      "Epoch [117/300], Step [20/42], Loss: 0.4901\n",
      "Epoch [117/300], Step [21/42], Loss: 0.3637\n",
      "Epoch [117/300], Step [22/42], Loss: 1.7518\n",
      "Epoch [117/300], Step [23/42], Loss: 0.7563\n",
      "Epoch [117/300], Step [24/42], Loss: 1.1226\n",
      "Epoch [117/300], Step [25/42], Loss: 1.0090\n",
      "Epoch [117/300], Step [26/42], Loss: 0.3972\n",
      "Epoch [117/300], Step [27/42], Loss: 0.8158\n",
      "Epoch [117/300], Step [28/42], Loss: 0.3663\n",
      "Epoch [117/300], Step [29/42], Loss: 0.2580\n",
      "Epoch [117/300], Step [30/42], Loss: 0.4295\n",
      "Epoch [117/300], Step [31/42], Loss: 0.5773\n",
      "Epoch [117/300], Step [32/42], Loss: 0.4066\n",
      "Epoch [117/300], Step [33/42], Loss: 0.7027\n",
      "Epoch [117/300], Step [34/42], Loss: 0.5543\n",
      "Epoch [117/300], Step [35/42], Loss: 0.5260\n",
      "Epoch [117/300], Step [36/42], Loss: 0.8394\n",
      "Epoch [117/300], Step [37/42], Loss: 4.1979\n",
      "Epoch [117/300], Step [38/42], Loss: 0.7507\n",
      "Epoch [117/300], Step [39/42], Loss: 0.3712\n",
      "Epoch [117/300], Step [40/42], Loss: 0.6921\n",
      "Epoch [117/300], Step [41/42], Loss: 0.6643\n",
      "Epoch [117/300], Step [42/42], Loss: 0.0965\n",
      "Val. loss :3.2450\n",
      "Epoch [118/300], Step [1/42], Loss: 6.1944\n",
      "Epoch [118/300], Step [2/42], Loss: 0.5210\n",
      "Epoch [118/300], Step [3/42], Loss: 0.5949\n",
      "Epoch [118/300], Step [4/42], Loss: 0.9058\n",
      "Epoch [118/300], Step [5/42], Loss: 0.3476\n",
      "Epoch [118/300], Step [6/42], Loss: 2.5912\n",
      "Epoch [118/300], Step [7/42], Loss: 0.6050\n",
      "Epoch [118/300], Step [8/42], Loss: 1.1292\n",
      "Epoch [118/300], Step [9/42], Loss: 1.0275\n",
      "Epoch [118/300], Step [10/42], Loss: 0.6937\n",
      "Epoch [118/300], Step [11/42], Loss: 0.2674\n",
      "Epoch [118/300], Step [12/42], Loss: 1.1867\n",
      "Epoch [118/300], Step [13/42], Loss: 0.4893\n",
      "Epoch [118/300], Step [14/42], Loss: 0.3199\n",
      "Epoch [118/300], Step [15/42], Loss: 1.0030\n",
      "Epoch [118/300], Step [16/42], Loss: 0.7243\n",
      "Epoch [118/300], Step [17/42], Loss: 0.6532\n",
      "Epoch [118/300], Step [18/42], Loss: 2.3019\n",
      "Epoch [118/300], Step [19/42], Loss: 0.7200\n",
      "Epoch [118/300], Step [20/42], Loss: 1.7025\n",
      "Epoch [118/300], Step [21/42], Loss: 0.4520\n",
      "Epoch [118/300], Step [22/42], Loss: 0.7461\n",
      "Epoch [118/300], Step [23/42], Loss: 0.8856\n",
      "Epoch [118/300], Step [24/42], Loss: 0.3042\n",
      "Epoch [118/300], Step [25/42], Loss: 0.4339\n",
      "Epoch [118/300], Step [26/42], Loss: 1.0724\n",
      "Epoch [118/300], Step [27/42], Loss: 0.3642\n",
      "Epoch [118/300], Step [28/42], Loss: 0.4311\n",
      "Epoch [118/300], Step [29/42], Loss: 1.0490\n",
      "Epoch [118/300], Step [30/42], Loss: 1.0023\n",
      "Epoch [118/300], Step [31/42], Loss: 0.5629\n",
      "Epoch [118/300], Step [32/42], Loss: 0.8542\n",
      "Epoch [118/300], Step [33/42], Loss: 1.3055\n",
      "Epoch [118/300], Step [34/42], Loss: 6.2072\n",
      "Epoch [118/300], Step [35/42], Loss: 0.1802\n",
      "Epoch [118/300], Step [36/42], Loss: 1.3033\n",
      "Epoch [118/300], Step [37/42], Loss: 0.5906\n",
      "Epoch [118/300], Step [38/42], Loss: 2.4544\n",
      "Epoch [118/300], Step [39/42], Loss: 1.5857\n",
      "Epoch [118/300], Step [40/42], Loss: 0.9426\n",
      "Epoch [118/300], Step [41/42], Loss: 1.1580\n",
      "Epoch [118/300], Step [42/42], Loss: 0.0969\n",
      "Val. loss :1.7675\n",
      "Epoch [119/300], Step [1/42], Loss: 0.7355\n",
      "Epoch [119/300], Step [2/42], Loss: 1.1948\n",
      "Epoch [119/300], Step [3/42], Loss: 2.4771\n",
      "Epoch [119/300], Step [4/42], Loss: 1.0605\n",
      "Epoch [119/300], Step [5/42], Loss: 3.0306\n",
      "Epoch [119/300], Step [6/42], Loss: 0.7721\n",
      "Epoch [119/300], Step [7/42], Loss: 0.2222\n",
      "Epoch [119/300], Step [8/42], Loss: 2.3148\n",
      "Epoch [119/300], Step [9/42], Loss: 0.3359\n",
      "Epoch [119/300], Step [10/42], Loss: 0.7916\n",
      "Epoch [119/300], Step [11/42], Loss: 0.4566\n",
      "Epoch [119/300], Step [12/42], Loss: 0.4636\n",
      "Epoch [119/300], Step [13/42], Loss: 1.1394\n",
      "Epoch [119/300], Step [14/42], Loss: 0.4299\n",
      "Epoch [119/300], Step [15/42], Loss: 0.5401\n",
      "Epoch [119/300], Step [16/42], Loss: 0.6012\n",
      "Epoch [119/300], Step [17/42], Loss: 0.7946\n",
      "Epoch [119/300], Step [18/42], Loss: 0.4837\n",
      "Epoch [119/300], Step [19/42], Loss: 0.2392\n",
      "Epoch [119/300], Step [20/42], Loss: 0.5131\n",
      "Epoch [119/300], Step [21/42], Loss: 5.0195\n",
      "Epoch [119/300], Step [22/42], Loss: 1.1909\n",
      "Epoch [119/300], Step [23/42], Loss: 0.7084\n",
      "Epoch [119/300], Step [24/42], Loss: 1.7253\n",
      "Epoch [119/300], Step [25/42], Loss: 0.5119\n",
      "Epoch [119/300], Step [26/42], Loss: 1.8170\n",
      "Epoch [119/300], Step [27/42], Loss: 1.1909\n",
      "Epoch [119/300], Step [28/42], Loss: 2.1473\n",
      "Epoch [119/300], Step [29/42], Loss: 0.7462\n",
      "Epoch [119/300], Step [30/42], Loss: 0.6444\n",
      "Epoch [119/300], Step [31/42], Loss: 0.4385\n",
      "Epoch [119/300], Step [32/42], Loss: 1.3726\n",
      "Epoch [119/300], Step [33/42], Loss: 0.5947\n",
      "Epoch [119/300], Step [34/42], Loss: 0.5270\n",
      "Epoch [119/300], Step [35/42], Loss: 1.1236\n",
      "Epoch [119/300], Step [36/42], Loss: 0.7021\n",
      "Epoch [119/300], Step [37/42], Loss: 0.2952\n",
      "Epoch [119/300], Step [38/42], Loss: 1.6713\n",
      "Epoch [119/300], Step [39/42], Loss: 0.8246\n",
      "Epoch [119/300], Step [40/42], Loss: 1.8058\n",
      "Epoch [119/300], Step [41/42], Loss: 0.5277\n",
      "Epoch [119/300], Step [42/42], Loss: 1.7528\n",
      "Val. loss :1.7315\n",
      "Epoch [120/300], Step [1/42], Loss: 2.4594\n",
      "Epoch [120/300], Step [2/42], Loss: 0.6873\n",
      "Epoch [120/300], Step [3/42], Loss: 0.5245\n",
      "Epoch [120/300], Step [4/42], Loss: 0.4614\n",
      "Epoch [120/300], Step [5/42], Loss: 0.5863\n",
      "Epoch [120/300], Step [6/42], Loss: 1.6851\n",
      "Epoch [120/300], Step [7/42], Loss: 0.8293\n",
      "Epoch [120/300], Step [8/42], Loss: 0.7570\n",
      "Epoch [120/300], Step [9/42], Loss: 1.1133\n",
      "Epoch [120/300], Step [10/42], Loss: 1.6276\n",
      "Epoch [120/300], Step [11/42], Loss: 0.7517\n",
      "Epoch [120/300], Step [12/42], Loss: 0.8726\n",
      "Epoch [120/300], Step [13/42], Loss: 0.2482\n",
      "Epoch [120/300], Step [14/42], Loss: 0.8508\n",
      "Epoch [120/300], Step [15/42], Loss: 1.3689\n",
      "Epoch [120/300], Step [16/42], Loss: 3.9012\n",
      "Epoch [120/300], Step [17/42], Loss: 0.3484\n",
      "Epoch [120/300], Step [18/42], Loss: 1.2206\n",
      "Epoch [120/300], Step [19/42], Loss: 0.5899\n",
      "Epoch [120/300], Step [20/42], Loss: 0.3858\n",
      "Epoch [120/300], Step [21/42], Loss: 2.2539\n",
      "Epoch [120/300], Step [22/42], Loss: 1.8395\n",
      "Epoch [120/300], Step [23/42], Loss: 5.1791\n",
      "Epoch [120/300], Step [24/42], Loss: 0.5849\n",
      "Epoch [120/300], Step [25/42], Loss: 0.4055\n",
      "Epoch [120/300], Step [26/42], Loss: 1.0814\n",
      "Epoch [120/300], Step [27/42], Loss: 1.1116\n",
      "Epoch [120/300], Step [28/42], Loss: 1.0745\n",
      "Epoch [120/300], Step [29/42], Loss: 0.3389\n",
      "Epoch [120/300], Step [30/42], Loss: 2.1695\n",
      "Epoch [120/300], Step [31/42], Loss: 1.2215\n",
      "Epoch [120/300], Step [32/42], Loss: 2.8058\n",
      "Epoch [120/300], Step [33/42], Loss: 0.3252\n",
      "Epoch [120/300], Step [34/42], Loss: 1.2570\n",
      "Epoch [120/300], Step [35/42], Loss: 0.7904\n",
      "Epoch [120/300], Step [36/42], Loss: 0.4640\n",
      "Epoch [120/300], Step [37/42], Loss: 0.2583\n",
      "Epoch [120/300], Step [38/42], Loss: 0.7359\n",
      "Epoch [120/300], Step [39/42], Loss: 0.6431\n",
      "Epoch [120/300], Step [40/42], Loss: 1.4909\n",
      "Epoch [120/300], Step [41/42], Loss: 1.5996\n",
      "Epoch [120/300], Step [42/42], Loss: 0.8944\n",
      "Val. loss :2.3108\n",
      "Epoch [121/300], Step [1/42], Loss: 4.1493\n",
      "Epoch [121/300], Step [2/42], Loss: 1.1004\n",
      "Epoch [121/300], Step [3/42], Loss: 0.8229\n",
      "Epoch [121/300], Step [4/42], Loss: 0.2532\n",
      "Epoch [121/300], Step [5/42], Loss: 0.6293\n",
      "Epoch [121/300], Step [6/42], Loss: 0.5625\n",
      "Epoch [121/300], Step [7/42], Loss: 1.5554\n",
      "Epoch [121/300], Step [8/42], Loss: 0.5430\n",
      "Epoch [121/300], Step [9/42], Loss: 0.3554\n",
      "Epoch [121/300], Step [10/42], Loss: 0.3971\n",
      "Epoch [121/300], Step [11/42], Loss: 1.1198\n",
      "Epoch [121/300], Step [12/42], Loss: 0.2612\n",
      "Epoch [121/300], Step [13/42], Loss: 0.5468\n",
      "Epoch [121/300], Step [14/42], Loss: 0.4596\n",
      "Epoch [121/300], Step [15/42], Loss: 0.3727\n",
      "Epoch [121/300], Step [16/42], Loss: 0.8450\n",
      "Epoch [121/300], Step [17/42], Loss: 2.8468\n",
      "Epoch [121/300], Step [18/42], Loss: 0.7158\n",
      "Epoch [121/300], Step [19/42], Loss: 0.4746\n",
      "Epoch [121/300], Step [20/42], Loss: 0.6112\n",
      "Epoch [121/300], Step [21/42], Loss: 0.6036\n",
      "Epoch [121/300], Step [22/42], Loss: 1.8838\n",
      "Epoch [121/300], Step [23/42], Loss: 0.8594\n",
      "Epoch [121/300], Step [24/42], Loss: 0.6611\n",
      "Epoch [121/300], Step [25/42], Loss: 0.5255\n",
      "Epoch [121/300], Step [26/42], Loss: 0.3791\n",
      "Epoch [121/300], Step [27/42], Loss: 0.9690\n",
      "Epoch [121/300], Step [28/42], Loss: 0.8974\n",
      "Epoch [121/300], Step [29/42], Loss: 0.4992\n",
      "Epoch [121/300], Step [30/42], Loss: 0.2496\n",
      "Epoch [121/300], Step [31/42], Loss: 0.2893\n",
      "Epoch [121/300], Step [32/42], Loss: 0.6992\n",
      "Epoch [121/300], Step [33/42], Loss: 0.5000\n",
      "Epoch [121/300], Step [34/42], Loss: 0.3535\n",
      "Epoch [121/300], Step [35/42], Loss: 0.2966\n",
      "Epoch [121/300], Step [36/42], Loss: 0.3175\n",
      "Epoch [121/300], Step [37/42], Loss: 1.9458\n",
      "Epoch [121/300], Step [38/42], Loss: 0.3106\n",
      "Epoch [121/300], Step [39/42], Loss: 0.1566\n",
      "Epoch [121/300], Step [40/42], Loss: 1.3008\n",
      "Epoch [121/300], Step [41/42], Loss: 0.8623\n",
      "Epoch [121/300], Step [42/42], Loss: 0.0422\n",
      "Val. loss :1.0582\n",
      "Epoch [122/300], Step [1/42], Loss: 0.4231\n",
      "Epoch [122/300], Step [2/42], Loss: 0.5355\n",
      "Epoch [122/300], Step [3/42], Loss: 1.0706\n",
      "Epoch [122/300], Step [4/42], Loss: 0.9742\n",
      "Epoch [122/300], Step [5/42], Loss: 0.5144\n",
      "Epoch [122/300], Step [6/42], Loss: 0.3124\n",
      "Epoch [122/300], Step [7/42], Loss: 1.1330\n",
      "Epoch [122/300], Step [8/42], Loss: 3.7316\n",
      "Epoch [122/300], Step [9/42], Loss: 0.2043\n",
      "Epoch [122/300], Step [10/42], Loss: 0.5578\n",
      "Epoch [122/300], Step [11/42], Loss: 0.7033\n",
      "Epoch [122/300], Step [12/42], Loss: 1.4503\n",
      "Epoch [122/300], Step [13/42], Loss: 0.6815\n",
      "Epoch [122/300], Step [14/42], Loss: 1.1103\n",
      "Epoch [122/300], Step [15/42], Loss: 0.5431\n",
      "Epoch [122/300], Step [16/42], Loss: 0.3215\n",
      "Epoch [122/300], Step [17/42], Loss: 0.3281\n",
      "Epoch [122/300], Step [18/42], Loss: 1.0860\n",
      "Epoch [122/300], Step [19/42], Loss: 1.8814\n",
      "Epoch [122/300], Step [20/42], Loss: 0.2698\n",
      "Epoch [122/300], Step [21/42], Loss: 0.3423\n",
      "Epoch [122/300], Step [22/42], Loss: 0.2658\n",
      "Epoch [122/300], Step [23/42], Loss: 0.4565\n",
      "Epoch [122/300], Step [24/42], Loss: 0.5766\n",
      "Epoch [122/300], Step [25/42], Loss: 1.1680\n",
      "Epoch [122/300], Step [26/42], Loss: 1.4947\n",
      "Epoch [122/300], Step [27/42], Loss: 0.4327\n",
      "Epoch [122/300], Step [28/42], Loss: 0.3244\n",
      "Epoch [122/300], Step [29/42], Loss: 0.7216\n",
      "Epoch [122/300], Step [30/42], Loss: 1.0591\n",
      "Epoch [122/300], Step [31/42], Loss: 0.6119\n",
      "Epoch [122/300], Step [32/42], Loss: 0.8012\n",
      "Epoch [122/300], Step [33/42], Loss: 0.3224\n",
      "Epoch [122/300], Step [34/42], Loss: 0.3003\n",
      "Epoch [122/300], Step [35/42], Loss: 1.4653\n",
      "Epoch [122/300], Step [36/42], Loss: 0.4688\n",
      "Epoch [122/300], Step [37/42], Loss: 1.0752\n",
      "Epoch [122/300], Step [38/42], Loss: 0.4322\n",
      "Epoch [122/300], Step [39/42], Loss: 1.0667\n",
      "Epoch [122/300], Step [40/42], Loss: 0.7937\n",
      "Epoch [122/300], Step [41/42], Loss: 0.4925\n",
      "Epoch [122/300], Step [42/42], Loss: 0.0487\n",
      "Val. loss :1.0780\n",
      "Epoch [123/300], Step [1/42], Loss: 0.2572\n",
      "Epoch [123/300], Step [2/42], Loss: 0.2249\n",
      "Epoch [123/300], Step [3/42], Loss: 1.1474\n",
      "Epoch [123/300], Step [4/42], Loss: 0.3642\n",
      "Epoch [123/300], Step [5/42], Loss: 0.4715\n",
      "Epoch [123/300], Step [6/42], Loss: 1.0359\n",
      "Epoch [123/300], Step [7/42], Loss: 0.2320\n",
      "Epoch [123/300], Step [8/42], Loss: 0.4182\n",
      "Epoch [123/300], Step [9/42], Loss: 0.5617\n",
      "Epoch [123/300], Step [10/42], Loss: 0.2317\n",
      "Epoch [123/300], Step [11/42], Loss: 0.3928\n",
      "Epoch [123/300], Step [12/42], Loss: 0.6559\n",
      "Epoch [123/300], Step [13/42], Loss: 0.3928\n",
      "Epoch [123/300], Step [14/42], Loss: 2.0840\n",
      "Epoch [123/300], Step [15/42], Loss: 0.5828\n",
      "Epoch [123/300], Step [16/42], Loss: 0.7440\n",
      "Epoch [123/300], Step [17/42], Loss: 0.2254\n",
      "Epoch [123/300], Step [18/42], Loss: 0.7104\n",
      "Epoch [123/300], Step [19/42], Loss: 1.2805\n",
      "Epoch [123/300], Step [20/42], Loss: 0.7368\n",
      "Epoch [123/300], Step [21/42], Loss: 3.1183\n",
      "Epoch [123/300], Step [22/42], Loss: 0.9222\n",
      "Epoch [123/300], Step [23/42], Loss: 0.3096\n",
      "Epoch [123/300], Step [24/42], Loss: 3.4969\n",
      "Epoch [123/300], Step [25/42], Loss: 0.4658\n",
      "Epoch [123/300], Step [26/42], Loss: 0.4803\n",
      "Epoch [123/300], Step [27/42], Loss: 0.3160\n",
      "Epoch [123/300], Step [28/42], Loss: 0.2532\n",
      "Epoch [123/300], Step [29/42], Loss: 0.7599\n",
      "Epoch [123/300], Step [30/42], Loss: 0.3893\n",
      "Epoch [123/300], Step [31/42], Loss: 3.2218\n",
      "Epoch [123/300], Step [32/42], Loss: 0.5136\n",
      "Epoch [123/300], Step [33/42], Loss: 0.6834\n",
      "Epoch [123/300], Step [34/42], Loss: 0.5382\n",
      "Epoch [123/300], Step [35/42], Loss: 0.8630\n",
      "Epoch [123/300], Step [36/42], Loss: 0.8083\n",
      "Epoch [123/300], Step [37/42], Loss: 0.6718\n",
      "Epoch [123/300], Step [38/42], Loss: 0.7188\n",
      "Epoch [123/300], Step [39/42], Loss: 0.3746\n",
      "Epoch [123/300], Step [40/42], Loss: 0.7724\n",
      "Epoch [123/300], Step [41/42], Loss: 0.6176\n",
      "Epoch [123/300], Step [42/42], Loss: 0.1326\n",
      "Val. loss :1.0682\n",
      "Epoch [124/300], Step [1/42], Loss: 0.5441\n",
      "Epoch [124/300], Step [2/42], Loss: 0.4871\n",
      "Epoch [124/300], Step [3/42], Loss: 1.3835\n",
      "Epoch [124/300], Step [4/42], Loss: 0.5300\n",
      "Epoch [124/300], Step [5/42], Loss: 0.3589\n",
      "Epoch [124/300], Step [6/42], Loss: 0.4199\n",
      "Epoch [124/300], Step [7/42], Loss: 0.6615\n",
      "Epoch [124/300], Step [8/42], Loss: 0.4647\n",
      "Epoch [124/300], Step [9/42], Loss: 0.9146\n",
      "Epoch [124/300], Step [10/42], Loss: 0.5988\n",
      "Epoch [124/300], Step [11/42], Loss: 0.8336\n",
      "Epoch [124/300], Step [12/42], Loss: 3.4053\n",
      "Epoch [124/300], Step [13/42], Loss: 0.5175\n",
      "Epoch [124/300], Step [14/42], Loss: 0.5193\n",
      "Epoch [124/300], Step [15/42], Loss: 0.3594\n",
      "Epoch [124/300], Step [16/42], Loss: 1.6223\n",
      "Epoch [124/300], Step [17/42], Loss: 0.4117\n",
      "Epoch [124/300], Step [18/42], Loss: 0.5105\n",
      "Epoch [124/300], Step [19/42], Loss: 0.3092\n",
      "Epoch [124/300], Step [20/42], Loss: 0.6615\n",
      "Epoch [124/300], Step [21/42], Loss: 0.2686\n",
      "Epoch [124/300], Step [22/42], Loss: 0.4278\n",
      "Epoch [124/300], Step [23/42], Loss: 0.5345\n",
      "Epoch [124/300], Step [24/42], Loss: 0.4501\n",
      "Epoch [124/300], Step [25/42], Loss: 0.2599\n",
      "Epoch [124/300], Step [26/42], Loss: 0.8071\n",
      "Epoch [124/300], Step [27/42], Loss: 0.2995\n",
      "Epoch [124/300], Step [28/42], Loss: 0.9750\n",
      "Epoch [124/300], Step [29/42], Loss: 0.6065\n",
      "Epoch [124/300], Step [30/42], Loss: 0.2141\n",
      "Epoch [124/300], Step [31/42], Loss: 0.5246\n",
      "Epoch [124/300], Step [32/42], Loss: 0.4833\n",
      "Epoch [124/300], Step [33/42], Loss: 1.9426\n",
      "Epoch [124/300], Step [34/42], Loss: 0.7088\n",
      "Epoch [124/300], Step [35/42], Loss: 0.5531\n",
      "Epoch [124/300], Step [36/42], Loss: 0.3515\n",
      "Epoch [124/300], Step [37/42], Loss: 0.4125\n",
      "Epoch [124/300], Step [38/42], Loss: 0.6468\n",
      "Epoch [124/300], Step [39/42], Loss: 1.4042\n",
      "Epoch [124/300], Step [40/42], Loss: 1.2050\n",
      "Epoch [124/300], Step [41/42], Loss: 0.4265\n",
      "Epoch [124/300], Step [42/42], Loss: 0.0952\n",
      "Val. loss :0.9497\n",
      "Epoch [125/300], Step [1/42], Loss: 0.4188\n",
      "Epoch [125/300], Step [2/42], Loss: 0.6719\n",
      "Epoch [125/300], Step [3/42], Loss: 0.3049\n",
      "Epoch [125/300], Step [4/42], Loss: 0.3049\n",
      "Epoch [125/300], Step [5/42], Loss: 0.7757\n",
      "Epoch [125/300], Step [6/42], Loss: 0.3059\n",
      "Epoch [125/300], Step [7/42], Loss: 0.2962\n",
      "Epoch [125/300], Step [8/42], Loss: 0.9013\n",
      "Epoch [125/300], Step [9/42], Loss: 0.6506\n",
      "Epoch [125/300], Step [10/42], Loss: 0.3690\n",
      "Epoch [125/300], Step [11/42], Loss: 0.3546\n",
      "Epoch [125/300], Step [12/42], Loss: 1.5166\n",
      "Epoch [125/300], Step [13/42], Loss: 1.5866\n",
      "Epoch [125/300], Step [14/42], Loss: 0.5752\n",
      "Epoch [125/300], Step [15/42], Loss: 0.3400\n",
      "Epoch [125/300], Step [16/42], Loss: 1.0197\n",
      "Epoch [125/300], Step [17/42], Loss: 1.0620\n",
      "Epoch [125/300], Step [18/42], Loss: 1.3832\n",
      "Epoch [125/300], Step [19/42], Loss: 2.4226\n",
      "Epoch [125/300], Step [20/42], Loss: 2.9570\n",
      "Epoch [125/300], Step [21/42], Loss: 1.2410\n",
      "Epoch [125/300], Step [22/42], Loss: 0.6313\n",
      "Epoch [125/300], Step [23/42], Loss: 0.5300\n",
      "Epoch [125/300], Step [24/42], Loss: 0.2502\n",
      "Epoch [125/300], Step [25/42], Loss: 1.4403\n",
      "Epoch [125/300], Step [26/42], Loss: 0.3932\n",
      "Epoch [125/300], Step [27/42], Loss: 0.8088\n",
      "Epoch [125/300], Step [28/42], Loss: 0.1920\n",
      "Epoch [125/300], Step [29/42], Loss: 1.0654\n",
      "Epoch [125/300], Step [30/42], Loss: 3.5811\n",
      "Epoch [125/300], Step [31/42], Loss: 0.3506\n",
      "Epoch [125/300], Step [32/42], Loss: 0.2863\n",
      "Epoch [125/300], Step [33/42], Loss: 2.2470\n",
      "Epoch [125/300], Step [34/42], Loss: 0.3921\n",
      "Epoch [125/300], Step [35/42], Loss: 0.7848\n",
      "Epoch [125/300], Step [36/42], Loss: 2.4531\n",
      "Epoch [125/300], Step [37/42], Loss: 0.6156\n",
      "Epoch [125/300], Step [38/42], Loss: 1.0048\n",
      "Epoch [125/300], Step [39/42], Loss: 0.5472\n",
      "Epoch [125/300], Step [40/42], Loss: 0.2126\n",
      "Epoch [125/300], Step [41/42], Loss: 0.4041\n",
      "Epoch [125/300], Step [42/42], Loss: 0.1195\n",
      "Val. loss :1.1661\n",
      "Epoch [126/300], Step [1/42], Loss: 0.5163\n",
      "Epoch [126/300], Step [2/42], Loss: 0.3352\n",
      "Epoch [126/300], Step [3/42], Loss: 0.3716\n",
      "Epoch [126/300], Step [4/42], Loss: 0.7143\n",
      "Epoch [126/300], Step [5/42], Loss: 0.7312\n",
      "Epoch [126/300], Step [6/42], Loss: 0.5243\n",
      "Epoch [126/300], Step [7/42], Loss: 0.6864\n",
      "Epoch [126/300], Step [8/42], Loss: 0.3879\n",
      "Epoch [126/300], Step [9/42], Loss: 2.5404\n",
      "Epoch [126/300], Step [10/42], Loss: 0.2522\n",
      "Epoch [126/300], Step [11/42], Loss: 0.3112\n",
      "Epoch [126/300], Step [12/42], Loss: 0.5158\n",
      "Epoch [126/300], Step [13/42], Loss: 0.5567\n",
      "Epoch [126/300], Step [14/42], Loss: 5.1363\n",
      "Epoch [126/300], Step [15/42], Loss: 0.6566\n",
      "Epoch [126/300], Step [16/42], Loss: 0.4409\n",
      "Epoch [126/300], Step [17/42], Loss: 0.5461\n",
      "Epoch [126/300], Step [18/42], Loss: 0.6544\n",
      "Epoch [126/300], Step [19/42], Loss: 2.1951\n",
      "Epoch [126/300], Step [20/42], Loss: 2.9380\n",
      "Epoch [126/300], Step [21/42], Loss: 1.1736\n",
      "Epoch [126/300], Step [22/42], Loss: 0.5747\n",
      "Epoch [126/300], Step [23/42], Loss: 4.3085\n",
      "Epoch [126/300], Step [24/42], Loss: 0.8822\n",
      "Epoch [126/300], Step [25/42], Loss: 0.4445\n",
      "Epoch [126/300], Step [26/42], Loss: 1.1985\n",
      "Epoch [126/300], Step [27/42], Loss: 1.8621\n",
      "Epoch [126/300], Step [28/42], Loss: 0.8957\n",
      "Epoch [126/300], Step [29/42], Loss: 2.2418\n",
      "Epoch [126/300], Step [30/42], Loss: 1.0266\n",
      "Epoch [126/300], Step [31/42], Loss: 1.0717\n",
      "Epoch [126/300], Step [32/42], Loss: 0.4846\n",
      "Epoch [126/300], Step [33/42], Loss: 0.8520\n",
      "Epoch [126/300], Step [34/42], Loss: 0.4591\n",
      "Epoch [126/300], Step [35/42], Loss: 2.0214\n",
      "Epoch [126/300], Step [36/42], Loss: 1.3458\n",
      "Epoch [126/300], Step [37/42], Loss: 1.2125\n",
      "Epoch [126/300], Step [38/42], Loss: 2.4087\n",
      "Epoch [126/300], Step [39/42], Loss: 0.7620\n",
      "Epoch [126/300], Step [40/42], Loss: 1.4175\n",
      "Epoch [126/300], Step [41/42], Loss: 0.5041\n",
      "Epoch [126/300], Step [42/42], Loss: 0.1166\n",
      "Val. loss :1.6314\n",
      "Epoch [127/300], Step [1/42], Loss: 0.6249\n",
      "Epoch [127/300], Step [2/42], Loss: 1.7359\n",
      "Epoch [127/300], Step [3/42], Loss: 1.7770\n",
      "Epoch [127/300], Step [4/42], Loss: 1.1447\n",
      "Epoch [127/300], Step [5/42], Loss: 1.7725\n",
      "Epoch [127/300], Step [6/42], Loss: 4.2279\n",
      "Epoch [127/300], Step [7/42], Loss: 0.6955\n",
      "Epoch [127/300], Step [8/42], Loss: 0.9239\n",
      "Epoch [127/300], Step [9/42], Loss: 0.7083\n",
      "Epoch [127/300], Step [10/42], Loss: 0.3726\n",
      "Epoch [127/300], Step [11/42], Loss: 0.5574\n",
      "Epoch [127/300], Step [12/42], Loss: 1.4239\n",
      "Epoch [127/300], Step [13/42], Loss: 0.3319\n",
      "Epoch [127/300], Step [14/42], Loss: 0.3747\n",
      "Epoch [127/300], Step [15/42], Loss: 4.5735\n",
      "Epoch [127/300], Step [16/42], Loss: 0.5545\n",
      "Epoch [127/300], Step [17/42], Loss: 0.9299\n",
      "Epoch [127/300], Step [18/42], Loss: 0.6312\n",
      "Epoch [127/300], Step [19/42], Loss: 2.3567\n",
      "Epoch [127/300], Step [20/42], Loss: 2.0990\n",
      "Epoch [127/300], Step [21/42], Loss: 0.4032\n",
      "Epoch [127/300], Step [22/42], Loss: 0.3809\n",
      "Epoch [127/300], Step [23/42], Loss: 0.4225\n",
      "Epoch [127/300], Step [24/42], Loss: 1.5895\n",
      "Epoch [127/300], Step [25/42], Loss: 0.2719\n",
      "Epoch [127/300], Step [26/42], Loss: 3.1821\n",
      "Epoch [127/300], Step [27/42], Loss: 0.8604\n",
      "Epoch [127/300], Step [28/42], Loss: 2.4987\n",
      "Epoch [127/300], Step [29/42], Loss: 3.8163\n",
      "Epoch [127/300], Step [30/42], Loss: 0.6296\n",
      "Epoch [127/300], Step [31/42], Loss: 0.5216\n",
      "Epoch [127/300], Step [32/42], Loss: 3.3831\n",
      "Epoch [127/300], Step [33/42], Loss: 0.1840\n",
      "Epoch [127/300], Step [34/42], Loss: 0.2234\n",
      "Epoch [127/300], Step [35/42], Loss: 0.7621\n",
      "Epoch [127/300], Step [36/42], Loss: 1.8998\n",
      "Epoch [127/300], Step [37/42], Loss: 1.8724\n",
      "Epoch [127/300], Step [38/42], Loss: 0.5372\n",
      "Epoch [127/300], Step [39/42], Loss: 0.3155\n",
      "Epoch [127/300], Step [40/42], Loss: 0.3463\n",
      "Epoch [127/300], Step [41/42], Loss: 0.5545\n",
      "Epoch [127/300], Step [42/42], Loss: 0.1704\n",
      "Val. loss :1.1434\n",
      "Epoch [128/300], Step [1/42], Loss: 0.6086\n",
      "Epoch [128/300], Step [2/42], Loss: 0.2935\n",
      "Epoch [128/300], Step [3/42], Loss: 0.2955\n",
      "Epoch [128/300], Step [4/42], Loss: 1.1124\n",
      "Epoch [128/300], Step [5/42], Loss: 2.6131\n",
      "Epoch [128/300], Step [6/42], Loss: 1.3691\n",
      "Epoch [128/300], Step [7/42], Loss: 6.4411\n",
      "Epoch [128/300], Step [8/42], Loss: 0.4100\n",
      "Epoch [128/300], Step [9/42], Loss: 0.3714\n",
      "Epoch [128/300], Step [10/42], Loss: 0.5513\n",
      "Epoch [128/300], Step [11/42], Loss: 0.5190\n",
      "Epoch [128/300], Step [12/42], Loss: 1.0152\n",
      "Epoch [128/300], Step [13/42], Loss: 3.1631\n",
      "Epoch [128/300], Step [14/42], Loss: 1.0622\n",
      "Epoch [128/300], Step [15/42], Loss: 0.6096\n",
      "Epoch [128/300], Step [16/42], Loss: 0.3687\n",
      "Epoch [128/300], Step [17/42], Loss: 0.5368\n",
      "Epoch [128/300], Step [18/42], Loss: 1.3233\n",
      "Epoch [128/300], Step [19/42], Loss: 1.0865\n",
      "Epoch [128/300], Step [20/42], Loss: 0.8252\n",
      "Epoch [128/300], Step [21/42], Loss: 0.4168\n",
      "Epoch [128/300], Step [22/42], Loss: 1.9397\n",
      "Epoch [128/300], Step [23/42], Loss: 0.4592\n",
      "Epoch [128/300], Step [24/42], Loss: 0.3829\n",
      "Epoch [128/300], Step [25/42], Loss: 0.4532\n",
      "Epoch [128/300], Step [26/42], Loss: 1.4508\n",
      "Epoch [128/300], Step [27/42], Loss: 0.4180\n",
      "Epoch [128/300], Step [28/42], Loss: 0.4535\n",
      "Epoch [128/300], Step [29/42], Loss: 0.3569\n",
      "Epoch [128/300], Step [30/42], Loss: 0.4373\n",
      "Epoch [128/300], Step [31/42], Loss: 0.5585\n",
      "Epoch [128/300], Step [32/42], Loss: 0.7665\n",
      "Epoch [128/300], Step [33/42], Loss: 0.5250\n",
      "Epoch [128/300], Step [34/42], Loss: 0.4227\n",
      "Epoch [128/300], Step [35/42], Loss: 0.2716\n",
      "Epoch [128/300], Step [36/42], Loss: 0.5199\n",
      "Epoch [128/300], Step [37/42], Loss: 2.5512\n",
      "Epoch [128/300], Step [38/42], Loss: 0.8949\n",
      "Epoch [128/300], Step [39/42], Loss: 0.3681\n",
      "Epoch [128/300], Step [40/42], Loss: 0.5236\n",
      "Epoch [128/300], Step [41/42], Loss: 0.6689\n",
      "Epoch [128/300], Step [42/42], Loss: 0.1028\n",
      "Val. loss :1.0558\n",
      "Epoch [129/300], Step [1/42], Loss: 1.4258\n",
      "Epoch [129/300], Step [2/42], Loss: 0.6114\n",
      "Epoch [129/300], Step [3/42], Loss: 0.3875\n",
      "Epoch [129/300], Step [4/42], Loss: 0.6367\n",
      "Epoch [129/300], Step [5/42], Loss: 0.4703\n",
      "Epoch [129/300], Step [6/42], Loss: 0.3977\n",
      "Epoch [129/300], Step [7/42], Loss: 0.6613\n",
      "Epoch [129/300], Step [8/42], Loss: 0.3166\n",
      "Epoch [129/300], Step [9/42], Loss: 1.5866\n",
      "Epoch [129/300], Step [10/42], Loss: 0.6552\n",
      "Epoch [129/300], Step [11/42], Loss: 0.6198\n",
      "Epoch [129/300], Step [12/42], Loss: 0.3594\n",
      "Epoch [129/300], Step [13/42], Loss: 0.9009\n",
      "Epoch [129/300], Step [14/42], Loss: 0.2618\n",
      "Epoch [129/300], Step [15/42], Loss: 0.4656\n",
      "Epoch [129/300], Step [16/42], Loss: 1.0987\n",
      "Epoch [129/300], Step [17/42], Loss: 0.2256\n",
      "Epoch [129/300], Step [18/42], Loss: 0.9057\n",
      "Epoch [129/300], Step [19/42], Loss: 0.6757\n",
      "Epoch [129/300], Step [20/42], Loss: 0.6052\n",
      "Epoch [129/300], Step [21/42], Loss: 0.2837\n",
      "Epoch [129/300], Step [22/42], Loss: 1.0069\n",
      "Epoch [129/300], Step [23/42], Loss: 0.4533\n",
      "Epoch [129/300], Step [24/42], Loss: 0.7381\n",
      "Epoch [129/300], Step [25/42], Loss: 1.3143\n",
      "Epoch [129/300], Step [26/42], Loss: 0.5278\n",
      "Epoch [129/300], Step [27/42], Loss: 0.4377\n",
      "Epoch [129/300], Step [28/42], Loss: 0.8786\n",
      "Epoch [129/300], Step [29/42], Loss: 0.6632\n",
      "Epoch [129/300], Step [30/42], Loss: 0.3428\n",
      "Epoch [129/300], Step [31/42], Loss: 0.3574\n",
      "Epoch [129/300], Step [32/42], Loss: 1.0186\n",
      "Epoch [129/300], Step [33/42], Loss: 0.2191\n",
      "Epoch [129/300], Step [34/42], Loss: 0.2576\n",
      "Epoch [129/300], Step [35/42], Loss: 0.8000\n",
      "Epoch [129/300], Step [36/42], Loss: 4.0725\n",
      "Epoch [129/300], Step [37/42], Loss: 0.1713\n",
      "Epoch [129/300], Step [38/42], Loss: 0.3105\n",
      "Epoch [129/300], Step [39/42], Loss: 0.7741\n",
      "Epoch [129/300], Step [40/42], Loss: 2.1795\n",
      "Epoch [129/300], Step [41/42], Loss: 0.3665\n",
      "Epoch [129/300], Step [42/42], Loss: 1.3839\n",
      "Val. loss :0.9620\n",
      "Epoch [130/300], Step [1/42], Loss: 0.9469\n",
      "Epoch [130/300], Step [2/42], Loss: 0.3128\n",
      "Epoch [130/300], Step [3/42], Loss: 0.4798\n",
      "Epoch [130/300], Step [4/42], Loss: 0.7190\n",
      "Epoch [130/300], Step [5/42], Loss: 0.2602\n",
      "Epoch [130/300], Step [6/42], Loss: 0.9309\n",
      "Epoch [130/300], Step [7/42], Loss: 0.2067\n",
      "Epoch [130/300], Step [8/42], Loss: 0.5542\n",
      "Epoch [130/300], Step [9/42], Loss: 0.3706\n",
      "Epoch [130/300], Step [10/42], Loss: 1.4788\n",
      "Epoch [130/300], Step [11/42], Loss: 1.3661\n",
      "Epoch [130/300], Step [12/42], Loss: 0.3542\n",
      "Epoch [130/300], Step [13/42], Loss: 0.7036\n",
      "Epoch [130/300], Step [14/42], Loss: 0.4951\n",
      "Epoch [130/300], Step [15/42], Loss: 3.2595\n",
      "Epoch [130/300], Step [16/42], Loss: 0.2984\n",
      "Epoch [130/300], Step [17/42], Loss: 0.3387\n",
      "Epoch [130/300], Step [18/42], Loss: 0.3924\n",
      "Epoch [130/300], Step [19/42], Loss: 0.4816\n",
      "Epoch [130/300], Step [20/42], Loss: 0.5847\n",
      "Epoch [130/300], Step [21/42], Loss: 0.8697\n",
      "Epoch [130/300], Step [22/42], Loss: 0.5765\n",
      "Epoch [130/300], Step [23/42], Loss: 0.3085\n",
      "Epoch [130/300], Step [24/42], Loss: 0.2595\n",
      "Epoch [130/300], Step [25/42], Loss: 0.3481\n",
      "Epoch [130/300], Step [26/42], Loss: 0.3364\n",
      "Epoch [130/300], Step [27/42], Loss: 0.5076\n",
      "Epoch [130/300], Step [28/42], Loss: 1.5724\n",
      "Epoch [130/300], Step [29/42], Loss: 0.5599\n",
      "Epoch [130/300], Step [30/42], Loss: 0.6062\n",
      "Epoch [130/300], Step [31/42], Loss: 0.8349\n",
      "Epoch [130/300], Step [32/42], Loss: 0.4770\n",
      "Epoch [130/300], Step [33/42], Loss: 0.5659\n",
      "Epoch [130/300], Step [34/42], Loss: 0.6892\n",
      "Epoch [130/300], Step [35/42], Loss: 0.9794\n",
      "Epoch [130/300], Step [36/42], Loss: 0.8378\n",
      "Epoch [130/300], Step [37/42], Loss: 0.4118\n",
      "Epoch [130/300], Step [38/42], Loss: 0.2663\n",
      "Epoch [130/300], Step [39/42], Loss: 0.5076\n",
      "Epoch [130/300], Step [40/42], Loss: 1.5570\n",
      "Epoch [130/300], Step [41/42], Loss: 1.6939\n",
      "Epoch [130/300], Step [42/42], Loss: 0.0534\n",
      "Val. loss :0.7826\n",
      "Epoch [131/300], Step [1/42], Loss: 0.3699\n",
      "Epoch [131/300], Step [2/42], Loss: 1.0332\n",
      "Epoch [131/300], Step [3/42], Loss: 0.7443\n",
      "Epoch [131/300], Step [4/42], Loss: 0.3827\n",
      "Epoch [131/300], Step [5/42], Loss: 0.2123\n",
      "Epoch [131/300], Step [6/42], Loss: 1.3393\n",
      "Epoch [131/300], Step [7/42], Loss: 0.5039\n",
      "Epoch [131/300], Step [8/42], Loss: 0.7087\n",
      "Epoch [131/300], Step [9/42], Loss: 0.3549\n",
      "Epoch [131/300], Step [10/42], Loss: 0.5212\n",
      "Epoch [131/300], Step [11/42], Loss: 0.2808\n",
      "Epoch [131/300], Step [12/42], Loss: 0.4603\n",
      "Epoch [131/300], Step [13/42], Loss: 0.9218\n",
      "Epoch [131/300], Step [14/42], Loss: 0.6066\n",
      "Epoch [131/300], Step [15/42], Loss: 0.5019\n",
      "Epoch [131/300], Step [16/42], Loss: 1.1823\n",
      "Epoch [131/300], Step [17/42], Loss: 1.9480\n",
      "Epoch [131/300], Step [18/42], Loss: 0.8970\n",
      "Epoch [131/300], Step [19/42], Loss: 0.4198\n",
      "Epoch [131/300], Step [20/42], Loss: 0.5008\n",
      "Epoch [131/300], Step [21/42], Loss: 1.4069\n",
      "Epoch [131/300], Step [22/42], Loss: 2.4687\n",
      "Epoch [131/300], Step [23/42], Loss: 0.4981\n",
      "Epoch [131/300], Step [24/42], Loss: 0.6411\n",
      "Epoch [131/300], Step [25/42], Loss: 0.5934\n",
      "Epoch [131/300], Step [26/42], Loss: 1.8480\n",
      "Epoch [131/300], Step [27/42], Loss: 0.2716\n",
      "Epoch [131/300], Step [28/42], Loss: 0.2550\n",
      "Epoch [131/300], Step [29/42], Loss: 0.6829\n",
      "Epoch [131/300], Step [30/42], Loss: 1.1019\n",
      "Epoch [131/300], Step [31/42], Loss: 0.4259\n",
      "Epoch [131/300], Step [32/42], Loss: 0.3314\n",
      "Epoch [131/300], Step [33/42], Loss: 0.4733\n",
      "Epoch [131/300], Step [34/42], Loss: 0.8612\n",
      "Epoch [131/300], Step [35/42], Loss: 0.5423\n",
      "Epoch [131/300], Step [36/42], Loss: 1.1865\n",
      "Epoch [131/300], Step [37/42], Loss: 0.3047\n",
      "Epoch [131/300], Step [38/42], Loss: 0.2977\n",
      "Epoch [131/300], Step [39/42], Loss: 0.9861\n",
      "Epoch [131/300], Step [40/42], Loss: 0.4110\n",
      "Epoch [131/300], Step [41/42], Loss: 0.6934\n",
      "Epoch [131/300], Step [42/42], Loss: 0.1701\n",
      "Val. loss :0.9262\n",
      "Epoch [132/300], Step [1/42], Loss: 0.6318\n",
      "Epoch [132/300], Step [2/42], Loss: 0.3044\n",
      "Epoch [132/300], Step [3/42], Loss: 0.3896\n",
      "Epoch [132/300], Step [4/42], Loss: 1.1056\n",
      "Epoch [132/300], Step [5/42], Loss: 0.5662\n",
      "Epoch [132/300], Step [6/42], Loss: 0.4508\n",
      "Epoch [132/300], Step [7/42], Loss: 0.5747\n",
      "Epoch [132/300], Step [8/42], Loss: 0.1865\n",
      "Epoch [132/300], Step [9/42], Loss: 0.3137\n",
      "Epoch [132/300], Step [10/42], Loss: 0.8353\n",
      "Epoch [132/300], Step [11/42], Loss: 0.3515\n",
      "Epoch [132/300], Step [12/42], Loss: 0.3631\n",
      "Epoch [132/300], Step [13/42], Loss: 1.1056\n",
      "Epoch [132/300], Step [14/42], Loss: 1.0241\n",
      "Epoch [132/300], Step [15/42], Loss: 1.5312\n",
      "Epoch [132/300], Step [16/42], Loss: 0.2702\n",
      "Epoch [132/300], Step [17/42], Loss: 2.1832\n",
      "Epoch [132/300], Step [18/42], Loss: 0.2629\n",
      "Epoch [132/300], Step [19/42], Loss: 0.4666\n",
      "Epoch [132/300], Step [20/42], Loss: 0.3327\n",
      "Epoch [132/300], Step [21/42], Loss: 0.4455\n",
      "Epoch [132/300], Step [22/42], Loss: 0.7677\n",
      "Epoch [132/300], Step [23/42], Loss: 0.7708\n",
      "Epoch [132/300], Step [24/42], Loss: 1.4081\n",
      "Epoch [132/300], Step [25/42], Loss: 0.4128\n",
      "Epoch [132/300], Step [26/42], Loss: 0.5982\n",
      "Epoch [132/300], Step [27/42], Loss: 0.5937\n",
      "Epoch [132/300], Step [28/42], Loss: 2.1942\n",
      "Epoch [132/300], Step [29/42], Loss: 0.5505\n",
      "Epoch [132/300], Step [30/42], Loss: 5.7425\n",
      "Epoch [132/300], Step [31/42], Loss: 0.3777\n",
      "Epoch [132/300], Step [32/42], Loss: 0.6031\n",
      "Epoch [132/300], Step [33/42], Loss: 0.3854\n",
      "Epoch [132/300], Step [34/42], Loss: 0.2052\n",
      "Epoch [132/300], Step [35/42], Loss: 0.5460\n",
      "Epoch [132/300], Step [36/42], Loss: 0.8737\n",
      "Epoch [132/300], Step [37/42], Loss: 0.3118\n",
      "Epoch [132/300], Step [38/42], Loss: 0.8306\n",
      "Epoch [132/300], Step [39/42], Loss: 0.4645\n",
      "Epoch [132/300], Step [40/42], Loss: 0.4072\n",
      "Epoch [132/300], Step [41/42], Loss: 0.4856\n",
      "Epoch [132/300], Step [42/42], Loss: 0.1674\n",
      "Val. loss :0.9539\n",
      "Epoch [133/300], Step [1/42], Loss: 0.4822\n",
      "Epoch [133/300], Step [2/42], Loss: 0.3915\n",
      "Epoch [133/300], Step [3/42], Loss: 0.9320\n",
      "Epoch [133/300], Step [4/42], Loss: 0.6519\n",
      "Epoch [133/300], Step [5/42], Loss: 0.4337\n",
      "Epoch [133/300], Step [6/42], Loss: 0.3549\n",
      "Epoch [133/300], Step [7/42], Loss: 0.2239\n",
      "Epoch [133/300], Step [8/42], Loss: 0.4226\n",
      "Epoch [133/300], Step [9/42], Loss: 0.8253\n",
      "Epoch [133/300], Step [10/42], Loss: 0.2280\n",
      "Epoch [133/300], Step [11/42], Loss: 3.0783\n",
      "Epoch [133/300], Step [12/42], Loss: 0.7880\n",
      "Epoch [133/300], Step [13/42], Loss: 0.4156\n",
      "Epoch [133/300], Step [14/42], Loss: 3.2454\n",
      "Epoch [133/300], Step [15/42], Loss: 0.5566\n",
      "Epoch [133/300], Step [16/42], Loss: 0.2244\n",
      "Epoch [133/300], Step [17/42], Loss: 0.3670\n",
      "Epoch [133/300], Step [18/42], Loss: 0.6583\n",
      "Epoch [133/300], Step [19/42], Loss: 0.5633\n",
      "Epoch [133/300], Step [20/42], Loss: 1.2910\n",
      "Epoch [133/300], Step [21/42], Loss: 0.9113\n",
      "Epoch [133/300], Step [22/42], Loss: 0.9068\n",
      "Epoch [133/300], Step [23/42], Loss: 0.4327\n",
      "Epoch [133/300], Step [24/42], Loss: 0.6315\n",
      "Epoch [133/300], Step [25/42], Loss: 0.5440\n",
      "Epoch [133/300], Step [26/42], Loss: 0.6346\n",
      "Epoch [133/300], Step [27/42], Loss: 0.3004\n",
      "Epoch [133/300], Step [28/42], Loss: 0.2250\n",
      "Epoch [133/300], Step [29/42], Loss: 0.4155\n",
      "Epoch [133/300], Step [30/42], Loss: 1.6202\n",
      "Epoch [133/300], Step [31/42], Loss: 1.3994\n",
      "Epoch [133/300], Step [32/42], Loss: 0.8390\n",
      "Epoch [133/300], Step [33/42], Loss: 1.0477\n",
      "Epoch [133/300], Step [34/42], Loss: 1.7981\n",
      "Epoch [133/300], Step [35/42], Loss: 0.5160\n",
      "Epoch [133/300], Step [36/42], Loss: 1.2792\n",
      "Epoch [133/300], Step [37/42], Loss: 0.4166\n",
      "Epoch [133/300], Step [38/42], Loss: 0.2730\n",
      "Epoch [133/300], Step [39/42], Loss: 0.3594\n",
      "Epoch [133/300], Step [40/42], Loss: 0.2730\n",
      "Epoch [133/300], Step [41/42], Loss: 0.6653\n",
      "Epoch [133/300], Step [42/42], Loss: 0.1167\n",
      "Val. loss :1.4115\n",
      "Epoch [134/300], Step [1/42], Loss: 0.6009\n",
      "Epoch [134/300], Step [2/42], Loss: 0.2293\n",
      "Epoch [134/300], Step [3/42], Loss: 0.3965\n",
      "Epoch [134/300], Step [4/42], Loss: 0.3441\n",
      "Epoch [134/300], Step [5/42], Loss: 0.9629\n",
      "Epoch [134/300], Step [6/42], Loss: 0.8501\n",
      "Epoch [134/300], Step [7/42], Loss: 0.4441\n",
      "Epoch [134/300], Step [8/42], Loss: 1.3343\n",
      "Epoch [134/300], Step [9/42], Loss: 0.8001\n",
      "Epoch [134/300], Step [10/42], Loss: 0.6827\n",
      "Epoch [134/300], Step [11/42], Loss: 1.6344\n",
      "Epoch [134/300], Step [12/42], Loss: 1.0678\n",
      "Epoch [134/300], Step [13/42], Loss: 0.7546\n",
      "Epoch [134/300], Step [14/42], Loss: 0.5096\n",
      "Epoch [134/300], Step [15/42], Loss: 0.6027\n",
      "Epoch [134/300], Step [16/42], Loss: 3.2190\n",
      "Epoch [134/300], Step [17/42], Loss: 0.9010\n",
      "Epoch [134/300], Step [18/42], Loss: 0.4365\n",
      "Epoch [134/300], Step [19/42], Loss: 1.0320\n",
      "Epoch [134/300], Step [20/42], Loss: 0.5001\n",
      "Epoch [134/300], Step [21/42], Loss: 0.3936\n",
      "Epoch [134/300], Step [22/42], Loss: 0.4099\n",
      "Epoch [134/300], Step [23/42], Loss: 0.3165\n",
      "Epoch [134/300], Step [24/42], Loss: 0.6103\n",
      "Epoch [134/300], Step [25/42], Loss: 0.3681\n",
      "Epoch [134/300], Step [26/42], Loss: 0.2625\n",
      "Epoch [134/300], Step [27/42], Loss: 0.6261\n",
      "Epoch [134/300], Step [28/42], Loss: 0.8519\n",
      "Epoch [134/300], Step [29/42], Loss: 0.3932\n",
      "Epoch [134/300], Step [30/42], Loss: 1.6623\n",
      "Epoch [134/300], Step [31/42], Loss: 0.4882\n",
      "Epoch [134/300], Step [32/42], Loss: 0.4205\n",
      "Epoch [134/300], Step [33/42], Loss: 0.3908\n",
      "Epoch [134/300], Step [34/42], Loss: 0.7889\n",
      "Epoch [134/300], Step [35/42], Loss: 1.3482\n",
      "Epoch [134/300], Step [36/42], Loss: 0.5834\n",
      "Epoch [134/300], Step [37/42], Loss: 0.2117\n",
      "Epoch [134/300], Step [38/42], Loss: 0.3474\n",
      "Epoch [134/300], Step [39/42], Loss: 0.3547\n",
      "Epoch [134/300], Step [40/42], Loss: 1.1164\n",
      "Epoch [134/300], Step [41/42], Loss: 0.1745\n",
      "Epoch [134/300], Step [42/42], Loss: 0.0301\n",
      "Val. loss :1.0808\n",
      "Epoch [135/300], Step [1/42], Loss: 0.2370\n",
      "Epoch [135/300], Step [2/42], Loss: 0.4238\n",
      "Epoch [135/300], Step [3/42], Loss: 0.4105\n",
      "Epoch [135/300], Step [4/42], Loss: 0.3608\n",
      "Epoch [135/300], Step [5/42], Loss: 0.7690\n",
      "Epoch [135/300], Step [6/42], Loss: 0.3987\n",
      "Epoch [135/300], Step [7/42], Loss: 0.5497\n",
      "Epoch [135/300], Step [8/42], Loss: 0.4550\n",
      "Epoch [135/300], Step [9/42], Loss: 1.2979\n",
      "Epoch [135/300], Step [10/42], Loss: 0.3081\n",
      "Epoch [135/300], Step [11/42], Loss: 0.5621\n",
      "Epoch [135/300], Step [12/42], Loss: 1.3832\n",
      "Epoch [135/300], Step [13/42], Loss: 0.2250\n",
      "Epoch [135/300], Step [14/42], Loss: 0.7669\n",
      "Epoch [135/300], Step [15/42], Loss: 0.5320\n",
      "Epoch [135/300], Step [16/42], Loss: 0.9663\n",
      "Epoch [135/300], Step [17/42], Loss: 0.2474\n",
      "Epoch [135/300], Step [18/42], Loss: 0.2740\n",
      "Epoch [135/300], Step [19/42], Loss: 0.6097\n",
      "Epoch [135/300], Step [20/42], Loss: 0.8670\n",
      "Epoch [135/300], Step [21/42], Loss: 1.7105\n",
      "Epoch [135/300], Step [22/42], Loss: 0.4671\n",
      "Epoch [135/300], Step [23/42], Loss: 0.5672\n",
      "Epoch [135/300], Step [24/42], Loss: 0.3629\n",
      "Epoch [135/300], Step [25/42], Loss: 1.3981\n",
      "Epoch [135/300], Step [26/42], Loss: 1.3913\n",
      "Epoch [135/300], Step [27/42], Loss: 0.6307\n",
      "Epoch [135/300], Step [28/42], Loss: 0.2804\n",
      "Epoch [135/300], Step [29/42], Loss: 0.4164\n",
      "Epoch [135/300], Step [30/42], Loss: 0.8820\n",
      "Epoch [135/300], Step [31/42], Loss: 0.4449\n",
      "Epoch [135/300], Step [32/42], Loss: 0.4238\n",
      "Epoch [135/300], Step [33/42], Loss: 0.3798\n",
      "Epoch [135/300], Step [34/42], Loss: 0.6642\n",
      "Epoch [135/300], Step [35/42], Loss: 0.4799\n",
      "Epoch [135/300], Step [36/42], Loss: 1.0863\n",
      "Epoch [135/300], Step [37/42], Loss: 0.4108\n",
      "Epoch [135/300], Step [38/42], Loss: 0.2665\n",
      "Epoch [135/300], Step [39/42], Loss: 0.3139\n",
      "Epoch [135/300], Step [40/42], Loss: 0.9941\n",
      "Epoch [135/300], Step [41/42], Loss: 0.8048\n",
      "Epoch [135/300], Step [42/42], Loss: 0.4029\n",
      "Val. loss :0.7816\n",
      "Epoch [136/300], Step [1/42], Loss: 0.3652\n",
      "Epoch [136/300], Step [2/42], Loss: 0.8365\n",
      "Epoch [136/300], Step [3/42], Loss: 0.2819\n",
      "Epoch [136/300], Step [4/42], Loss: 0.5187\n",
      "Epoch [136/300], Step [5/42], Loss: 3.3992\n",
      "Epoch [136/300], Step [6/42], Loss: 0.5798\n",
      "Epoch [136/300], Step [7/42], Loss: 0.3363\n",
      "Epoch [136/300], Step [8/42], Loss: 0.5635\n",
      "Epoch [136/300], Step [9/42], Loss: 0.3839\n",
      "Epoch [136/300], Step [10/42], Loss: 0.4555\n",
      "Epoch [136/300], Step [11/42], Loss: 1.2073\n",
      "Epoch [136/300], Step [12/42], Loss: 0.3976\n",
      "Epoch [136/300], Step [13/42], Loss: 0.1932\n",
      "Epoch [136/300], Step [14/42], Loss: 1.3205\n",
      "Epoch [136/300], Step [15/42], Loss: 0.5621\n",
      "Epoch [136/300], Step [16/42], Loss: 0.2626\n",
      "Epoch [136/300], Step [17/42], Loss: 0.2897\n",
      "Epoch [136/300], Step [18/42], Loss: 0.5151\n",
      "Epoch [136/300], Step [19/42], Loss: 0.6170\n",
      "Epoch [136/300], Step [20/42], Loss: 0.5505\n",
      "Epoch [136/300], Step [21/42], Loss: 1.9902\n",
      "Epoch [136/300], Step [22/42], Loss: 0.7508\n",
      "Epoch [136/300], Step [23/42], Loss: 0.3395\n",
      "Epoch [136/300], Step [24/42], Loss: 2.0646\n",
      "Epoch [136/300], Step [25/42], Loss: 0.3188\n",
      "Epoch [136/300], Step [26/42], Loss: 1.0624\n",
      "Epoch [136/300], Step [27/42], Loss: 1.1210\n",
      "Epoch [136/300], Step [28/42], Loss: 0.3658\n",
      "Epoch [136/300], Step [29/42], Loss: 0.4170\n",
      "Epoch [136/300], Step [30/42], Loss: 0.4135\n",
      "Epoch [136/300], Step [31/42], Loss: 0.2500\n",
      "Epoch [136/300], Step [32/42], Loss: 0.2451\n",
      "Epoch [136/300], Step [33/42], Loss: 0.9858\n",
      "Epoch [136/300], Step [34/42], Loss: 0.3772\n",
      "Epoch [136/300], Step [35/42], Loss: 0.4953\n",
      "Epoch [136/300], Step [36/42], Loss: 0.5533\n",
      "Epoch [136/300], Step [37/42], Loss: 1.1529\n",
      "Epoch [136/300], Step [38/42], Loss: 0.5704\n",
      "Epoch [136/300], Step [39/42], Loss: 0.8185\n",
      "Epoch [136/300], Step [40/42], Loss: 0.5699\n",
      "Epoch [136/300], Step [41/42], Loss: 0.4848\n",
      "Epoch [136/300], Step [42/42], Loss: 0.1749\n",
      "Val. loss :1.2319\n",
      "Epoch [137/300], Step [1/42], Loss: 0.4572\n",
      "Epoch [137/300], Step [2/42], Loss: 0.3552\n",
      "Epoch [137/300], Step [3/42], Loss: 0.2656\n",
      "Epoch [137/300], Step [4/42], Loss: 1.0355\n",
      "Epoch [137/300], Step [5/42], Loss: 0.2317\n",
      "Epoch [137/300], Step [6/42], Loss: 0.3219\n",
      "Epoch [137/300], Step [7/42], Loss: 2.0360\n",
      "Epoch [137/300], Step [8/42], Loss: 0.6006\n",
      "Epoch [137/300], Step [9/42], Loss: 0.4099\n",
      "Epoch [137/300], Step [10/42], Loss: 0.3027\n",
      "Epoch [137/300], Step [11/42], Loss: 0.2867\n",
      "Epoch [137/300], Step [12/42], Loss: 0.1415\n",
      "Epoch [137/300], Step [13/42], Loss: 0.2709\n",
      "Epoch [137/300], Step [14/42], Loss: 1.5422\n",
      "Epoch [137/300], Step [15/42], Loss: 0.9385\n",
      "Epoch [137/300], Step [16/42], Loss: 0.4908\n",
      "Epoch [137/300], Step [17/42], Loss: 0.4872\n",
      "Epoch [137/300], Step [18/42], Loss: 0.4119\n",
      "Epoch [137/300], Step [19/42], Loss: 3.7072\n",
      "Epoch [137/300], Step [20/42], Loss: 0.3063\n",
      "Epoch [137/300], Step [21/42], Loss: 0.2726\n",
      "Epoch [137/300], Step [22/42], Loss: 0.2449\n",
      "Epoch [137/300], Step [23/42], Loss: 0.4654\n",
      "Epoch [137/300], Step [24/42], Loss: 1.5022\n",
      "Epoch [137/300], Step [25/42], Loss: 1.3628\n",
      "Epoch [137/300], Step [26/42], Loss: 0.5164\n",
      "Epoch [137/300], Step [27/42], Loss: 0.3129\n",
      "Epoch [137/300], Step [28/42], Loss: 0.7222\n",
      "Epoch [137/300], Step [29/42], Loss: 0.2956\n",
      "Epoch [137/300], Step [30/42], Loss: 0.5125\n",
      "Epoch [137/300], Step [31/42], Loss: 0.1982\n",
      "Epoch [137/300], Step [32/42], Loss: 0.9624\n",
      "Epoch [137/300], Step [33/42], Loss: 0.7240\n",
      "Epoch [137/300], Step [34/42], Loss: 0.7101\n",
      "Epoch [137/300], Step [35/42], Loss: 0.4027\n",
      "Epoch [137/300], Step [36/42], Loss: 0.6322\n",
      "Epoch [137/300], Step [37/42], Loss: 0.6662\n",
      "Epoch [137/300], Step [38/42], Loss: 0.6850\n",
      "Epoch [137/300], Step [39/42], Loss: 1.0221\n",
      "Epoch [137/300], Step [40/42], Loss: 0.3047\n",
      "Epoch [137/300], Step [41/42], Loss: 0.7042\n",
      "Epoch [137/300], Step [42/42], Loss: 0.0790\n",
      "Val. loss :0.7535\n",
      "Epoch [138/300], Step [1/42], Loss: 0.3663\n",
      "Epoch [138/300], Step [2/42], Loss: 0.2129\n",
      "Epoch [138/300], Step [3/42], Loss: 0.2447\n",
      "Epoch [138/300], Step [4/42], Loss: 0.7539\n",
      "Epoch [138/300], Step [5/42], Loss: 0.5802\n",
      "Epoch [138/300], Step [6/42], Loss: 0.9972\n",
      "Epoch [138/300], Step [7/42], Loss: 0.3167\n",
      "Epoch [138/300], Step [8/42], Loss: 0.8629\n",
      "Epoch [138/300], Step [9/42], Loss: 0.3896\n",
      "Epoch [138/300], Step [10/42], Loss: 1.0658\n",
      "Epoch [138/300], Step [11/42], Loss: 0.8468\n",
      "Epoch [138/300], Step [12/42], Loss: 0.8703\n",
      "Epoch [138/300], Step [13/42], Loss: 1.5403\n",
      "Epoch [138/300], Step [14/42], Loss: 2.7813\n",
      "Epoch [138/300], Step [15/42], Loss: 0.5799\n",
      "Epoch [138/300], Step [16/42], Loss: 0.3510\n",
      "Epoch [138/300], Step [17/42], Loss: 0.3258\n",
      "Epoch [138/300], Step [18/42], Loss: 0.9004\n",
      "Epoch [138/300], Step [19/42], Loss: 0.8786\n",
      "Epoch [138/300], Step [20/42], Loss: 0.4354\n",
      "Epoch [138/300], Step [21/42], Loss: 0.2801\n",
      "Epoch [138/300], Step [22/42], Loss: 1.4305\n",
      "Epoch [138/300], Step [23/42], Loss: 0.7852\n",
      "Epoch [138/300], Step [24/42], Loss: 1.0123\n",
      "Epoch [138/300], Step [25/42], Loss: 0.2710\n",
      "Epoch [138/300], Step [26/42], Loss: 4.5505\n",
      "Epoch [138/300], Step [27/42], Loss: 0.3945\n",
      "Epoch [138/300], Step [28/42], Loss: 1.0627\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import glob\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def random_cutout(image, label, max_h=50, max_w=50):\n",
    "    \"\"\"\n",
    "    Apply random cutout to both image and label.\n",
    "    Args:\n",
    "        image: The input image tensor.\n",
    "        label: The label tensor.\n",
    "        max_h: Maximum height of the cutout box.\n",
    "        max_w: Maximum width of the cutout box.\n",
    "    Returns:\n",
    "        image: Image after cutout.\n",
    "        label: Label after cutout.\n",
    "    \"\"\"\n",
    "    _, h, w = image.shape\n",
    "    cutout_height = random.randint(10, max_h)\n",
    "    cutout_width = random.randint(10, max_w)\n",
    "\n",
    "    # Randomly choose the position for the cutout\n",
    "    top = random.randint(0, h - cutout_height)\n",
    "    left = random.randint(0, w - cutout_width)\n",
    "\n",
    "    # Apply the cutout to the image and label (set to 0)\n",
    "    image[:, top:top + cutout_height, left:left + cutout_width] = 0\n",
    "    label[:, top:top + cutout_height, left:left + cutout_width] = 0\n",
    "\n",
    "    return image, label\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class facemapdataset(Dataset):\n",
    "    def __init__(self, data_file=\"data/dolensek_facemap_224.pt\", transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.data, self.targets = torch.load(data_file)\n",
    "        \n",
    "        # Filter out entries where any labels contain NaN values\n",
    "        valid_indices = [i for i, label in enumerate(self.targets) if not np.any(np.isnan(label))]\n",
    "        \n",
    "        # Keep only the valid entries\n",
    "        self.data = self.data[valid_indices]\n",
    "        self.targets = self.targets[valid_indices]\n",
    "        \n",
    "        # Ensure the targets are tensors and NaNs are replaced with zeros\n",
    "        self.targets = torch.Tensor(self.targets)\n",
    "        self.targets = torch.nan_to_num(self.targets, nan=0)  # Optionally replace NaNs with 0 or any value\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_index = index % len(self.data)  # This will prevent out-of-bounds errors\n",
    "        aug_type = index // len(self.data)   # This will determine which augmentation to apply\n",
    "\n",
    "        # Load the original image and label\n",
    "        image, label = self.data[base_index].clone(), self.targets[base_index].clone()\n",
    "        #image, label = self.data[index].clone(), self.targets[index].clone()\n",
    "        if self.transform is not None:\n",
    "            if aug_type == 1:  # Flipping\n",
    "                image = image.flip([2])  # Horizontal flip\n",
    "                label[::2] = 224 - label[::2]  # Flip keypoints horizontally\n",
    "                            # Apply random cutout with probability\n",
    "            if random.random() < self.cutout_prob:\n",
    "                image, label = random_cutout(image, label)\n",
    "        return image, label\n",
    "\n",
    "# Make dataset\n",
    "dataset = facemapdataset()  # This will now automatically filter out entries with NaN values\n",
    "\n",
    "x = dataset[0][0]\n",
    "dim = x.shape[-1]\n",
    "print('Using %d size of images' % dim)\n",
    "\n",
    "N = len(dataset)\n",
    "#train_sampler = SubsetRandomSampler(np.arange(int(0.6 * N)))\n",
    "#valid_sampler = SubsetRandomSampler(np.arange(int(0.6 * N), int(0.8 * N)))\n",
    "#test_sampler = SubsetRandomSampler(np.arange(int(0.8 * N), N))\n",
    "\n",
    "#try randomization\n",
    "indices = np.random.permutation(N)\n",
    "train_indices = indices[:int(0.6*N)]\n",
    "valid_indices = indices[int(0.6*N):int(0.8*N)]\n",
    "test_indices = indices[int(0.8*N):]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Initialize loss and metrics\n",
    "loss_fun = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Initialize input dimensions\n",
    "num_train = len(train_sampler)\n",
    "num_valid = len(valid_sampler)\n",
    "num_test = len(test_sampler)\n",
    "print(\"Num. train = %d, Num. val = %d, Num. test = %d\" % (num_train, num_valid, num_test))\n",
    "\n",
    "# Initialize dataloaders\n",
    "loader_train = DataLoader(dataset=dataset, drop_last=False, num_workers=0,\n",
    "                          batch_size=batch_size, pin_memory=True, sampler=train_sampler)\n",
    "loader_valid = DataLoader(dataset=dataset, drop_last=True, num_workers=0,\n",
    "                          batch_size=batch_size, pin_memory=True, sampler=valid_sampler)\n",
    "loader_test = DataLoader(dataset=dataset, drop_last=True, num_workers=0,\n",
    "                         batch_size=1, pin_memory=True, sampler=test_sampler)\n",
    "\n",
    "nValid = len(loader_valid)\n",
    "nTrain = len(loader_train)\n",
    "nTest = len(loader_test)\n",
    "\n",
    "### Hyperparameters\n",
    "lr = 5e-4\n",
    "num_epochs = 300\n",
    "num_input_channels = 1  # Change this to the desired number of input channels\n",
    "num_output_classes = 24  # Change this to the desired number of output classes\n",
    "\n",
    "model = timm.create_model('vit_base_patch8_224',\n",
    "                          pretrained=True, in_chans=1, num_classes=num_output_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "nParam = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters:%d M\" % (nParam / 1e6))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "minLoss = 1e6\n",
    "convIter = 0\n",
    "patience = 1000\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(loader_train):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        scores = F.softplus(model(inputs))\n",
    "        loss = loss_fun(\n",
    "            torch.log(scores[labels != 0]), torch.log(F.softplus(labels[labels != 0]))\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, num_epochs, i + 1, nTrain, loss.item()))\n",
    "        tr_loss += loss.item()\n",
    "    train_loss.append(tr_loss / (i + 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for i, (inputs, labels) in enumerate(loader_valid):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            scores = F.softplus(model(inputs))\n",
    "            loss = loss_fun(\n",
    "                torch.log(scores[labels != 0]),\n",
    "                torch.log(F.softplus(labels[labels != 0])),\n",
    "            )\n",
    "            val_loss += loss.item()\n",
    "        val_loss = val_loss / (i + 1)\n",
    "        \n",
    "        valid_loss.append(val_loss)\n",
    "\n",
    "        print('Val. loss :%.4f' % val_loss)\n",
    "        \n",
    "        img = inputs.squeeze().detach().cpu().numpy()\n",
    "        pred = scores.squeeze().detach().cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        for i in range(batch_size):\n",
    "            plt.subplot(1, batch_size, i + 1)\n",
    "            plt.imshow(img[i], cmap='gray')\n",
    "            plt.plot(pred[i, ::2], pred[i, 1::2], 'x', c='tab:red', label='pred.')\n",
    "            plt.plot(labels[i, ::2], labels[i, 1::2], 'o', c='tab:green', label='label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('logs/epoch_%03d.jpg' % epoch)\n",
    "        plt.close()\n",
    "            \n",
    "        if minLoss > val_loss:\n",
    "            convEpoch = epoch\n",
    "            minLoss = val_loss\n",
    "            convIter = 0\n",
    "            torch.save(model.state_dict(), 'models/best_model.pt')\n",
    "        else:\n",
    "            convIter += 1\n",
    "\n",
    "        if convIter == patience:\n",
    "            print('Converged at epoch %d with val. loss %.4f' % (convEpoch + 1, minLoss))\n",
    "            break\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(train_loss, label='Training')\n",
    "plt.plot(valid_loss, label='Valid')\n",
    "plt.plot(convEpoch, valid_loss[convEpoch], 'x', label='Final Model')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curve.pdf')\n",
    "\n",
    "### Load best model for inference\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader_test):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        scores = F.softplus(model(inputs))\n",
    "        loss = loss_fun(torch.log(scores), torch.log(F.softplus(labels)))\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        img = inputs.squeeze().detach().cpu().numpy()\n",
    "        pred = scores.squeeze().detach().cpu().numpy()\n",
    "        labels = labels.squeeze().cpu().numpy()\n",
    "        plt.clf()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.plot(pred[::2], pred[1::2], 'x', c='tab:red')\n",
    "        plt.plot(labels[::2], labels[1::2], 'o', c='tab:green')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('preds/test_%03d.jpg' % i)\n",
    "        plt.close()\n",
    "\n",
    "    val_loss = val_loss / (i + 1)\n",
    "    print('Test. loss :%.4f' % val_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
